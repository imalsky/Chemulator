===== /Users/imalsky/Desktop/Chemulator/dump.py =====
#!/usr/bin/env python3
"""
dump_code_and_jsonc.py — Recursively collects all .py and .jsonc files in a directory tree
and writes their paths and contents to a single dump file.

"""

import os
import sys
from pathlib import Path

# Configuration
ROOT_DIR = "."                   # Directory to start searching from
OUTPUT_FILE = "all_code_dump.txt"  # File to write the dump into
EXTENSIONS = (".py", ".jsonc")   # File extensions to include

def dump_files(root_dir: str, output_path: str, exts: tuple) -> None:
    """
    Recursively write every file with a matching extension into *output_path*.

    Symlinks, the output file itself, and unreadable paths are skipped
    to prevent infinite recursion and permission errors.
    """
    import os, sys
    output_abs  = os.path.abspath(output_path)
    seen_dirs   = set()

    with open(output_path, "w", encoding="utf-8") as out:
        for dirpath, _, filenames in os.walk(root_dir, followlinks=False):
            dir_abs = os.path.abspath(dirpath)
            if dir_abs in seen_dirs:
                continue
            seen_dirs.add(dir_abs)

            for name in filenames:
                if not any(name.lower().endswith(ext.lower()) for ext in exts):
                    continue
                file_abs = os.path.abspath(os.path.join(dirpath, name))
                if file_abs == output_abs or os.path.islink(file_abs):
                    continue

                out.write(f"===== {file_abs} =====\n")
                try:
                    with open(file_abs, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as exc:
                    out.write(f"# Could not read file: {type(exc).__name__}: {exc}\n")
                out.write("\n\n")


if __name__ == "__main__":
    # Allow command line arguments for flexibility
    if len(sys.argv) > 1:
        root_dir = sys.argv[1]
    else:
        root_dir = ROOT_DIR
    
    if len(sys.argv) > 2:
        output_file = sys.argv[2]
    else:
        output_file = OUTPUT_FILE
    
    if len(sys.argv) > 3:
        extensions = tuple(sys.argv[3:])
    else:
        extensions = EXTENSIONS
    
    dump_files(root_dir, output_file, extensions)
    print(f"Dumped all {', '.join(extensions)} files under '{root_dir}' into '{output_file}'")

===== /Users/imalsky/Desktop/Chemulator/config/config.jsonc =====
{
// ════════════════════════════ 1. PATHS ════════════════════════════
// Directories & file lists used throughout the pipeline
"paths": {
// List of raw HDF5 files (absolute or relative). Order is irrelevant.
"raw_data_files": [
"data/raw/run8001-result.h5",
"data/raw/run8002-result.h5",
"data/raw/run8003-result.h5",
"data/raw/run8004-result.h5",
"data/raw/run8005-result.h5",
"data/raw/run8006-result.h5",
"data/raw/run8007-result.h5",
"data/raw/run8008-result.h5",
"data/raw/run8009-result.h5"
],

// Where pre‑processed NPY shards go
"processed_data_dir": "data/processed",

// Parent dir for run‑specific model folders
"model_save_dir": "data/models",

// Central log directory (one file per run inside)
"log_dir": "logs"
},

// ════════════════════════════ 2. DATA ════════════════════════════
// Dataset layout & processing settings
"data": {
"species_variables": [
"C2H2_evolution",
"CH4_evolution",
"CO2_evolution",
"CO_evolution",
"H2O_evolution",
"H2_evolution",
"HCN_evolution",
"H_evolution",
"N2_evolution",
"NH3_evolution",
"OH_evolution",
"O_evolution"
],
"global_variables": ["P_init","T_init"],

// Name of the time column
"time_variable": "t_time",

// Read/write chunk (rows) – aligned with typical batch sizes
"chunk_size": 4096,

// NPY shard size (samples per shard file)
"shard_size": 1000000
},

// ═══════════════════════ 3. NORMALISATION ════════════════════════
// Preprocessing‑only; never used at run‑time inference
"normalization": {
// Fallback for any var not listed in "methods"
// Allowed: "standard" | "log-standard" | "log-min-max" | "symlog" | "none"
"default_method": "log-min-max",

// Per‑variable overrides (omit to inherit default_method)
"methods": {
"T_init": "standard",
"P_init": "log-min-max",
"t_time": "log-min-max"
},

// Numerical safety knobs – see DataNormalizer
"epsilon": 1e-37,          // floor added before log ops
"min_std": 1e-10,          // lower‑bound on σ to avoid divide‑by‑zero
"symlog_percentile": 0.1,  // threshold as |x|‑percentile for symlog
"clamp_value": 50.0        // post‑norm clipping limit
},

// ═══════════════════════════ 4. MODEL ════════════════════════════
// Architecture‑specific knobs.  Unused keys are ignored.
"model": {
// "siren" | "resnet" | "deeponet"
"type": "deeponet",

// ── shared / optional ───────────────────────────────────────────
"activation": "gelu",          // "gelu" | "relu" | "tanh"
"dropout": 0.0,                // 0.0‑1.0 (only if you add dropout layers)
"use_time_embedding": false,   // (future feature)
"time_embedding_dim": 256,
"output_scale": 1.0,           // scalar multiplier on final layer
"use_residual": true,          // add Δy onto y₀ (DeepONet only)

// ── SIREN / ResNet common ───────────────────────────────────────
"hidden_dims": [256, 256, 256],

// ── DeepONet specific ───────────────────────────────────────────
"branch_layers": [256, 256, 256],
"trunk_layers":  [64, 64, 64],
"basis_dim": 64
},

// ═════════════════════════ 5. TRAINING ═══════════════════════════
// Everything the Trainer consumes
"training": {
// ── dataset split ───────────────────────────────────────────────
"val_fraction":  0.15,   // 0‑1 – must leave ≥0 for train
"test_fraction": 0.15,   // 0‑1
"use_fraction":  0.1,    // subsample for quick tests

// ── global schedule ────────────────────────────────────────────
"epochs": 200,
"batch_size": 8192,
"gradient_accumulation_steps": 2,

// ── DataLoader knobs (overrides hardware auto‑tuning) ──────────
"num_workers": 16,
"pin_memory": true,
"persistent_workers": true,
"prefetch_factor": 4,
"drop_last": true,

// ── optimisation ───────────────────────────────────────────────
"learning_rate": 1e-4,
"weight_decay": 1e-5,
"gradient_clip": 1.0,

// ── LR scheduler ───────────────────────────────────────────────
// "plateau" | "cosine"
"scheduler": "plateau",

//   • For "plateau": factor, patience, min_lr
//   • For "cosine" : T_0 (epochs), T_mult, eta_min
"scheduler_params": {
// Plateau defaults
"factor": 0.5,
"patience": 5,
"min_lr": 1e-10

//Cosine example:
//"T_0": 20,
//"T_mult": 2,
//"eta_min": 1e-8
},

// ── loss ───────────────────────────────────────────────────────
// "mse" | "huber"
"loss": "mse",
"huber_delta": 0.25,

// ── mixed precision ────────────────────────────────────────────
"use_amp": true,
// "float16" (needs GradScaler on CUDA) | "bfloat16"
"amp_dtype": "bfloat16",

// ── early stopping & logging ───────────────────────────────────
"early_stopping_patience": 30,
"min_delta": 1e-10,        // improvement threshold
"log_interval": 10,        // batches
"save_interval": 20       // epochs
},

// ════════════════════════ 6. OPTUNA ═════════════════════════════
// Hyperparameter search configuration
"optuna": {
"enabled": false,              // Set to true to enable hyperparameter search
"n_trials": 50,               // Number of trials to run
"n_jobs": 4,                   // Number of parallel jobs
"study_name": "chemulator_optimization",
"direction": "minimize",       // minimize validation loss
"sampler": "TPE",             // TPE, Random, or Grid
"pruner": "HyperbandPruner",     // MedianPruner, HyperbandPruner, or None

// Search spaces - each parameter can be:
// - {"type": "float", "low": min, "high": max, "log": true/false}
// - {"type": "int", "low": min, "high": max}
// - {"type": "categorical", "choices": [...]}
// - {"type": "fixed", "value": ...} (not searched)
"search_space": {
// Model architecture search
"model.basis_dim": {
"type": "categorical",
"choices": [32, 64, 128, 256]
},
"model.branch_layers": {
"type": "categorical",
"choices": [
[128, 128, 128],
[256, 256, 256],
[512, 256, 128],
[256, 256, 256, 256]
]
},
"model.trunk_layers": {
"type": "categorical",
"choices": [
[32, 32, 32],
[64, 64, 64],
[128, 64, 32]
]
},
"model.activation": {
"type": "categorical",
"choices": ["gelu", "tanh"]
},

// Training hyperparameters
"training.learning_rate": {
"type": "float",
"low": 1e-5,
"high": 5e-4,
"log": true
},
"training.weight_decay": {
"type": "float",
"low": 1e-7,
"high": 1e-3,
"log": true
},
"training.batch_size": {
"type": "categorical",
"choices": [1024, 2048, 4096, 8192]
},
"training.gradient_accumulation_steps": {
"type": "categorical",
"choices": [1, 2, 4, 8]
},

// Keep some parameters fixed
"training.epochs": {
"type": "fixed",
"value": 300  // Shorter for hyperparameter search
}
}
},

// ═════════════════════════ 7. SYSTEM ════════════════════════════
"system": {
"seed": 42,                       // global RNG seed

// torch.compile & friends
"use_torch_compile": true,
"compile_mode": "default",        // "default" | "max-autotune"
"compile_fullgraph": false,
"compile_dynamic_shapes": false,

// Memory‑saving / correctness helpers
"gradient_checkpointing": true,
"detect_anomaly": false,

// Model export helpers
"use_torch_export": true,

// CUDA / cuDNN micro‑tuning
"cudnn_benchmark": true,
"tf32": true,                     // enable TF32 matmuls on Ampere+

// Cache management inside training loop
"empty_cache_interval": 10000        // batches between torch.cuda.empty_cache()
}
}

===== /Users/imalsky/Desktop/Chemulator/src/optuna_optimization.py =====
#!/usr/bin/env python3
"""
Hyperparameter optimization using Optuna for chemical kinetics models.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional
import copy

import optuna
from optuna.pruners import MedianPruner, HyperbandPruner
from optuna.samplers import TPESampler, RandomSampler, GridSampler
import torch
import numpy as np

from models.model import create_model
from training.trainer import Trainer
from data.dataset import NPYDataset, create_dataloader
from utils.hardware import setup_device, optimize_hardware
from utils.utils import seed_everything, save_json, load_json


class OptunaOptimizer:
    """
    Hyperparameter optimizer using Optuna for chemical kinetics models.
    """
    
    def __init__(
        self,
        base_config: Dict[str, Any],
        processed_dir: Path,
        save_dir: Path,
        device: torch.device
    ):
        self.base_config = base_config
        self.processed_dir = processed_dir
        self.save_dir = save_dir
        self.device = device
        
        self.logger = logging.getLogger(__name__)
        
        # Get optuna configuration
        self.optuna_config = base_config["optuna"]
        
        # Create study directory
        self.study_dir = save_dir / "optuna_study"
        self.study_dir.mkdir(parents=True, exist_ok=True)
        
        # Load normalization stats for dataset creation
        norm_path = self.processed_dir / "normalization.json"
        if norm_path.exists():
            self.norm_stats = load_json(norm_path)
        else:
            self.logger.warning("Normalization stats not found")
            self.norm_stats = None
    
    def _parse_search_space(self, trial: optuna.Trial, search_space: Dict[str, Any]) -> Dict[str, Any]:
        """Parse search space configuration and suggest values."""
        suggested_params = {}
        
        for param_path, param_config in search_space.items():
            param_type = param_config["type"]
            
            if param_type == "fixed":
                value = param_config["value"]
            elif param_type == "float":
                value = trial.suggest_float(
                    param_path,
                    param_config["low"],
                    param_config["high"],
                    log=param_config.get("log", False)
                )
            elif param_type == "int":
                value = trial.suggest_int(
                    param_path,
                    param_config["low"],
                    param_config["high"]
                )
            elif param_type == "categorical":
                value = trial.suggest_categorical(
                    param_path,
                    param_config["choices"]
                )
            else:
                raise ValueError(f"Unknown parameter type: {param_type}")
            
            suggested_params[param_path] = value
        
        return suggested_params
    
    def _update_config_with_params(self, config: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """Update configuration dictionary with suggested parameters."""
        config = copy.deepcopy(config)
        
        for param_path, value in params.items():
            # Split path (e.g., "model.learning_rate" -> ["model", "learning_rate"])
            keys = param_path.split(".")
            
            # Navigate to the correct location in config
            current = config
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            
            # Set the value
            current[keys[-1]] = value
        
        return config
    
    def _create_datasets(self, config: Dict[str, Any]) -> tuple:
        """Create datasets for training."""
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            indices=np.load(self.processed_dir / "train_indices.npy"),
            config=config,
            device=self.device,
            split_name="train"
        )
        
        val_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            indices=np.load(self.processed_dir / "val_indices.npy"),
            config=config,
            device=self.device,
            split_name="validation"
        )
        
        # For hyperparameter search, we typically don't use test set
        # to avoid overfitting hyperparameters to test performance
        return train_dataset, val_dataset
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Objective function for Optuna optimization.
        
        Args:
            trial: Optuna trial object
            
        Returns:
            Validation loss to minimize
        """
        # Log trial start
        self.logger.info(f"Starting trial {trial.number}")
        trial_start = time.time()
        
        # Suggest hyperparameters
        suggested_params = self._parse_search_space(
            trial, 
            self.optuna_config["search_space"]
        )
        
        # Update configuration
        trial_config = self._update_config_with_params(
            self.base_config,
            suggested_params
        )
        
        # Set seed for reproducibility
        seed = self.base_config["system"]["seed"] + trial.number
        seed_everything(seed)
        
        # Create model
        model = create_model(trial_config, self.device)
        
        # Create datasets
        train_dataset, val_dataset = self._create_datasets(trial_config)
        
        # Create trial-specific save directory
        trial_save_dir = self.study_dir / f"trial_{trial.number:04d}"
        trial_save_dir.mkdir(exist_ok=True)
        
        # Save trial configuration
        save_json(trial_config, trial_save_dir / "config.json")
        save_json(suggested_params, trial_save_dir / "suggested_params.json")
        
        # Initialize trainer with pruning callback
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=None,  # Don't use test set during hyperparameter search
            config=trial_config,
            save_dir=trial_save_dir,
            device=self.device
        )
        
        # Training with pruning
        best_val_loss = float('inf')
        pruned = False
        
        try:
            # Modified training loop to support pruning
            for epoch in range(1, trial_config["training"]["epochs"] + 1):
                trainer.current_epoch = epoch
                
                # Train one epoch
                train_loss, train_metrics = trainer._train_epoch()
                
                # Validate
                val_loss, val_metrics = trainer._validate()
                
                # Update scheduler
                if not trainer.scheduler_step_on_batch:
                    trainer.scheduler.step(val_loss)
                
                # Track best loss
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                
                # Report to Optuna and check for pruning
                trial.report(val_loss, epoch)
                
                if trial.should_prune():
                    self.logger.info(f"Trial {trial.number} pruned at epoch {epoch}")
                    pruned = True
                    raise optuna.TrialPruned()
                
                # Early stopping check
                if hasattr(trainer, 'patience_counter'):
                    if trainer.patience_counter >= trainer.early_stopping_patience:
                        self.logger.info(f"Trial {trial.number} early stopped at epoch {epoch}")
                        break
        
        except optuna.TrialPruned:
            pruned = True
            raise
        
        except Exception as e:
            self.logger.error(f"Trial {trial.number} failed: {e}")
            raise
        
        finally:
            # Save trial results
            trial_time = time.time() - trial_start
            results = {
                "trial_number": trial.number,
                "best_val_loss": best_val_loss,
                "pruned": pruned,
                "suggested_params": suggested_params,
                "trial_time": trial_time
            }
            save_json(results, trial_save_dir / "results.json")
            
            # Clean up GPU memory
            del model
            del trainer
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        return best_val_loss
    
    def optimize(self) -> optuna.Study:
        """
        Run hyperparameter optimization.
        
        Returns:
            Optuna study object
        """
        self.logger.info("="*80)
        self.logger.info("Starting Optuna hyperparameter optimization")
        self.logger.info(f"Study name: {self.optuna_config['study_name']}")
        self.logger.info(f"N trials: {self.optuna_config['n_trials']}")
        self.logger.info(f"Direction: {self.optuna_config['direction']}")
        
        # Create sampler
        sampler_type = self.optuna_config.get("sampler", "TPE")
        if sampler_type == "TPE":
            sampler = TPESampler(seed=self.base_config["system"]["seed"])
        elif sampler_type == "Random":
            sampler = RandomSampler(seed=self.base_config["system"]["seed"])
        elif sampler_type == "Grid":
            sampler = GridSampler(self._create_grid_search_space())
        else:
            raise ValueError(f"Unknown sampler: {sampler_type}")
        
        # Create pruner
        pruner_type = self.optuna_config.get("pruner", "MedianPruner")
        if pruner_type == "MedianPruner":
            pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=10)
        elif pruner_type == "HyperbandPruner":
            pruner = HyperbandPruner()
        elif pruner_type == "None" or pruner_type is None:
            pruner = None
        else:
            raise ValueError(f"Unknown pruner: {pruner_type}")
        
        # Create or load study
        study_path = self.study_dir / "study.db"
        storage = f"sqlite:///{study_path}"
        
        study = optuna.create_study(
            study_name=self.optuna_config["study_name"],
            direction=self.optuna_config["direction"],
            sampler=sampler,
            pruner=pruner,
            storage=storage,
            load_if_exists=True
        )
        
        # Run optimization
        optimization_start = time.time()
        
        study.optimize(
            self.objective,
            n_trials=self.optuna_config["n_trials"],
            n_jobs=self.optuna_config.get("n_jobs", 1),
            gc_after_trial=True
        )
        
        optimization_time = time.time() - optimization_start
        
        # Log results
        self.logger.info("="*80)
        self.logger.info("Optimization completed!")
        self.logger.info(f"Total time: {optimization_time/3600:.2f} hours")
        self.logger.info(f"Best trial: {study.best_trial.number}")
        self.logger.info(f"Best value: {study.best_value:.6f}")
        self.logger.info("Best parameters:")
        for param, value in study.best_params.items():
            self.logger.info(f"  {param}: {value}")
        
        # Save study results
        study_results = {
            "best_trial": study.best_trial.number,
            "best_value": study.best_value,
            "best_params": study.best_params,
            "n_trials": len(study.trials),
            "optimization_time": optimization_time
        }
        save_json(study_results, self.study_dir / "study_results.json")
        
        # Save best configuration
        best_config = self._update_config_with_params(
            self.base_config,
            study.best_params
        )
        save_json(best_config, self.study_dir / "best_config.json")
        
        return study
    
    def _create_grid_search_space(self) -> Dict[str, Any]:
        """Create grid search space from configuration."""
        grid_space = {}
        
        for param_path, param_config in self.optuna_config["search_space"].items():
            if param_config["type"] == "categorical":
                grid_space[param_path] = param_config["choices"]
            elif param_config["type"] == "fixed":
                grid_space[param_path] = [param_config["value"]]
            else:
                self.logger.warning(
                    f"Grid search only supports categorical parameters, "
                    f"skipping {param_path}"
                )
        
        return grid_space

===== /Users/imalsky/Desktop/Chemulator/src/main.py =====
#!/usr/bin/env python3

import os
os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'

"""
Main entry point for chemical kinetics neural network training.
Supports both standard training and hyperparameter optimization with Optuna.
Uses NPY shards for efficient data storage and loading.
"""

import json
import logging
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional
import os
import hashlib
import warnings
import platform

# Check for OpenMP library conflicts on macOS
if platform.system() == "Darwin" and "KMP_DUPLICATE_LIB_OK" not in os.environ:
    # Only set this on macOS where the issue commonly occurs
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    warnings.warn(
        "Setting KMP_DUPLICATE_LIB_OK=TRUE for macOS compatibility. "
        "This may mask underlying library conflicts."
    )

import torch
import numpy as np

# Local imports
from utils.hardware import setup_device, optimize_hardware
from utils.utils import (
    setup_logging, 
    seed_everything, 
    ensure_directories,
    load_json_config,
    save_json
)
from data.preprocessor import DataPreprocessor
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer

# Only suppress specific known warnings
warnings.filterwarnings("ignore", message=".*torch.compile.*", category=UserWarning)
warnings.filterwarnings("ignore", message=".*flash attention.*", category=UserWarning)


def compute_config_hash(config: Dict[str, Any]) -> str:
    """
    Compute a hash of the configuration for cache validation.
    """
    # Extract all parts that affect data preprocessing
    relevant_config = {
        "raw_data_files": sorted(config["paths"]["raw_data_files"]),  # Sort for consistency
        "data": config["data"],
        "normalization": config["normalization"],
        "training": {
            "val_fraction": config["training"]["val_fraction"],
            "test_fraction": config["training"]["test_fraction"],
            "use_fraction": config["training"]["use_fraction"]
        }
    }
    
    # Convert to JSON string for hashing
    config_str = json.dumps(relevant_config, sort_keys=True)
    
    # Compute SHA256 hash
    return hashlib.sha256(config_str.encode()).hexdigest()[:16]


class ChemicalKineticsPipeline:
    """
    Complete training pipeline for chemical kinetics prediction.
    Uses NPY shards for optimal performance.
    """
    
    def __init__(self, config_path: Path):
        """Initialize the pipeline with configuration."""
        # Load configuration
        self.config = load_json_config(config_path)
        
        # Setup paths
        self.setup_paths()
        
        # Setup logging
        log_file = self.log_dir / f"pipeline_{time.strftime('%Y%m%d_%H%M%S')}.log"
        setup_logging(log_file=log_file)
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("="*80)
        self.logger.info("Chemical Kinetics Neural Network Pipeline")
        self.logger.info(f"Configuration: {config_path}")
        
        # Set random seed for reproducibility
        seed = self.config["system"]["seed"]
        seed_everything(seed)
        
        # Setup hardware
        self.device = setup_device()
        optimize_hardware(self.config["system"], self.device)
        
        self.logger.info("Pipeline initialized with NPY shards for high-performance data loading")
        
    def setup_paths(self):
        """Create directory structure and setup paths."""
        paths = self.config["paths"]
        
        # Create run-specific directory
        model_type = self.config["model"]["type"]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.run_save_dir = Path(paths["model_save_dir"]) / f"trained_model_{model_type}_{timestamp}"
        
        # Convert to Path objects
        self.raw_data_files = [Path(f) for f in paths["raw_data_files"]]
        
        # Use environment variable if set, otherwise use config
        self.processed_dir = Path(os.getenv("PROCESSED_DATA_DIR", paths["processed_data_dir"]))
        self.log_dir = Path(paths["log_dir"])
        
        # Create directories
        ensure_directories(self.processed_dir, self.run_save_dir, self.log_dir)
        
        # Define processed data paths
        self.normalization_file = self.processed_dir / "normalization.json"
        self.config_hash_file = self.processed_dir / "config_hash.txt"
        self.shard_index_file = self.processed_dir / "shard_index.json"

    def preprocess_data(self) -> bool:
        """
        Pre-process raw files to normalized NPY shards.
        """
        cfg_hash = compute_config_hash(self.config)
        
        # Expanded cache check: verify all required files
        split_files = [
            self.processed_dir / "train_indices.npy",
            self.processed_dir / "val_indices.npy",
            self.processed_dir / "test_indices.npy"
        ]
        
        cache_ok = (
            self.shard_index_file.exists()
            and self.normalization_file.exists()
            and self.config_hash_file.exists()
            and all(f.exists() for f in split_files)  # New: check split files
            and self.config_hash_file.read_text().strip() == cfg_hash
        )
        
        if cache_ok:
            self.logger.info("Using cached NPY shards and normalization.")
            return False
        
        # If cache invalid or incomplete, log why and reprocess
        if not cache_ok:
            missing_files = [str(f) for f in [self.shard_index_file, self.normalization_file, self.config_hash_file] + split_files if not f.exists()]
            if missing_files:
                self.logger.warning(f"Cache incomplete: missing files {missing_files}. Reprocessing data.")
            else:
                self.logger.info("Config hash mismatch. Reprocessing data.")
        
        # Check raw files exist
        missing = [p for p in self.raw_data_files if not p.exists()]
        if missing:
            raise FileNotFoundError(f"Missing raw data files: {missing}")
        
        # Process to NPY shards with integrated normalization
        self.logger.info("Starting NPY shard creation from raw files with normalization...")
        
        preprocessor = DataPreprocessor(
            raw_files=self.raw_data_files,
            output_dir=self.processed_dir,
            config=self.config
        )
        
        split_indices = preprocessor.process_to_npy_shards()
        
        # Check for errors or empty data
        total_samples = sum(len(indices) for indices in split_indices.values())
        if total_samples == 0:
            self.logger.error("No valid data processed from raw files. Exiting.")
            sys.exit(1)
        
        # Save config hash
        self.config_hash_file.write_text(cfg_hash)
        
        self.logger.info("Pre-processing complete.")
        return True

    def train_model(self):
        """Train the neural network model."""
        self.logger.info("Starting model training...")
        
        # Save the exact config used for this run
        save_json(self.config, self.run_save_dir / "run_config.json")

        # Load normalization stats (for potential denorm in inference)
        with open(self.normalization_file, 'r') as f:
            norm_stats = json.load(f)
        
        # Create model
        model = create_model(self.config, self.device)
        
        # Log model info
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"Model: {self.config['model']['type'].upper()}")
        self.logger.info(f"Parameters: {total_params:,}")
        
        # Create datasets
        self.logger.info("Creating NPY datasets for high-performance loading...")
        
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            indices=np.load(self.processed_dir / "train_indices.npy"),
            config=self.config,
            device=self.device,
            split_name="train"
        )
        
        val_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            indices=np.load(self.processed_dir / "val_indices.npy"),
            config=self.config,
            device=self.device,
            split_name="validation"
        )
        
        test_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            indices=np.load(self.processed_dir / "test_indices.npy"),
            config=self.config,
            device=self.device,
            split_name="test"
        )
        
        # Log dataset info
        train_info = train_dataset.get_batch_info()
        self.logger.info(f"NPY Dataset info: {train_info}")
        
        # Initialize trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device
        )
        
        # Train model
        best_val_loss = trainer.train()
        
        # Evaluate on test set
        test_loss = trainer.evaluate_test()
        
        self.logger.info(f"Training complete! Best validation loss: {best_val_loss:.6f}")
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        # Save final results
        results = {
            "config_path": str(self.run_save_dir / "run_config.json"),
            "best_val_loss": best_val_loss,
            "test_loss": test_loss,
            "model_path": str(self.run_save_dir / "best_model.pt"),
            "training_time": trainer.total_training_time,
            "data_format": "npy_shards"
        }
        
        save_json(results, self.run_save_dir / "results.json")
    
    def optimize_hyperparameters(self):
        """Run hyperparameter optimization using Optuna."""
        if not self.config["optuna"]["enabled"]:
            self.logger.info("Optuna optimization not enabled in config")
            return
        
        # Import here to avoid dependency if not using Optuna
        try:
            from optuna_optimization import OptunaOptimizer
        except ImportError:
            self.logger.error("Optuna not installed. Install with: pip install optuna")
            sys.exit(1)
        
        self.logger.info("Starting hyperparameter optimization with Optuna...")
        
        # Ensure data is preprocessed
        self.preprocess_data()
        
        # Create optimizer
        optimizer = OptunaOptimizer(
            base_config=self.config,
            processed_dir=self.processed_dir,
            save_dir=self.run_save_dir,
            device=self.device
        )
        
        # Run optimization
        study = optimizer.optimize()
        
        # Train final model with best parameters
        self.logger.info("Training final model with best hyperparameters...")
        
        # Load best config
        best_config_path = self.run_save_dir / "optuna_study" / "best_config.json"
        self.config = load_json_config(best_config_path)
        
        # Train with best config
        self.train_model()
    
    def run(self, mode: str = "train"):
        """
        Execute the pipeline.
        
        Args:
            mode: "train" for standard training, "optimize" for hyperparameter optimization
        """
        try:
            # Step 1: Preprocess data (if needed)
            self.preprocess_data()
            
            # Step 2: Train or optimize
            if mode == "optimize":
                self.optimize_hyperparameters()
            else:
                self.train_model()
            
            self.logger.info("="*80)
            self.logger.info("Pipeline completed successfully!")
            self.logger.info(f"Results saved in: {self.run_save_dir}")
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            sys.exit(1)


def main():
    """Main entry point."""
    import argparse
    parser = argparse.ArgumentParser(
        description="Chemical Kinetics Neural Network Training Pipeline"
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/config.jsonc"),
        help="Path to configuration file (default: config/config.jsonc)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["train", "optimize"],
        default="train",
        help="Mode: train (standard training) or optimize (hyperparameter optimization)"
    )
    
    args = parser.parse_args()
    
    # Validate config path
    if not args.config.exists():
        print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
        sys.exit(1)
    
    # Check for json5 if using .jsonc file
    if args.config.suffix == ".jsonc":
        try:
            import json5
        except ImportError:
            print("Error: JSON with comments (.jsonc) requires json5 package", file=sys.stderr)
            print("Install it with: pip install json5", file=sys.stderr)
            sys.exit(1)
    
    # Run pipeline
    pipeline = ChemicalKineticsPipeline(args.config)
    pipeline.run(mode=args.mode)


if __name__ == "__main__":
    main()

===== /Users/imalsky/Desktop/Chemulator/src/training/trainer.py =====
#!/usr/bin/env python3
"""
Optimized training pipeline for chemical kinetics models.

Features:
- Mixed precision training with BFloat16 on A100
- Gradient accumulation for large effective batch sizes
- Advanced learning rate scheduling
- Memory-efficient training with periodic cache clearing
- Comprehensive logging
- Compatible with all device types (CUDA, MPS, CPU)
- Full CUDA graph support with proper step marking
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple
import sys

import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau
from torch.utils.data import DataLoader

from data.dataset import NPYDataset, create_dataloader
from models.model import export_model  # Updated to modern export
from data.device_prefetch import DevicePrefetchLoader


# Training constants
DEFAULT_BETAS = (0.9, 0.999)
DEFAULT_EPS = 1e-8
LOGGING_INTERVAL_SECONDS = 30.0
SCHEDULER_PLATEAU_FACTOR = 0.5
SCHEDULER_PLATEAU_PATIENCE = 10
SCHEDULER_PLATEAU_MIN_LR = 1e-7
DEFAULT_EMPTY_CACHE_INTERVAL = 999999


class Trainer:
    """
    Optimised trainer for chemical-kinetics networks.
    """
    def __init__(               
        self,
        model: nn.Module,
        train_dataset: NPYDataset,
        val_dataset: NPYDataset,
        test_dataset: NPYDataset,
        config: Dict[str, Any],
        save_dir: Path,
        device: torch.device,
    ):
        self.logger = logging.getLogger(__name__)

        self.model          = model
        self.config         = config
        self.save_dir       = save_dir
        self.device         = device
        self.train_config   = config["training"]
        self.system_config  = config["system"]

        # Data loaders
        base_train = create_dataloader(train_dataset, config, shuffle=True,  device=device)
        base_val   = create_dataloader(val_dataset,   config, shuffle=False, device=device)
        base_test  = create_dataloader(test_dataset,  config, shuffle=False, device=device)

        self.logger.info("Using pre-normalized data - direct GPU streaming enabled")
        
        try:
            from data.device_prefetch import DevicePrefetchLoader
            use_prefetch = device.type == "cuda"
        except ModuleNotFoundError:
            self.logger.warning("DevicePrefetchLoader not found - falling back to plain loaders")
            use_prefetch = False

        if use_prefetch:
            self.train_loader = DevicePrefetchLoader(base_train, device)
            self.val_loader = DevicePrefetchLoader(base_val, device)
            self.test_loader = DevicePrefetchLoader(base_test, device)
            self.logger.info("GPU pre-fetch enabled")
        else:
            self.train_loader = base_train
            self.val_loader = base_val
            self.test_loader = base_test

        # Misc setup
        self.log_interval            = self.train_config.get("log_interval", 10)
        self.current_epoch           = 0
        self.global_step             = 0
        self.best_val_loss           = float("inf")
        self.best_epoch              = -1
        self.total_training_time     = 0
        self.patience_counter        = 0
        self.early_stopping_patience = self.train_config["early_stopping_patience"]
        self.min_delta               = self.train_config["min_delta"]
        self.empty_cache_interval    = self.system_config.get("empty_cache_interval", DEFAULT_EMPTY_CACHE_INTERVAL)

        self.log_file         = self.save_dir / "training_log.json"
        self.training_history = {"config": config, "epochs": []}
        self.max_history_epochs     = 1_000
        self.history_save_interval  = 100

        self._setup_optimizer()
        self._setup_scheduler()
        self._setup_loss()
        self._setup_amp()
        self._cuda_graphs_enabled = False
        self._check_cuda_graphs_compatibility()
        
    def _check_cuda_graphs_compatibility(self):
        """Check if we should use CUDA graphs based on the configuration and device."""
        if (self.device.type == "cuda" and 
            self.system_config.get("use_torch_compile", False) and 
            self.train_config.get("use_amp", False) and
            self.system_config.get("compile_mode") == "max-autotune"):
            self._cuda_graphs_enabled = True
            self.logger.info("CUDA graphs compatibility mode enabled")
            
            # Import the cudagraph marking function with fallback
            try:
                from torch._inductor import cudagraph_mark_step_begin
                self.cudagraph_mark_step = cudagraph_mark_step_begin
            except ImportError:
                try:
                    from torch._dynamo import mark_step_begin
                    self.cudagraph_mark_step = mark_step_begin
                except ImportError:
                    self.logger.warning("CUDA graph step marking not available, disabling")
                    self._cuda_graphs_enabled = False
        else:
            self._cuda_graphs_enabled = False
    
    def _setup_optimizer(self):
        """Setup AdamW optimizer with weight decay."""
        # Separate parameters for weight decay
        decay_params = []
        no_decay_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            # Don't apply weight decay to biases, layer norm, and embeddings
            if param.dim() == 1 or "bias" in name or "norm" in name.lower() or "embed" in name.lower():
                no_decay_params.append(param)
            else:
                decay_params.append(param)
        
        param_groups = [
            {"params": decay_params, "weight_decay": self.train_config["weight_decay"]},
            {"params": no_decay_params, "weight_decay": 0.0}
        ]
        
        self.optimizer = AdamW(
            param_groups,
            lr=self.train_config["learning_rate"],
            betas=DEFAULT_BETAS,
            eps=DEFAULT_EPS
        )
        
        self.logger.info(
            f"Optimizer: AdamW with lr={self.train_config['learning_rate']:.2e}, "
            f"weight_decay={self.train_config['weight_decay']:.2e}"
        )
    
    def _setup_scheduler(self):
        """Setup learning rate scheduler."""
        scheduler_type = self.train_config["scheduler"]
        
        if scheduler_type == "cosine":
            params = self.train_config["scheduler_params"]
            
            # Calculate T_0 in steps - ensure at least 1
            steps_per_epoch = max(1, len(self.train_loader) // self.train_config["gradient_accumulation_steps"])
            T_0 = max(1, params["T_0"] * steps_per_epoch)
            
            self.scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=T_0,
                T_mult=params["T_mult"],
                eta_min=params["eta_min"]
            )
            self.scheduler_step_on_batch = True
            
        elif scheduler_type == "plateau":
            params = self.train_config.get("scheduler_params", {})
            
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                factor=params.get("factor", SCHEDULER_PLATEAU_FACTOR),
                patience=params.get("patience", SCHEDULER_PLATEAU_PATIENCE),
                min_lr=params.get("min_lr", SCHEDULER_PLATEAU_MIN_LR)
            )
            self.scheduler_step_on_batch = False
        
        else:
            raise ValueError(f"Unknown scheduler: {scheduler_type}")
        
        self.logger.info(f"Scheduler: {scheduler_type}")
    
    def _setup_loss(self):
        """Setup loss function."""
        loss_type = self.train_config["loss"]
        
        if loss_type == "mse":
            self.criterion = nn.MSELoss()
        elif loss_type == "huber":
            self.criterion = nn.HuberLoss(delta=self.train_config["huber_delta"])
        else:
            raise ValueError(f"Unknown loss: {loss_type}")
        
        self.logger.info(f"Loss function: {loss_type}")
    
    def _setup_amp(self):
        self.use_amp = self.train_config.get("use_amp", False) and self.device.type in ("cuda", "mps")
        self.scaler = None
        self.amp_dtype = None

        if not self.use_amp:
            self.logger.info("AMP disabled")
            return

        dtype_str = str(self.train_config.get("amp_dtype", "float16")).lower()
        if dtype_str not in ("float16", "bfloat16"):
            raise ValueError(f"Invalid amp_dtype '{dtype_str}' – choose 'float16' or 'bfloat16'.")

        self.amp_dtype = torch.bfloat16 if dtype_str == "bfloat16" else torch.float16

        # GradScaler only meaningful for fp16 on CUDA
        if self.amp_dtype is torch.float16 and self.device.type == "cuda":
            self.scaler = GradScaler()
        elif self.device.type == "mps" and self.amp_dtype is torch.float16:
            self.logger.warning("GradScaler not supported on MPS – continuing without it.")

        self.logger.info("AMP enabled (dtype=%s, device=%s)", dtype_str, self.device.type)
        
    def train(self) -> float:
        """
        Execute the training loop.
        
        Returns:
            Best validation loss achieved
        """
        self.logger.info("Starting training...")
        self.logger.info(f"Train batches: {len(self.train_loader)}")
        self.logger.info(f"Val batches: {len(self.val_loader)}")
        
        if len(self.train_loader) == 0:
            self.logger.error("Training dataset is empty! Check data splits.")
            sys.exit(1)
        
        try:
            for epoch in range(1, self.train_config["epochs"] + 1):
                self.current_epoch = epoch
                epoch_start = time.time()
                
                # Training phase
                train_loss, train_metrics = self._train_epoch()
                
                # Validation phase
                val_loss, val_metrics = self._validate()
                
                # Update scheduler
                if not self.scheduler_step_on_batch:
                    self.scheduler.step(val_loss)
                
                # Track time
                epoch_time = time.time() - epoch_start
                self.total_training_time += epoch_time
                
                # Log results
                self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)
                
                # Check for improvement
                if val_loss < self.best_val_loss - self.min_delta:
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1
                
                # Early stopping
                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            
            self.logger.info(
                f"Training completed. Best validation loss: {self.best_val_loss:.6f} "
                f"at epoch {self.best_epoch}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
        
        except Exception as e:
            self.logger.error(f"Training failed: {e}", exc_info=True)
            raise
        
        finally:
            # Save training history
            with open(self.log_file, 'w') as f:
                json.dump(self.training_history, f, indent=2)
        
        return self.best_val_loss
    
    def _train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """
        One training epoch with gradient accumulation.
        Ensures all batches contribute to gradient updates.
        """
        self.model.train()
        total_loss, total_samples = 0.0, 0
        grad_norm_ema = 0.0
        ema_decay = 0.98
        accumulation_steps = self.train_config["gradient_accumulation_steps"]
        last_log_time = time.time()
        accumulated_loss = 0.0
        accumulated_batches = 0

        if self._cuda_graphs_enabled and hasattr(self, "cudagraph_mark_step"):
            self.cudagraph_mark_step()

        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            with autocast(enabled=self.use_amp, device_type=self.device.type, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets) / accumulation_steps

            (self.scaler.scale(loss) if self.scaler else loss).backward()
            accumulated_loss += loss.item() * accumulation_steps
            accumulated_batches += 1

            # Perform optimizer step at accumulation boundary or last batch
            is_accumulation_boundary = (batch_idx + 1) % accumulation_steps == 0
            is_last_batch = (batch_idx + 1) == len(self.train_loader)
            
            if is_accumulation_boundary or is_last_batch:
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)

                grad = nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.train_config["gradient_clip"]
                )
                grad_norm_ema = ema_decay * grad_norm_ema + (1 - ema_decay) * float(grad)

                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()

                self.optimizer.zero_grad(set_to_none=True)
                
                # Only step scheduler on full accumulation boundaries
                if self.scheduler_step_on_batch and is_accumulation_boundary:
                    self.scheduler.step()
                    
                self.global_step += 1

                if self._cuda_graphs_enabled and hasattr(self, "cudagraph_mark_step"):
                    self.cudagraph_mark_step()

                # Reset accumulation tracking
                accumulated_loss = 0.0
                accumulated_batches = 0

            # Track total loss
            batch_loss = loss.item() * accumulation_steps
            total_loss += batch_loss * inputs.size(0)
            total_samples += inputs.size(0)

            # Progress logging
            if time.time() - last_log_time > LOGGING_INTERVAL_SECONDS:
                pct = 100.0 * (batch_idx + 1) / len(self.train_loader)
                self.logger.info(
                    f"Epoch {self.current_epoch:03d} "
                    f"{pct:5.1f}%  Loss {batch_loss:.4e}"
                )
                last_log_time = time.time()

            # Memory management for large batches
            if self.device.type == 'cuda' and (batch_idx + 1) % self.empty_cache_interval == 0:
                torch.cuda.empty_cache()

        avg_loss = total_loss / total_samples if total_samples else float("inf")
        metrics = {
            "grad_norm_ema": grad_norm_ema,
            "learning_rate": self.optimizer.param_groups[0]["lr"],
        }
        
        return avg_loss, metrics
    
    @torch.no_grad()
    def _run_eval_loop(self, loader: DataLoader, enable_logging: bool = True) -> Tuple[float, Dict[str, float]]:
        """
        Run evaluation loop on any dataset with proper empty loader handling.
        
        Args:
            loader: DataLoader to evaluate
            enable_logging: Whether to enable progress logging
            
        Returns:
            Tuple of (average loss, metrics dict)
        """
        # Check for empty loader
        if len(loader) == 0:
            self.logger.error("Evaluation DataLoader is empty!")
            raise ValueError("Cannot evaluate on empty DataLoader. Check data splits and filtering.")
        
        self.model.eval()
        
        total_loss = 0.0
        total_samples = 0
        
        # Time tracking
        eval_start = time.time()
        last_log_time = eval_start
        
        for batch_idx, (inputs, targets) in enumerate(loader):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)
            
            if self.use_amp:
                with autocast(device_type=self.device.type, dtype=self.amp_dtype):
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, targets)
            else:
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)
            
            total_loss += loss.item() * inputs.size(0)
            total_samples += inputs.size(0)
            
            if enable_logging:
                current_time = time.time()
                if current_time - last_log_time > LOGGING_INTERVAL_SECONDS:
                    progress = (batch_idx + 1) / len(loader) * 100
                    elapsed = current_time - eval_start
                    self.logger.info(
                        f"Epoch {self.current_epoch:03d} Val: {progress:.1f}% "
                        f"({batch_idx+1}/{len(loader)} batches), "
                        f"Elapsed: {elapsed:.1f}s"
                    )
                    last_log_time = current_time
        
        elapsed_total = time.time() - eval_start
        if enable_logging:
            self.logger.info(f"Evaluation completed in {elapsed_total:.1f}s")
        
        if total_samples == 0:
            self.logger.error("No samples processed during evaluation!")
            raise ValueError("Evaluation processed zero samples. Check batch processing.")
            
        avg_loss = total_loss / total_samples
            
        return avg_loss, {}
        
    def _validate(self) -> Tuple[float, Dict[str, float]]:
        """Validate the model during training."""
        return self._run_eval_loop(self.val_loader, enable_logging=True)
    
    @torch.no_grad()
    def evaluate_test(self) -> float:
        """Evaluate on test set."""
        self.logger.info("Evaluating on test set...")
        
        # Load best model
        checkpoint_path = self.save_dir / "best_model.pt"
        if checkpoint_path.exists():
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.logger.info(f"Loaded best model from epoch {checkpoint['epoch']}")
        
        test_loss, _ = self._run_eval_loop(self.test_loader, enable_logging=False)
        
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        return test_loss
    
    def _save_best_model(self):
        """Save best model checkpoint."""
        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "best_val_loss": self.best_val_loss,
            "config": self.config,
            "training_time": self.total_training_time
        }
        
        path = self.save_dir / "best_model.pt"
        torch.save(checkpoint, path)
        self.logger.info(f"Saved best model (epoch {self.current_epoch})")
        
        # Also export model if requested
        if self.system_config.get("save_jit_model", False):
            self._export_model()
    
    def _export_model(self):
        """Export model using modern torch.export with improved error handling."""
        try:
            if len(self.val_loader) == 0:
                self.logger.warning("Validation loader is empty, trying train loader for export")
                if len(self.train_loader) == 0:
                    self.logger.warning("Train loader is also empty, skipping export")
                    return
                loader_to_use = self.train_loader
            else:
                loader_to_use = self.val_loader
            
            example_batch = next(iter(loader_to_use))
            example_input = example_batch[0][:1].to(self.device)  # Single sample
            
            export_path = self.save_dir / "best_model_exported.pt"
            export_model(self.model, example_input, export_path)
            
            example_path = self.save_dir / "export_example_input.pt"
            torch.save(example_input, example_path)
            self.logger.info(f"Saved export example input to {example_path}")
            
        except Exception as e:
            self.logger.warning(f"Model export failed: {e}")
            self.logger.info("Model saved in standard format, export can be done manually later")
    
    def _log_epoch(        
        self,
        train_loss: float,
        val_loss: float,
        train_metrics: Dict[str, float],
        val_metrics: Dict[str, float],
        epoch_time: float,
    ):
        improvement = self.best_val_loss - val_loss
        grad_norm   = train_metrics.get("grad_norm_ema", 0.0)

        msg = (f"Epoch {self.current_epoch:03d} | "
               f"Train {train_loss:.4e} | Val {val_loss:.4e} | "
               f"LR {train_metrics['learning_rate']:.2e} | "
               f"Grad {grad_norm:.3e} | Time {epoch_time:.1f}s")
        if improvement > 0:
            msg += f" | ▲ {improvement:.4e}"
        self.logger.info(msg)

        self.training_history["epochs"].append(
            {
                "epoch": self.current_epoch,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "grad_norm": grad_norm,
                "lr": train_metrics["learning_rate"],
                "time_s": epoch_time,
                "improvement": max(improvement, 0.0),
            }
        )

        if len(self.training_history["epochs"]) > self.max_history_epochs:
            self.training_history["epochs"] = self.training_history["epochs"][-self.max_history_epochs :]

        if self.current_epoch % self.history_save_interval == 0:
            try:
                with open(self.log_file, "w") as f:
                    json.dump(self.training_history, f, indent=2)
            except Exception as exc:
                self.logger.warning(f"Failed to save history: {exc}")

===== /Users/imalsky/Desktop/Chemulator/src/utils/utils.py =====
#!/usr/bin/env python3
"""
General utility functions for the chemical kinetics pipeline.

Provides helpers for:
- Configuration management with JSON5 support
- Logging setup
- Random seed control
- File I/O operations
- Data validation
"""

import json
import logging
import os
import random
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import numpy as np
import torch


def setup_logging(
    level: int = logging.INFO,
    log_file: Optional[Path] = None,
    format_string: Optional[str] = None
) -> None:
    """
    Configure logging for the application.
    
    Args:
        level: Logging level (default: INFO)
        log_file: Optional file path for logging
        format_string: Custom format string for log messages
    """
    # Default format
    if format_string is None:
        format_string = "%(asctime)s | %(levelname)-8s | %(name)s - %(message)s"
    
    # Remove all existing handlers from all loggers
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Also clean up any child logger handlers to prevent duplication
    for logger_name in logging.root.manager.loggerDict:
        logger = logging.getLogger(logger_name)
        logger.handlers.clear()
        logger.propagate = True
    
    # Configure root logger
    root_logger.setLevel(level)
    
    # Create formatter
    formatter = logging.Formatter(format_string, datefmt="%Y-%m-%d %H:%M:%S")
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(level)
    root_logger.addHandler(console_handler)
    
    # File handler (optional)
    if log_file is not None:
        try:
            # Ensure directory exists
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Create file handler
            file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
            file_handler.setFormatter(formatter)
            file_handler.setLevel(level)
            root_logger.addHandler(file_handler)
            
            print(f"Logging to file: {log_file}")
        except Exception as e:
            print(f"Failed to setup file logging: {e}", file=sys.stderr)
    
    # Log initial message
    logger = logging.getLogger(__name__)
    logger.info("Logging system initialized")


def seed_everything(seed: int) -> None:
    """
    Set random seeds for reproducibility across all libraries.
    
    Args:
        seed: Random seed value
    """
    # Python random
    random.seed(seed)
    
    # Numpy
    np.random.seed(seed)
    
    # PyTorch
    torch.manual_seed(seed)
    
    # Only set CUDA seed if CUDA is available
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # Environment variable for hash randomization
    os.environ["PYTHONHASHSEED"] = str(seed)
    
    # For better reproducibility (optional - may impact performance)
    # torch.use_deterministic_algorithms(True)
    # if torch.cuda.is_available():
    #     torch.backends.cudnn.deterministic = True
    #     torch.backends.cudnn.benchmark = False
    
    logger = logging.getLogger(__name__)
    logger.info(f"Random seed set to {seed}")


def load_json_config(path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load JSON configuration file with validation.
    
    Supports both standard JSON and JSON with comments (using json5 if available).
    
    Args:
        path: Path to configuration file
        
    Returns:
        Configuration dictionary
        
    Raises:
        FileNotFoundError: If configuration file doesn't exist
        ValueError: If configuration is invalid
        ImportError: If json5 is needed but not installed
    """
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"Configuration file not found: {path}")
    
    # Try json5 first for comment support
    try:
        import json5
        with open(path, 'r', encoding='utf-8') as f:
            config = json5.load(f)
    except ImportError:
        # Check if file likely contains comments
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
            if '//' in content or '/*' in content:
                raise ImportError(
                    "Configuration file appears to contain comments but json5 is not installed.\n"
                    "Install it with: pip install json5"
                )
        
        # Fallback to standard json
        with open(path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    # Validate configuration
    validate_config(config)
    
    return config


def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate configuration dictionary structure.
    
    Args:
        config: Configuration dictionary to validate
        
    Raises:
        ValueError: If configuration is invalid
    """
    # Required top-level sections
    required_sections = ["paths", "data", "normalization", "model", "training", "system"]
    
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required configuration section: '{section}'")
    
    # Validate paths
    paths = config["paths"]
    if not isinstance(paths.get("raw_data_files"), list):
        raise ValueError("'paths.raw_data_files' must be a list")
    
    # Validate data specification
    data = config["data"]
    if not isinstance(data.get("species_variables"), list) or not data["species_variables"]:
        raise ValueError("'data.species_variables' must be a non-empty list")
    
    if not isinstance(data.get("global_variables"), list):
        raise ValueError("'data.global_variables' must be a list")
    
    # Enhanced validation: Check variable names
    all_variables = data["species_variables"] + data["global_variables"] + [data.get("time_variable", "t_time")]
    duplicates = []
    seen = set()
    
    for var in all_variables:
        # Check for duplicates
        if var in seen:
            duplicates.append(var)
        seen.add(var)
    
    if duplicates:
        raise ValueError(f"Duplicate variable names found: {duplicates}")
    
    # Validate model configuration
    model = config["model"]
    if model.get("type") not in ["siren", "resnet", "deeponet"]:
        raise ValueError("'model.type' must be either 'siren', 'resnet', or 'deeponet'")
    
    # Validate model-specific parameters
    model_type = model.get("type")
    
    if model_type in ["siren", "resnet"]:
        # These models require hidden_dims
        if not isinstance(model.get("hidden_dims"), list) or not model["hidden_dims"]:
            raise ValueError("'model.hidden_dims' must be a non-empty list for siren and resnet models")
    
    elif model_type == "deeponet":
        # DeepONet requires branch_layers, trunk_layers, and basis_dim
        if not isinstance(model.get("branch_layers"), list) or not model["branch_layers"]:
            raise ValueError("'model.branch_layers' must be a non-empty list for deeponet model")
        
        if not isinstance(model.get("trunk_layers"), list) or not model["trunk_layers"]:
            raise ValueError("'model.trunk_layers' must be a non-empty list for deeponet model")
        
        if not isinstance(model.get("basis_dim"), int) or model["basis_dim"] <= 0:
            raise ValueError("'model.basis_dim' must be a positive integer for deeponet model")
        
        # Validate activation function if specified
        if "activation" in model:
            valid_activations = {"gelu", "relu", "tanh"}
            if model["activation"] not in valid_activations:
                raise ValueError(f"'model.activation' must be one of {valid_activations}")
    
    # Validate training parameters
    training = config["training"]
    
    # Check numeric parameters
    numeric_params = {
        "batch_size": (1, None),
        "epochs": (1, None),
        "learning_rate": (0, 1),
        "gradient_clip": (0, None),
        "val_fraction": (0, 1),
        "test_fraction": (0, 1)
    }
    
    for param, (min_val, max_val) in numeric_params.items():
        value = training.get(param)
        if value is None:
            raise ValueError(f"Missing training parameter: '{param}'")
        
        if not isinstance(value, (int, float)):
            raise ValueError(f"'{param}' must be numeric")
        
        if min_val is not None and value < min_val:
            raise ValueError(f"'{param}' must be >= {min_val}")
        
        if max_val is not None and value > max_val:
            raise ValueError(f"'{param}' must be <= {max_val}")
    
    # Check split fractions sum
    if training["val_fraction"] + training["test_fraction"] >= 1.0:
        raise ValueError("Sum of val_fraction and test_fraction must be < 1.0")
    
    # Validate normalization methods
    norm_config = config["normalization"]
    valid_methods = {"standard", "log-standard", "log-min-max", "symlog", "none"}
    
    default_method = norm_config.get("default_method")
    if default_method not in valid_methods:
        raise ValueError(f"Invalid default normalization method: {default_method}")
    
    # Check per-variable methods
    methods = norm_config.get("methods", {})
    for var, method in methods.items():
        if method not in valid_methods:
            raise ValueError(f"Invalid normalization method for '{var}': {method}")


def save_json(
    data: Dict[str, Any],
    path: Union[str, Path],
    indent: int = 2
) -> None:
    """
    Save dictionary to JSON file with pretty printing.
    
    Args:
        data: Dictionary to save
        path: Output file path
        indent: JSON indentation level
    """
    path = Path(path)
    
    # Ensure directory exists
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Custom encoder for special types
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.Tensor):
                return obj.cpu().numpy().tolist()
            elif isinstance(obj, Path):
                return str(obj)
            elif isinstance(obj, datetime):
                return obj.isoformat()
            return super().default(obj)
    
    # Write file
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, cls=NumpyEncoder)


def load_json(path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load JSON file.
    
    Args:
        path: Path to JSON file
        
    Returns:
        Loaded dictionary
    """
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def ensure_directories(*paths: Union[str, Path]) -> None:
    """
    Create directories if they don't exist with thread safety.
    
    Args:
        *paths: Variable number of directory paths to create
    """
    for path in paths:
        Path(path).mkdir(parents=True, exist_ok=True)

===== /Users/imalsky/Desktop/Chemulator/src/utils/hardware.py =====
#!/usr/bin/env python3
"""
Hardware detection and optimization utilities.

This module provides functions to:
- Detect and configure available compute devices
- Optimize settings for specific hardware (especially A100)
- Configure memory and computation settings
"""

import logging
import os
from typing import Dict, Any, Optional

import torch
import numpy as np

# Hardware constants
DEFAULT_CUDA_ALLOC_MB = 512
DEFAULT_THREAD_RATIO = 0.5
MIN_WORKERS = 4
MAX_WORKERS = 8
MIN_CPU_WORKERS = 2
MPS_WORKERS = 0
MEMORY_BUFFER_RATIO = 0.9
DEFAULT_PREFETCH_FACTOR = 2
MAX_PREFETCH_FACTOR = 16  # PyTorch hard limit
MIN_PREFETCH_FACTOR = 2   # PyTorch minimum

# Compute capability thresholds
AMPERE_COMPUTE_CAPABILITY = 8  # For TF32, BFloat16, Flash Attention


def setup_device() -> torch.device:
    """
    Detect and configure the best available compute device.
    
    Priority order:
    1. CUDA (NVIDIA GPUs)
    2. MPS (Apple Silicon)
    3. CPU
    
    Returns:
        Configured torch.device
    """
    logger = logging.getLogger(__name__)
    
    if torch.cuda.is_available():
        # CUDA available
        device = torch.device("cuda")
        
        # Log GPU information
        gpu_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1e9
        
        logger.info(f"Using CUDA device: {gpu_name}")
        logger.info(f"GPU memory: {gpu_memory:.1f} GB")
        logger.info(f"Number of GPUs: {gpu_count}")
        
        # Set default GPU
        torch.cuda.set_device(current_device)
        
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        # Apple Silicon GPU
        device = torch.device("mps")
        logger.info("Using Apple Silicon MPS device")
        logger.warning("MPS backend has limited operator support")
        logger.warning("torch.compile is disabled for MPS devices")
        
    else:
        # CPU fallback
        device = torch.device("cpu")
        logger.info("Using CPU device")
        logger.warning("Training will be significantly slower on CPU")
        
        # Log CPU info
        cpu_count = os.cpu_count()
        logger.info(f"CPU cores: {cpu_count}")
    
    return device


def optimize_hardware(config: Dict[str, Any], device: torch.device) -> None:
    """
    Apply hardware-specific optimizations based on configuration.
    
    Args:
        config: System configuration dictionary
        device: The device being used
    """
    logger = logging.getLogger(__name__)
    
    # Disable torch.compile for MPS devices
    if device.type == "mps" and config.get("use_torch_compile", False):
        logger.warning("Disabling torch.compile for MPS device due to compatibility issues")
        config["use_torch_compile"] = False
    
    # CUDA optimizations
    if torch.cuda.is_available():
        # Enable TensorFloat-32 on Ampere GPUs (A100, RTX 30xx)
        if config.get("tf32", True):
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("TensorFloat-32 (TF32) enabled for matrix operations")
        
        # Enable cuDNN autotuner for optimal convolution algorithms
        if config.get("cudnn_benchmark", True):
            torch.backends.cudnn.benchmark = True
            logger.info("cuDNN autotuner enabled")
        
        # Set CUDA memory allocator settings
        if "PYTORCH_CUDA_ALLOC_CONF" not in os.environ:
            # Use larger allocation blocks to reduce fragmentation
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = f"max_split_size_mb:{DEFAULT_CUDA_ALLOC_MB}"
        
        # Disable CUDA synchronous operations for better performance
        if "CUDA_LAUNCH_BLOCKING" not in os.environ:
            os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
    
    # Set number of threads for CPU operations
    if "OMP_NUM_THREADS" not in os.environ:
        # Use half the CPU cores for OpenMP threads
        cpu_count = os.cpu_count() or 1
        # For systems with E-cores, limit to physical cores
        omp_threads = max(1, min(int(cpu_count * DEFAULT_THREAD_RATIO), cpu_count // 2))
        os.environ["OMP_NUM_THREADS"] = str(omp_threads)
    
    # PyTorch threading settings - must be set before any operations
    torch.set_num_threads(int(os.environ.get("OMP_NUM_THREADS", "1")))
    
    # Enable anomaly detection if requested (debugging only)
    if config.get("detect_anomaly", False):
        torch.autograd.set_detect_anomaly(True)
        logger.warning("Anomaly detection enabled - this will slow down training")


def get_device_info(device: torch.device = None) -> Dict[str, Any]:
    """
    Get detailed information about the current compute device.
    
    Args:
        device: Device to query (defaults to current device)
        
    Returns:
        Dictionary containing device information
    """
    if device is None:
        device = setup_device()
        
    info = {
        "device_type": device.type,
        "device_name": None,
        "memory_gb": None,
        "compute_capability": None,
        "supports_tf32": False,
        "supports_bfloat16": False,
        "supports_flash_attention": False,
        "supports_compile": False
    }
    
    if device.type == "cuda":
        device_props = torch.cuda.get_device_properties(device.index or 0)
        
        info["device_name"] = device_props.name
        info["memory_gb"] = device_props.total_memory / 1e9
        info["compute_capability"] = f"{device_props.major}.{device_props.minor}"
        
        # Check capabilities based on compute capability
        compute_major = device_props.major
        compute_minor = device_props.minor
        
        # TF32 support (Ampere and newer, compute capability 8.0+)
        info["supports_tf32"] = compute_major >= AMPERE_COMPUTE_CAPABILITY
        
        # BFloat16 support (Ampere and newer)
        info["supports_bfloat16"] = compute_major >= AMPERE_COMPUTE_CAPABILITY
        
        # Flash Attention support (Ampere and newer with specific requirements)
        # Note: Ada (8.9) and Hopper (9.0) also support it
        info["supports_flash_attention"] = compute_major >= AMPERE_COMPUTE_CAPABILITY
        
        # Compilation support
        info["supports_compile"] = True
        
    elif device.type == "mps":
        info["device_name"] = "Apple Silicon GPU"
        info["supports_compile"] = False  # MPS has limited compile support
        
    else:
        info["device_name"] = "CPU"
        info["supports_compile"] = True
        
    return info


def optimize_dataloader_settings(
    batch_size: int,
    device_type: str,
    num_workers: Optional[int] = None
) -> Dict[str, Any]:
    """
    Get optimized DataLoader settings based on hardware.
    Fixed prefetch_factor logic to prevent PyTorch errors.
    
    Args:
        batch_size: Batch size for training
        device_type: Type of compute device (cuda/mps/cpu)
        num_workers: Override for number of workers
        
    Returns:
        Dictionary of DataLoader settings
    """
    settings = {
        "batch_size": batch_size,
        "num_workers": 0,
        "pin_memory": False,
        "persistent_workers": False,
        "prefetch_factor": None  # Must be None when num_workers=0
    }
    
    if device_type == "cuda":
        # Enable pinned memory for faster GPU transfer
        settings["pin_memory"] = True
        
        # Use multiple workers for data loading
        if num_workers is None:
            # Use 4-8 workers typically
            cpu_count = os.cpu_count() or 1
            settings["num_workers"] = min(MAX_WORKERS, max(MIN_WORKERS, cpu_count // 2))
        else:
            settings["num_workers"] = num_workers
        
        # Set prefetch_factor only when num_workers > 0
        if settings["num_workers"] > 0:
            settings["persistent_workers"] = True
            # Ensure prefetch_factor is within valid range [2, 16]
            settings["prefetch_factor"] = min(
                MAX_PREFETCH_FACTOR,
                max(MIN_PREFETCH_FACTOR, settings["num_workers"] // 2)
            )
        # prefetch_factor remains None when num_workers=0
            
    elif device_type == "mps":
        # MPS doesn't work well with multiprocessing
        settings["num_workers"] = MPS_WORKERS  # 0
        settings["pin_memory"] = False
        settings["persistent_workers"] = False
        # prefetch_factor must be None when num_workers=0
        
    elif device_type == "cpu":
        # For CPU, use fewer workers to avoid overhead
        if num_workers is None:
            settings["num_workers"] = min(MIN_CPU_WORKERS, os.cpu_count() or 1)
        else:
            settings["num_workers"] = num_workers
            
        # Set prefetch_factor only for CPU workers > 0
        if settings["num_workers"] > 0:
            settings["persistent_workers"] = True
            settings["prefetch_factor"] = MIN_PREFETCH_FACTOR
        # prefetch_factor remains None when num_workers=0
    
    # Log configuration for debugging
    logger = logging.getLogger(__name__)
    logger.debug(
        f"DataLoader settings for {device_type}: "
        f"workers={settings['num_workers']}, "
        f"prefetch={settings['prefetch_factor']}, "
        f"pin_memory={settings['pin_memory']}"
    )
    
    return settings

===== /Users/imalsky/Desktop/Chemulator/src/models/model.py =====
#!/usr/bin/env python3
"""
Model definitions for chemical kinetics prediction.

Provides implementations of:
- FiLM-SIREN
- ResNet
- DeepONet

With optional torch.compile support and model export using torch.export (modern best practice replacing legacy JIT/TorchScript).
"""

import logging
import time
from pathlib import Path
from typing import Dict, Any, List

import torch
import torch.nn as nn

# Constants
MIN_CONCENTRATION = 1e-30

# =============================================================================
# SIREN Model
# =============================================================================

class SIREN(nn.Module):
    """
    SIREN (Sinusoidal Representation Network) for chemical kinetics.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]
        
        self.layers = nn.ModuleList()
        prev_dim = self.num_species + self.num_globals + 1  # inputs + time
        
        for dim in self.hidden_dims:
            self.layers.append(nn.Linear(prev_dim, dim))
            prev_dim = dim
        
        self.output_layer = nn.Linear(prev_dim, self.num_species)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer in self.layers:
            x = torch.sin(layer(x))
        return self.output_layer(x)

# =============================================================================
# ResNet Model
# =============================================================================

class ChemicalResNet(nn.Module):
    """
    ResNet for chemical kinetics prediction.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]
        
        self.input_layer = nn.Linear(self.num_species + self.num_globals + 1, self.hidden_dims[0])
        
        self.res_blocks = nn.ModuleList()
        for i in range(len(self.hidden_dims) - 1):
            self.res_blocks.append(
                nn.Sequential(
                    nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]),
                    nn.ReLU(),
                    nn.Linear(self.hidden_dims[i+1], self.hidden_dims[i+1])
                )
            )
        
        self.output_layer = nn.Linear(self.hidden_dims[-1], self.num_species)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = nn.functional.relu(self.input_layer(x))
        
        for block in self.res_blocks:
            residual = x
            x = nn.functional.relu(block(x) + residual)
        
        return torch.clamp(self.output_layer(x), min=MIN_CONCENTRATION)

# =============================================================================
# DeepONet Model
# =============================================================================

class ChemicalDeepONet(nn.Module):
    """
    Deep Operator Network for chemical kinetics.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        
        branch_layers = config["model"]["branch_layers"]
        trunk_layers = config["model"]["trunk_layers"]
        self.basis_dim = config["model"]["basis_dim"]
        self.activation = self._get_activation(config["model"].get("activation", "gelu"))
        self.use_residual = config["model"].get("use_residual", True)
        self.output_scale = config["model"].get("output_scale", 1.0)
        
        # Branch net: processes initial conditions, outputs basis for each species
        self.branch_net = self._build_mlp(
            self.num_species + self.num_globals,
            branch_layers,
            self.basis_dim * self.num_species
        )
        
        # Trunk net: processes time
        self.trunk_net = self._build_mlp(1, trunk_layers, self.basis_dim)
        
        # Bias
        self.bias = nn.Parameter(torch.zeros(1, self.num_species))
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(),
            "tanh": nn.Tanh()
        }
        return activations.get(name.lower(), nn.GELU())
    
    def _build_mlp(self, input_dim: int, hidden_layers: List[int], output_dim: int) -> nn.Sequential:
        layers = []
        prev_dim = input_dim
        
        for dim in hidden_layers:
            layers.extend([
                nn.Linear(prev_dim, dim),
                self.activation
            ])
            prev_dim = dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        return nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with corrected multi-output handling.
        
        Args:
            x: Input tensor (batch_size, num_species + num_globals + 1)
            
        Returns:
            Predicted species concentrations (batch_size, num_species)
        """
        initial_conditions = x[:, :-1]
        time = x[:, -1].unsqueeze(1)
        initial_species = x[:, :self.num_species]
        
        # Compute branch and trunk
        branch_out = self.branch_net(initial_conditions)  # (batch, basis_dim * num_species)
        trunk_out = self.trunk_net(time)  # (batch, basis_dim)
        
        # Reshape branch output
        branch_out = branch_out.view(-1, self.num_species, self.basis_dim)  # (batch, num_species, basis_dim)
        
        # Compute output for each species
        output = torch.einsum('bki,bi->bk', branch_out, trunk_out)  # (batch, num_species)
        output = output * self.output_scale + self.bias
        
        if self.use_residual:
            output = initial_species + output
            
        return torch.clamp(output, min=MIN_CONCENTRATION)

# =============================================================================
# Model Factory with Optional Compilation
# =============================================================================

def create_model(
    config: Dict[str, Any],
    device: torch.device
) -> nn.Module:
    """
    Construct the requested network, move it to the target device and – if
    enabled – compile it in default mode.
    """
    kind = config["model"]["type"].lower()
    if kind == "siren":
        model = SIREN(config)
    elif kind == "resnet":
        model = ChemicalResNet(config)
    elif kind == "deeponet":
        model = ChemicalDeepONet(config)
    else:
        raise ValueError(f"Unknown model type: {kind}")

    model = model.to(device)  # move first

    # -------------------------- optional compile -------------------------
    if not config["system"].get("use_torch_compile", False):
        return model

    # Only compile if supported
    if device.type != "cuda":
        logging.info("Compilation only supported on CUDA devices – running eager")
        return model

    # Timing
    compile_start = time.time()
    logging.info("Starting model compilation in default mode…")

    try:
        model = torch.compile(model, mode="default")
    except Exception as e:
        logging.warning(f"Compilation failed: {e} – falling back to eager mode")
        return model
    finally:
        logging.info(f"Compilation finished in {time.time() - compile_start:.1f}s")

    return model


def export_model(
    model: nn.Module,
    example_input: torch.Tensor,
    save_path: Path
):
    """
    Export model using torch.export with proper handling for compiled models.
    
    Args:
        model: Model to export (compiled or eager)
        example_input: Example input tensor
        save_path: Path to save exported model
    """
    logger = logging.getLogger(__name__)
    
    # Set to eval mode
    model.eval()
    
    # Check if model is compiled and get the original model if needed
    is_compiled = hasattr(model, '_orig_mod')
    if is_compiled:
        logger.info("Detected compiled model, extracting original model for export")
        original_model = model._orig_mod
    else:
        original_model = model
    
    with torch.no_grad():
        try:
            # Start export timer for large models
            export_start = time.time()
            logger.info("Starting model export...")
            
            # Use torch.export.export for modern export
            exported = torch.export.export(original_model, (example_input,))
            
            # Save the exported program
            torch.export.save(exported, str(save_path))
            
            export_time = time.time() - export_start
            logger.info(f"Model exported successfully to {save_path} in {export_time:.1f}s")
            
        except Exception as e:
            logger.error(f"Model export failed: {e}")
            logger.error("Common causes: dynamic shapes, unsupported ops, or compilation artifacts")
            logger.info("Try exporting the model before compilation if this persists")
            raise

===== /Users/imalsky/Desktop/Chemulator/src/data/device_prefetch.py =====
#!/usr/bin/env python3
"""
Device prefetching utilities for overlapping data transfer with computation.
"""

import torch
from torch.utils.data import DataLoader
from typing import Iterator, Tuple


class DevicePrefetchLoader:
    """
    Wraps a DataLoader to prefetch batches to GPU in a separate CUDA stream.
    This overlaps data transfer with model computation for better GPU utilization.
    """
    
    def __init__(self, loader: DataLoader, device: torch.device):
        """
        Initialize the prefetch loader.
        
        Args:
            loader: The DataLoader to wrap
            device: Target device (should be cuda)
        """
        self.loader = loader
        self.device = device
        
        # Only use CUDA streams for CUDA devices
        if device.type == 'cuda':
            self.stream = torch.cuda.Stream(device=device)
        else:
            self.stream = None
        
        self.preload_batch = None
    
    def __len__(self):
        return len(self.loader)
    
    def __iter__(self):
        # Start the preload
        loader_iter = iter(self.loader)
        self._preload(loader_iter)
        
        while self.preload_batch is not None:
            # Wait for the preload to complete and get the batch
            if self.stream is not None:
                torch.cuda.current_stream(self.device).wait_stream(self.stream)
            
            batch = self.preload_batch
            
            # Start preloading the next batch
            self._preload(loader_iter)
            
            yield batch
    
    def _preload(self, loader_iter: Iterator):
        """Preload the next batch in a separate stream."""
        try:
            data = next(loader_iter)
        except StopIteration:
            self.preload_batch = None
            return
        
        if self.stream is not None:
            with torch.cuda.stream(self.stream):
                # Move batch to device asynchronously
                self.preload_batch = self._move_to_device(data)
        else:
            # No CUDA stream available, just move normally
            self.preload_batch = self._move_to_device(data)
    
    def _move_to_device(self, data):
        """Move data to device with non_blocking."""
        if isinstance(data, (tuple, list)):
            return tuple(
                d.to(self.device, non_blocking=True) if isinstance(d, torch.Tensor) else d
                for d in data
            )
        elif isinstance(data, dict):
            return {
                k: v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v
                for k, v in data.items()
            }
        elif isinstance(data, torch.Tensor):
            return data.to(self.device, non_blocking=True)
        else:
            return data
        
    # ──────────────────────────────────────────────────────────────
    def __getattr__(self, name):
        """
        Forward attribute look-ups that this wrapper does not define to the
        underlying DataLoader. This exposes .num_workers, .batch_size, etc.,
        so downstream code can treat DevicePrefetchLoader just like a regular
        DataLoader.

        NOTE: magic (dunder) names are excluded to avoid infinite recursion.
        """
        if name.startswith("__"):
            raise AttributeError(name)
        return getattr(self.loader, name)

===== /Users/imalsky/Desktop/Chemulator/src/data/preprocessor.py =====
#!/usr/bin/env python3
"""
Preprocessor for chemical kinetics data.
Converts raw HDF5 files directly to normalized NPY shards.
"""

import logging
import random
import time
import re
from pathlib import Path
from typing import Dict, List, Any

import h5py
import numpy as np
import torch
from utils.utils import save_json

from .normalizer import DataNormalizer, NormalizationHelper

DEFAULT_TOSS = 1e-25  # Minimum threshold for species values

class DataPreprocessor:
    """
    Preprocess raw HDF5 files to normalized NPY shards.
    """
    
    def __init__(
        self,
        raw_files: List[Path],
        output_dir: Path,
        config: Dict[str, Any]
    ):
        self.raw_files = raw_files
        self.output_dir = output_dir
        self.config = config
        
        self.logger = logging.getLogger(__name__)
        
        self.data_config = config["data"]
        self.species_vars = self.data_config["species_variables"]
        self.global_vars = self.data_config["global_variables"]
        self.time_var = self.data_config["time_variable"]
        self.var_order = self.species_vars + self.global_vars + [self.time_var]
        
        self.n_vars = len(self.var_order)
        self.n_species = len(self.species_vars)
        self.n_globals = len(self.global_vars)
        
        self.shard_size = self.data_config.get("shard_size", 1000000)  # Default if not specified
        self.chunk_size = self.data_config["chunk_size"]
        
        # Initialize summary counters
        self.total_groups = 0
        self.skipped_fraction = 0
        self.skipped_missing = 0
        self.skipped_pattern = 0
        self.skipped_nonfinite = 0
        self.total_nonfinite_values = 0

        self._init_shard_index()  # Initialize shard_index structure
    
    def _init_shard_index(self) -> None:
        """Initialize an empty shard-index structure."""
        self.shard_index: Dict[str, Any] = {
            "format": "npy_shards_v1",
            "n_species": self.n_species,
            "n_globals": self.n_globals,
            "samples_per_shard": self.shard_size,
            "n_shards": 0,
            "total_samples": 0,
            "shards": [],
            "split_files": {
                "train": "train_indices.npy",
                "validation": "val_indices.npy",
                "test": "test_indices.npy"
            }
        }
    
    def process_to_npy_shards(self) -> Dict[str, Any]:
        """
        Two-stage pipeline:
          1. Single streaming sweep to accumulate statistics and count rows.
          2. Second streaming sweep to write normalised NPY shards.

        Returns:
            Dictionary with train_indices, val_indices, test_indices, and misc metadata.
        """
        self.logger.info("─" * 80)
        self.logger.info("Stage 1 – collecting normalisation statistics")

        ########################
        # Pass 1 – statistics  #
        ########################
        pass1_start = time.time()
        max_timesteps = 0
        accumulators  = self._initialize_accumulators()

        for raw_file in self.raw_files:
            file_start = time.time()
            self.logger.info(f"Processing stats from {raw_file}")
            
            with h5py.File(raw_file, "r") as f:
                for gname in f.keys():
                    self.total_groups += 1  # Track all attempted groups

                    grp = f[gname]
                    if not self._accept_group(grp, gname):
                        continue

                    n_t = grp[self.time_var].shape[0]
                    if n_t > 10000:
                        self.logger.debug(f"Processing large group {gname} with {n_t:,} timesteps")
                    
                    max_timesteps = max(max_timesteps, n_t)
                    profile_np    = self._group_to_profile(grp, gname, n_t)
                    # Reshape to 3D (n_profiles=1, n_t, n_vars) for normalizer compatibility
                    profile_3d = profile_np.reshape(1, n_t, self.n_vars)
                    self.normalizer._update_accumulators(
                        profile_3d, accumulators, n_t
                    )
            
            file_time = time.time() - file_start
            self.logger.info(f"Processed stats for file {raw_file} in {file_time:.1f}s")
        
        pass1_time = time.time() - pass1_start
        self.logger.info(f"Stage 1 completed in {pass1_time:.1f}s")

        norm_stats = self.normalizer._finalize_statistics(accumulators)
        try:
            save_json(norm_stats, self.output_dir / "normalization.json")
        except OSError as e:
            self.logger.error(f"Failed to write normalization.json: {e}")
            raise

        ########################
        # Pass 2 – shard write #
        ########################
        self.logger.info("Stage 2 – writing normalised shards")
        pass2_start = time.time()
        
        helper       = NormalizationHelper(
            norm_stats, torch.device("cpu"),
            self.species_vars, self.global_vars, self.time_var, self.config
        )
        shard_id     = 0
        shard_rows   = []
        splits       = {"train": [], "validation": [], "test": []}
        val_f, test_f = self.config["training"]["val_fraction"], self.config["training"]["test_fraction"]
        global_idx   = 0

        for raw_file in self.raw_files:
            file_start = time.time()
            self.logger.info(f"Processing {raw_file} to shards")
            
            with h5py.File(raw_file, "r") as f:
                for gname in f.keys():
                    grp = f[gname]
                    if not self._accept_group(grp, gname):
                        continue

                    n_t = grp[self.time_var].shape[0]
                    if n_t > 10000:
                        group_start = time.time()
                        self.logger.debug(f"Starting processing for large group {gname} with {n_t:,} timesteps")
                    
                    # Get and normalize profile
                    profile = self._group_to_profile(grp, gname, n_t)
                    profile_tensor = torch.from_numpy(profile)
                    normalized_profile = helper.normalize_profile(profile_tensor).numpy()
                    
                    # Convert to samples
                    samples = self._group_to_samples(normalized_profile, n_t)   # List[np.ndarray]
                    shard_rows.extend(samples)

                    # flush?
                    while len(shard_rows) >= self.shard_size:
                        rows, shard_rows = shard_rows[:self.shard_size], shard_rows[self.shard_size:]
                        self._write_shard(
                            shard_id, rows, self.shard_index
                        )
                        shard_id += 1

                    # split bookkeeping
                    for _ in range(n_t - 1):
                        r = random.random()
                        tgt = ("test"        if r < test_f else
                               "validation"  if r < test_f + val_f else
                               "train")
                        splits[tgt].append(global_idx)
                        global_idx += 1
                    
                    if n_t > 10000:
                        group_time = time.time() - group_start
                        self.logger.debug(f"Completed processing for large group {gname} in {group_time:.1f}s")
            
            file_time = time.time() - file_start
            self.logger.info(f"Sharded file {raw_file} in {file_time:.1f}s")

        # final partial shard
        if shard_rows:
            self._write_shard(shard_id, shard_rows, self.shard_index)

        # Update total_samples
        self.shard_index["total_samples"] = global_idx
        save_json(self.shard_index, self.output_dir / "shard_index.json")
        
        pass2_time = time.time() - pass2_start
        self.logger.info(f"Stage 2 completed in {pass2_time:.1f}s")

        # Save split indices as NPY
        # Use the split_files mapping so 'validation' → 'val_indices.npy'
        for split_name, idx_list in splits.items():
            idx_arr = np.array(idx_list, dtype=np.int64)
            filename = self.shard_index["split_files"].get(
                split_name,
                f"{split_name}_indices.npy"
            )
            np.save(self.output_dir / filename, idx_arr)
            self.logger.info(f"{split_name} split: {len(idx_list):,} samples (saved to {filename})")

        # Log skipped summary
        skipped_total = self.skipped_fraction + self.skipped_missing + self.skipped_pattern + self.skipped_nonfinite
        self.logger.info(f"Preprocessing summary: Total groups attempted: {self.total_groups}, "
                         f"Processed: {self.total_groups - skipped_total}, "
                         f"Skipped: {skipped_total} ({skipped_total / self.total_groups * 100 if self.total_groups else 0:.1f}%) "
                         f"[fraction: {self.skipped_fraction}, missing keys: {self.skipped_missing}, "
                         f"pattern mismatch: {self.skipped_pattern}, non-finite: {self.skipped_nonfinite}]")

        if global_idx == 0:
            self.logger.error("No valid data processed. Exiting with warning.")
            raise SystemExit("Preprocessing failed: No valid data found in raw files. Check input data for issues like missing variables or non-finite values.")

        return {f"{k}_indices": splits[k] for k in ["train", "validation", "test"]}
    
    def _initialize_accumulators(self):
        """
        Initialize accumulators for stats collection.
        First sweep to discover the true max_timesteps, then init normalizer.
        """
        # 1) Find true maximum timesteps across all groups
        max_timesteps = 0
        for raw_file in self.raw_files:
            with h5py.File(raw_file, "r") as f:
                for gname in f.keys():
                    n_t = f[gname][self.time_var].shape[0]
                    max_timesteps = max(max_timesteps, n_t)

        # 2) Initialize normalizer now that we know max_timesteps
        self.normalizer = DataNormalizer(
            self.config,
            actual_timesteps=max_timesteps
        )
        return self.normalizer._initialize_accumulators(max_timesteps)
    
    def _accept_group(self, group: h5py.Group, gname: str) -> bool:
        """Combined acceptance check: validate + pattern match + use_fraction."""
        if random.random() > self.config["training"]["use_fraction"]:
            self.skipped_fraction += 1
            return False
        
        # Check all required variables exist
        required_keys = set(self.species_vars + [self.time_var])
        missing_vars = required_keys - set(group.keys())
        if missing_vars:
            if not hasattr(self, '_warned_missing_vars'):
                self._warned_missing_vars = True
                self.logger.error(f"Missing variables in HDF5: {missing_vars}")
            self.skipped_missing += 1
            return False
        
        if not self._validate_group(group):
            self.skipped_nonfinite += 1
            return False
        
        match = re.match(r"run_T_(?P<T_init>[\d.eE+-]+)_P_(?P<P_init>[\d.eE+-]+)_SEED_\d+", gname)
        if not match:
            self.skipped_pattern += 1
            return False
        
        return True
    
    def _group_to_profile(self, group: h5py.Group, gname: str, n_t: int) -> np.ndarray:
        """Extract profile from group using chunked reading to handle large n_t."""
        match = re.match(r"run_T_(?P<T_init>[\d.eE+-]+)_P_(?P<P_init>[\d.eE+-]+)_SEED_\d+", gname)
        try:
            T_init = float(match.group('T_init'))
            P_init = float(match.group('P_init'))
        except (ValueError, TypeError) as e:
            self.logger.warning(f"Invalid T_init or P_init in group {gname}: {e}")
            raise ValueError(f"Cannot parse initial conditions from {gname}")
        
        profile = np.zeros((n_t, self.n_vars), dtype=np.float32)
        
        # Chunked load for variables from HDF5
        for start in range(0, n_t, self.chunk_size):
            end = min(start + self.chunk_size, n_t)
            chunk_size = end - start
            if chunk_size > 10000:
                self.logger.debug(f"Loading chunk {start}-{end} for group {gname}")
            
            for i, var in enumerate(self.var_order):
                if var in self.species_vars or var == self.time_var:
                    profile[start:end, i] = group[var][start:end]
                elif var == "T_init":
                    profile[start:end, i] = T_init
                elif var == "P_init":
                    profile[start:end, i] = P_init
        
        return profile
    
    def _group_to_samples(self, normalized_profile: np.ndarray, n_t: int) -> List[np.ndarray]:
        """Flatten normalized profile to samples (exclude t=0) using vectorized operations."""
        if n_t <= 1:
            return []
        
        # Log processing for large profiles
        if n_t > 10000:
            self.logger.info(f"Converting profile with {n_t:,} timesteps to {n_t-1:,} training samples")
        
        # Initial species (repeat for all samples)
        init_species = np.repeat(normalized_profile[0, :self.n_species][np.newaxis, :], n_t - 1, axis=0)
        
        # Globals (from t=1 to end)
        globals_t = normalized_profile[1:, self.n_species:self.n_species + self.n_globals]
        
        # Time (from t=1 to end)
        time_t = normalized_profile[1:, -1][:, np.newaxis]
        
        # Targets (species from t=1 to end)
        targets = normalized_profile[1:, :self.n_species]
        
        # Concatenate along columns
        samples_array = np.concatenate([init_species, globals_t, time_t, targets], axis=1)
        
        # Convert to list of arrays (for extend compatibility)
        return [row for row in samples_array]
    
    def _write_shard(
        self,
        shard_idx: int,
        data: List[np.ndarray],
        shard_index: Dict[str, Any]
    ) -> None:
        """
        Write a single NPY shard (data already normalized).
        Updates shard_index with correct start/end offsets.
        """
        shard_start = time.time()
        shard_path = self.output_dir / f"shard_{shard_idx:04d}.npy"

        # Serialize data to disk
        shard_array = np.array(data, dtype=np.float32)
        np.save(shard_path, shard_array)

        # Measure file size
        file_size = shard_path.stat().st_size

        # Compute correct start/end indices from running total_samples
        start_idx = shard_index.get(
            "total_samples",
            shard_idx * shard_index["samples_per_shard"]
        )
        end_idx = start_idx + len(data)

        # Record shard metadata
        shard_info = {
            "shard_idx": shard_idx,
            "filename": shard_path.name,
            "start_idx": start_idx,
            "end_idx": end_idx,
            "n_samples": len(data),
            "file_size": int(file_size),
        }
        shard_index["shards"].append(shard_info)
        shard_index["n_shards"] += 1

        # Update total_samples for the next shard
        shard_index["total_samples"] = end_idx

        # Debug log
        shard_time = time.time() - shard_start
        self.logger.debug(
            f"Wrote shard {shard_idx}: {len(data):,} samples, "
            f"{file_size/1e6:.1f} MB in {shard_time:.1f}s"
        )
    
    def _validate_group(self, group: h5py.Group) -> bool:
        """
        Validate group data and log rejection reasons.
        Returns False if any NaN/Inf is found or if any species value is 
        below threshold.
        """
        required_keys = self.species_vars + [self.time_var]
        is_valid = True
        n_t = group[self.time_var].shape[0]
        min_threshold = DEFAULT_TOSS
        
        for key in required_keys:
            bad_count = 0  # For non-finites
            below_threshold_count = 0  # For values below threshold
            
            for start in range(0, n_t, self.chunk_size):
                end = min(start + self.chunk_size, n_t)
                data_chunk = group[key][start:end]
                
                # Check for non-finites
                bad_mask = ~np.isfinite(data_chunk)
                bad_count += int(bad_mask.sum())
                
                # Check for species values below threshold
                if key in self.species_vars:
                    below_mask = data_chunk < min_threshold
                    below_threshold_count += int(below_mask.sum())
                    
                    if np.any(below_mask):
                        is_valid = False
            
            self.total_nonfinite_values += bad_count
            if bad_count > 0:
                is_valid = False
        
        return is_valid

===== /Users/imalsky/Desktop/Chemulator/src/data/dataset.py =====
#!/usr/bin/env python3
"""
Optimized dataset for chemical kinetics data.
Uses NPYDataset for high-performance memory-mapped data loading on pre-normalized shards.
"""

import logging
import random
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader


class NPYDataset(Dataset):
    """
    High-performance dataset using memory-mapped NPY shards.
    Assumes pre-normalized data for zero runtime overhead.
    """

    def __init__(
        self,
        shard_dir: Path,
        indices: List[int],
        config: Dict[str, Any],
        device: torch.device,
        split_name: Optional[str] = None,
    ):
        self.shard_dir = Path(shard_dir)
        super().__init__()

        self.logger = logging.getLogger(__name__)
        self.config = config
        self.device_type = device.type
        self.split_name = split_name

        # Load shard index
        with open(self.shard_dir / "shard_index.json") as jf:
            self.shard_index = json.load(jf)
        if self.shard_index.get("format") != "npy_shards_v1":
            raise ValueError(f"{self.shard_dir}: unsupported shard format "
                             f"{self.shard_index.get('format')}")

        # Store frequently-used constants
        self.n_species = self.shard_index["n_species"]
        self.n_globals = self.shard_index["n_globals"]
        data_cfg = self.config["data"]
        self.species_vars = data_cfg["species_variables"]
        self.global_vars = data_cfg["global_variables"]
        self.time_var = data_cfg["time_variable"]

        # Split handling - prioritize passed indices
        if indices is not None:
            self.sample_indices = np.asarray(indices, dtype=np.int64)
            self.n_total_samples = int(self.sample_indices.shape[0])
            self.logger.info(f"{split_name or 'custom'} split: {self.n_total_samples:,} samples (indices passed in)")
        else:
            split_file = (self.shard_dir /
                          self.shard_index["split_files"].get(split_name or "", ""))
            if split_file.exists():
                self.sample_indices = np.load(split_file, mmap_mode="r")
                self.n_total_samples = int(self.sample_indices.shape[0])
                self.logger.info(f"{split_name.capitalize()} split: {self.n_total_samples:,} samples (indices mmap-loaded)")
            else:
                self.sample_indices = None
                self.n_total_samples = int(self.shard_index["total_samples"])

        # Worker-local cache using instance attributes (per-process after pickling)
        self.samples_per_shard = int(self.shard_index["samples_per_shard"])
        self._current_shard_idx = -1
        self._current_shard_data = None
        self._shard_cache = {}

        self.logger.info(f"NPYDataset ready: {self.n_total_samples:,} samples, "
                         f"{self.shard_index['n_shards']} shards (pre-normalized)")
    
    def _get_shard_data(self, shard_idx: int) -> np.ndarray:
        """
        Get memory-mapped shard data with caching.
        Uses instance attributes for multiprocess compatibility.
        """
        # Check if we already have this shard loaded
        if shard_idx == self._current_shard_idx and self._current_shard_data is not None:
            return self._current_shard_data
        
        # Check cache
        if shard_idx in self._shard_cache:
            self._current_shard_idx = shard_idx
            self._current_shard_data = self._shard_cache[shard_idx]
            return self._current_shard_data
        
        # Load new shard
        shard_info = self.shard_index["shards"][shard_idx]
        shard_path = self.shard_dir / shard_info["filename"]
        
        # Memory-map the file for zero-copy access
        shard_data = np.load(shard_path, mmap_mode='r')
        
        # Cache management - keep more shards for better sequential access
        max_cache_size = 8
        self._shard_cache[shard_idx] = shard_data
        if len(self._shard_cache) > max_cache_size:
            # Remove least recently used shard
            oldest_idx = min(self._shard_cache.keys())
            if oldest_idx != shard_idx:
                del self._shard_cache[oldest_idx]
        
        self._current_shard_idx = shard_idx
        self._current_shard_data = shard_data
        
        return shard_data
    
    def _global_to_shard_idx(self, global_idx: int) -> Tuple[int, int]:
        """
        Convert global sample index to (shard_idx, local_idx).
        """
        samples_per_shard = self.shard_index["samples_per_shard"]
        shard_idx = global_idx // samples_per_shard
        local_idx = global_idx % samples_per_shard
        return shard_idx, local_idx
    
    def __len__(self) -> int:
        """Return total number of samples."""
        return self.n_total_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a single sample with efficient memory-mapped access.
        Assumes pre-normalized data - direct tensor creation.
        """
        # Map to actual sample index if using split indices
        if self.sample_indices is not None:
            if idx >= len(self.sample_indices):
                raise IndexError(f"Index {idx} out of range for split with {len(self.sample_indices)} samples")
            global_idx = self.sample_indices[idx]
        else:
            global_idx = idx
        
        # Find which shard contains this sample
        shard_idx, local_idx = self._global_to_shard_idx(global_idx)
        
        # Get shard data (memory-mapped)
        shard_data = self._get_shard_data(shard_idx)
        
        # Extract row (this is a view, not a copy)
        row = shard_data[local_idx]
        
        # Split into input and target
        n_input = self.n_species + self.n_globals + 1
        
        # Direct tensor creation without any processing
        input_arr = row[:n_input]
        target_arr = row[n_input:]
        
        # Pin memory if target device is CUDA
        pin = self.device_type == "cuda"
        input_tensor = torch.from_numpy(input_arr.copy()).pin_memory() if pin else torch.from_numpy(input_arr.copy())
        target_tensor = torch.from_numpy(target_arr.copy()).pin_memory() if pin else torch.from_numpy(target_arr.copy())


        return input_tensor, target_tensor
    
    def get_batch_info(self) -> Dict[str, Any]:
        """Get information about dataset structure for logging."""
        return {
            "format": "npy_shards",
            "n_shards": self.shard_index["n_shards"],
            "samples_per_shard": self.shard_index["samples_per_shard"],
            "total_samples": self.n_total_samples,
            "split": self.split_name
        }


def worker_init_fn(worker_id: int):
    """Initialize worker with proper random seed."""
    worker_seed = torch.initial_seed() % 2**32 + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def create_dataloader(
    dataset: Dataset,
    config: Dict[str, Any],
    *,
    shuffle: bool = True,
    device: Optional[torch.device] = None
) -> DataLoader:
    """
    Construct a DataLoader whose defaults adapt to the host hardware.
    Fixed multiprocessing initialization to avoid conflicts.
    """
    train_cfg = config["training"]
    device_type = (device or torch.device("cuda")).type
    
    from utils.hardware import optimize_dataloader_settings
    opts = optimize_dataloader_settings(
        batch_size=train_cfg["batch_size"],
        device_type=device_type,
        num_workers=train_cfg.get("num_workers"),
    )
    
    # Override with config values if specified
    opts["num_workers"] = train_cfg.get("num_workers", min(16, opts["num_workers"]))
    opts["prefetch_factor"] = None if opts["num_workers"] == 0 else min(
        4, max(2, opts["num_workers"] // 4)
    )
    
    # Setup multiprocessing safely
    if opts["num_workers"] > 0:
        import torch.multiprocessing as mp
        try:
            # Only set if not already set
            if mp.get_start_method(allow_none=True) is None:
                mp.set_start_method("forkserver")
        except RuntimeError:
            # Already set, which is fine
            pass
    
    kwargs = dict(
        dataset=dataset,
        batch_size=opts["batch_size"],
        shuffle=shuffle,
        num_workers=opts["num_workers"],
        pin_memory=opts["pin_memory"],
        persistent_workers=opts["persistent_workers"] if opts["num_workers"] > 0 else False,
        drop_last=True,
        worker_init_fn=worker_init_fn if opts["num_workers"] > 0 else None,
    )
    
    if opts["prefetch_factor"] is not None:
        kwargs["prefetch_factor"] = int(opts["prefetch_factor"])
    
    # Set CUDA memory fraction if using pinned memory
    if opts["pin_memory"] and device_type == "cuda":
        try:
            torch.cuda.set_per_process_memory_fraction(0.9)
        except AttributeError:  # older PyTorch
            pass
    
    logging.getLogger(__name__).info(
        f"DataLoader with {opts['num_workers']} worker(s), "
        f"prefetch={kwargs.get('prefetch_factor', 'N/A')}"
    )
    
    return DataLoader(**kwargs)

===== /Users/imalsky/Desktop/Chemulator/src/data/normalizer.py =====
#!/usr/bin/env python3
"""
Data normalization module for chemical kinetics datasets.

Provides efficient normalization with multiple methods
and numerical stability guarantees.
Used only during preprocessing; no runtime calls.
"""

import logging
import math
from typing import Dict, List, Any, Optional

import numpy as np
import torch
import time


# Normalization constants
DEFAULT_EPSILON = 1e-37
DEFAULT_MIN_STD = 1e-10
DEFAULT_CLAMP = 100.0
DEFAULT_SYMLOG_PERCENTILE = 0.5
FIXED_RESERVOIR_SIZE = 1000000  # Fixed size for symlog quantile estimation

FLOAT32_MAX_EXPONENT = 38  # log10(float32_max) ≈ 38.5

class DataNormalizer:
    """
    Calculate normalization statistics from data during preprocessing.
    """

    def __init__(self,
                 config: Dict[str, Any],
                 *,
                 actual_timesteps: int) -> None:
        """
        Initialize the normalizer.

        Args:
            config: Full configuration dictionary.
            actual_timesteps: Number of timesteps in the data (for reservoir sizing).
        """
        self.config        = config
        self.data_config   = config["data"]
        self.norm_config   = config["normalization"]

        # Variable lists
        self.species_vars  = self.data_config["species_variables"]
        self.global_vars   = self.data_config["global_variables"]
        self.time_var      = self.data_config["time_variable"]
        self.all_vars      = self.species_vars + self.global_vars + [self.time_var]

        # Numerical constants
        self.epsilon       = self.norm_config.get("epsilon", 1e-37)
        self.min_std       = self.norm_config.get("min_std", 1e-10)

        self._actual_timesteps = actual_timesteps

        self.logger = logging.getLogger(__name__)
        
    def _initialize_accumulators(self, n_profiles: int) -> Dict[str, Dict[str, Any]]:
        """
        Initialise per-variable statistics accumulators.
        Uses fixed reservoir size for simplicity and to avoid dynamic estimation issues.
        """
        self.logger.info(
            f"Using fixed reservoir size {FIXED_RESERVOIR_SIZE:,} for symlog quantile estimation"
        )

        accumulators: Dict[str, Dict[str, Any]] = {}
        for i, var in enumerate(self.all_vars):
            method = self._get_method(var)
            if method == "none":
                continue
            acc = dict(
                method     = method,
                index      = i,
                count      = 0,
                mean       = 0.0,
                m2         = 0.0,
                min        = float("inf"),
                max        = float("-inf"),
                reservoir  = ReservoirSampler(FIXED_RESERVOIR_SIZE, seed=self.config["system"]["seed"]) if method == "symlog" else None,
            )
            accumulators[var] = acc
        return accumulators
    
    def _get_method(self, var: str) -> str:
        """Get normalization method for a variable."""
        methods = self.norm_config.get("methods", {})
        return methods.get(var, self.norm_config["default_method"])
    
    def _update_accumulators(
        self, 
        data: np.ndarray,
        accumulators: Dict[str, Dict[str, Any]],
        n_timesteps: int
    ):
        """
        Update accumulators with a chunk of data using Chan et al.'s parallel algorithm.
        Assumes data shape: (n_profiles, n_timesteps, n_vars)
        """
        n_profiles, n_t_check, _ = data.shape
        if n_t_check != n_timesteps:
            raise ValueError(f"Mismatched timesteps: expected {n_timesteps}, got {n_t_check}")
        
        # Optional profiling if anomaly detection enabled
        if self.config["system"].get("detect_anomaly", False):
            with torch.profiler.profile(with_stack=True, profile_memory=True) as prof:
                self._perform_update_accumulators(data, accumulators, n_timesteps)
            prof.export_chrome_trace("update_accumulators_trace.json")
            self.logger.info("Profiling trace saved to update_accumulators_trace.json")
        else:
            self._perform_update_accumulators(data, accumulators, n_timesteps)

    def _perform_update_accumulators(
            self,
            data: np.ndarray,
            accumulators: Dict[str, Dict[str, Any]],
            n_timesteps: int
        ):
            """Helper for core update logic, separable for profiling."""
            n_profiles, n_t_check, _ = data.shape
            if n_t_check != n_timesteps:
                raise ValueError(f"Mismatched timesteps: expected {n_timesteps}, got {n_t_check}")

            if n_timesteps > 10000:
                start_time = time.time()
                self.logger.info(f"Starting accumulator update for large group ({n_timesteps:,} timesteps)")

            for var, acc in accumulators.items():
                var_idx = acc["index"]
                method = acc["method"]
                
                if var in self.global_vars:
                    # Globals are constant, use scalar value for efficiency
                    value = data[0, 0, var_idx]
                    value = float(value)  # Ensure scalar
                    n_b = n_profiles * n_timesteps
                    # Apply transformation if needed
                    if method in ["log-standard", "log-min-max"]:
                        value = np.log10(np.maximum(value, self.epsilon))
                    
                    mean_b = value
                    m2_b = 0.0
                    min_b = max_b = value
                    
                    # Filter invalid (though globals usually valid)
                    if not np.isfinite(value):
                        continue
                    
                    # Update min/max
                    acc["min"] = min(acc["min"], value)
                    acc["max"] = max(acc["max"], value)
                    
                    # Chan update
                    if acc["count"] == 0:
                        acc["count"] = n_b
                        acc["mean"] = mean_b
                        acc["m2"] = 0.0
                    else:
                        n_a = acc["count"]
                        mean_a = acc["mean"]
                        m2_a = acc["m2"]
                        
                        delta = mean_b - mean_a
                        n_ab = n_a + n_b
                        mean_ab = mean_a + delta * n_b / n_ab
                        m2_ab = m2_a + delta**2 * n_a * n_b / n_ab  # m2_b=0
                        
                        acc["count"] = n_ab
                        acc["mean"] = float(mean_ab)
                        acc["m2"] = float(m2_ab)
                    
                    # Reservoir for symlog (add once since constant)
                    if acc["reservoir"] is not None:
                        acc["reservoir"].add_samples(np.array([value]))
                else:
                    # Species/time: original logic
                    var_data = data[:, :, var_idx].flatten().astype(np.float64)
                    
                    valid_mask = np.isfinite(var_data)
                    var_data = var_data[valid_mask]
                    
                    if len(var_data) == 0:
                        continue
                    
                    if method in ["log-standard", "log-min-max"]:
                        var_data = np.log10(np.maximum(var_data, self.epsilon))
                    
                    n_b = len(var_data)
                    mean_b = np.mean(var_data, dtype=np.float64)
                    
                    acc["min"] = min(acc["min"], float(np.min(var_data)))
                    acc["max"] = max(acc["max"], float(np.max(var_data)))
                    
                    # Chan update
                    if acc["count"] == 0:
                        acc["count"] = n_b
                        acc["mean"] = float(mean_b)
                        if n_b > 1:
                            acc["m2"] = float(np.sum((var_data - mean_b)**2, dtype=np.float64))
                        else:
                            acc["m2"] = 0.0
                    else:
                        n_a = acc["count"]
                        mean_a = acc["mean"]
                        m2_a = acc["m2"]
                        
                        delta = mean_b - mean_a
                        mean_ab = mean_a + delta * n_b / (n_a + n_b)
                        
                        if n_b > 1:
                            m2_b = float(np.sum((var_data - mean_b)**2, dtype=np.float64))
                        else:
                            m2_b = 0.0
                        
                        m2_ab = m2_a + m2_b + delta**2 * n_a * n_b / (n_a + n_b)
                        
                        acc["count"] = n_a + n_b
                        acc["mean"] = float(mean_ab)
                        acc["m2"] = float(m2_ab)
                    
                    if method == "symlog" and acc["reservoir"]:
                        acc["reservoir"].add_samples(var_data)
            
            if n_timesteps > 10000:
                end_time = time.time()
                self.logger.info(f"Accumulator update completed in {end_time - start_time:.2f} seconds")
    
    def _finalize_statistics(self, accumulators: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Finalize statistics from accumulators."""
        stats = {
            "normalization_methods": {},
            "per_key_stats": {}
        }
        
        for var, acc in accumulators.items():
            method = acc["method"]
            stats["normalization_methods"][var] = method
            
            if method == "none":
                continue
            
            var_stats = {"method": method}
            
            # Calculate standard deviation
            if acc["count"] > 1:
                variance = acc["m2"] / (acc["count"] - 1)
                std = max(math.sqrt(variance), self.min_std)
            else:
                std = 1.0
            
            # Store statistics based on method
            if method == "standard":
                var_stats["mean"] = acc["mean"]
                var_stats["std"] = std
                
            elif method == "log-standard":
                var_stats["log_mean"] = acc["mean"]
                var_stats["log_std"] = std
                
            elif method == "log-min-max":
                var_stats["min"] = acc["min"]
                var_stats["max"] = acc["max"]
                if acc["max"] - acc["min"] < self.epsilon:
                    var_stats["max"] = acc["min"] + 1.0
                
            elif method == "symlog":
                if acc["reservoir"] and acc["reservoir"].size > 0:
                    values = acc["reservoir"].get_samples()
                    percentile = self.norm_config.get("symlog_percentile", DEFAULT_SYMLOG_PERCENTILE)
                    threshold = np.percentile(np.abs(values), percentile * 100)
                    var_stats["threshold"] = max(float(threshold), self.epsilon)
                    
                    abs_vals = np.abs(values)
                    linear_mask = abs_vals <= var_stats["threshold"]
                    transformed = np.zeros_like(values)
                    
                    transformed[linear_mask] = values[linear_mask] / var_stats["threshold"]
                    log_vals = np.maximum(abs_vals[~linear_mask] / var_stats["threshold"], self.epsilon)
                    transformed[~linear_mask] = np.sign(values[~linear_mask]) * (
                        np.log10(log_vals) + 1
                    )
                    
                    var_stats["scale_factor"] = max(float(np.max(np.abs(transformed))), 1.0)
                else:
                    var_stats["threshold"] = 1.0
                    var_stats["scale_factor"] = 1.0
            
            stats["per_key_stats"][var] = var_stats
        
        # Add methods for variables not in accumulators
        for var in self.all_vars:
            if var not in stats["normalization_methods"]:
                stats["normalization_methods"][var] = "none"
        
        stats["epsilon"] = self.epsilon
        stats["clamp_value"] = self.norm_config.get("clamp_value", DEFAULT_CLAMP)
        
        return stats


class ReservoirSampler:
    """
    Efficient reservoir sampling for quantile estimation.
    Memory-bounded implementation suitable for large datasets.
    """
    
    def __init__(self, capacity: int, seed: int):
        # Validate capacity to prevent memory issues
        if capacity > 10_000_000:  # 10M samples max
            logging.getLogger(__name__).warning(
                f"Reservoir capacity {capacity} is very large, capping at 10M samples"
            )
            capacity = 10_000_000
            
        self.capacity = capacity
        self.reservoir = []
        self.count = 0
        # Initialize RNG with fixed seed for reproducibility
        self.rng = np.random.RandomState(seed)
        
        # Pre-allocate for efficiency if capacity is known
        if capacity <= 1_000_000:
            self.reservoir = [0.0] * capacity
            self.size = 0
        else:
            self.reservoir = []
            self.size = 0
    
    def add_samples(self, samples: np.ndarray):
        """Add samples to the reservoir with memory-efficient batching."""
        # Process in chunks for very large sample arrays
        chunk_size = 100_000
        
        if len(samples) > chunk_size:
            for i in range(0, len(samples), chunk_size):
                chunk = samples[i:i + chunk_size]
                self._add_chunk(chunk)
        else:
            self._add_chunk(samples)
    
    def _add_chunk(self, samples: np.ndarray):
        """Add a chunk of samples to the reservoir."""
        for sample in samples:
            self.count += 1
            
            if self.size < self.capacity:
                # Fill reservoir
                if isinstance(self.reservoir, list) and len(self.reservoir) == self.capacity:
                    # Pre-allocated list
                    self.reservoir[self.size] = float(sample)
                else:
                    # Dynamic list
                    self.reservoir.append(float(sample))
                self.size += 1
            else:
                # Randomly replace
                j = self.rng.randint(0, self.count)
                if j < self.capacity:
                    self.reservoir[j] = float(sample)
    
    def get_samples(self) -> np.ndarray:
        """Get reservoir samples as numpy array."""
        if isinstance(self.reservoir, list) and self.size < len(self.reservoir):
            # Pre-allocated list not fully filled
            return np.array(self.reservoir[:self.size], dtype=np.float64)
        else:
            return np.array(self.reservoir, dtype=np.float64)


class NormalizationHelper:
    """
    Normalization helper for use during preprocessing.
    """

    def __init__(self,
                 stats: Dict[str, Any],
                 device: torch.device,
                 species_vars: List[str],
                 global_vars: List[str],
                 time_var: str,
                 config: Optional[Dict[str, Any]] = None):
        """
        Initialize the normalization helper.

        Args:
            stats: Pre-computed normalization statistics
            device: Target device for operations (usually CPU)
            species_vars: List of species variable names
            global_vars: List of global variable names
            time_var: Time variable name
            config: Full configuration dictionary (optional)
        """
        self.stats = stats
        self.device = device
        self.species_vars = species_vars
        self.global_vars = global_vars
        self.time_var = time_var

        # Use lengths to compute column offsets
        self.n_species = len(species_vars)
        self.n_globals = len(global_vars)
        self.methods = stats["normalization_methods"]
        self.per_key_stats = stats["per_key_stats"]

        # Numerical constants from stats or config or defaults
        self.epsilon = stats.get("epsilon", DEFAULT_EPSILON)

        # Get clamp_value from stats first, then config, then default
        self.clamp_value = stats.get("clamp_value", DEFAULT_CLAMP)
        if config and "normalization" in config:
            self.clamp_value = config["normalization"].get("clamp_value", self.clamp_value)

        # Pre-compute normalization parameters on device
        self._precompute_parameters()

        # Pre-compute for sample batch normalization
        self._precompute_sample_columns()

    def _precompute_parameters(self):
        """Pre-compute normalization parameters for efficiency."""
        self.norm_params = {}

        # Group variables by normalization method for vectorized operations
        self.method_groups = {
            "standard": [],
            "log-standard": [],
            "log-min-max": [],
            "symlog": [],
            "none": []
        }

        # Create parameter tensors for each variable
        for var, method in self.methods.items():
            if method == "none" or var not in self.per_key_stats:
                self.method_groups["none"].append(var)
                continue

            var_stats = self.per_key_stats[var]
            params = {"method": method}

            if method == "standard":
                params["mean"] = torch.tensor(var_stats["mean"], dtype=torch.float32, device=self.device)
                params["std"] = torch.tensor(var_stats["std"], dtype=torch.float32, device=self.device)

            elif method == "log-standard":
                params["log_mean"] = torch.tensor(var_stats["log_mean"], dtype=torch.float32, device=self.device)
                params["log_std"] = torch.tensor(var_stats["log_std"], dtype=torch.float32, device=self.device)

            elif method == "log-min-max":
                params["min"] = torch.tensor(var_stats["min"], dtype=torch.float32, device=self.device)
                params["max"] = torch.tensor(var_stats["max"], dtype=torch.float32, device=self.device)

            elif method == "symlog":
                params["threshold"] = torch.tensor(var_stats["threshold"], dtype=torch.float32, device=self.device)
                params["scale_factor"] = torch.tensor(var_stats["scale_factor"], dtype=torch.float32, device=self.device)

            self.norm_params[var] = params
            self.method_groups[method].append(var)

        # Pre-compute column indices for each method group
        self._compute_column_indices()

    def _compute_column_indices(self):
        """Pre-compute column indices for vectorized operations."""
        self.col_indices = {}

        all_vars = self.species_vars + self.global_vars + [self.time_var]
        var_to_col = {var: i for i, var in enumerate(all_vars)}

        for method, vars_list in self.method_groups.items():
            if vars_list:
                self.col_indices[method] = [var_to_col[var] for var in vars_list]

    def _precompute_sample_columns(self):
        """Pre-compute variable mapping and indices for sample batches."""
        # Sample columns: species_init + globals + time + species_target
        self.sample_col_vars = self.species_vars + self.global_vars + [self.time_var] + self.species_vars

        # Group sample columns by method
        self.sample_col_indices = {}
        for method in self.method_groups:
            self.sample_col_indices[method] = [i for i, var in enumerate(self.sample_col_vars) if self.methods.get(var, "none") == method]

    def normalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """
        Normalize a complete profile tensor using vectorized operations.

        Args:
            profile: Tensor of shape (timesteps, n_species + n_globals + 1)
        Returns:
            Normalized profile tensor
        """
        # Ensure profile is on the same device as normalization parameters
        if profile.device != self.device:
            profile = profile.to(self.device)

        normalized = profile.clone()

        # Apply normalization for each method group (vectorized)
        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none":
                continue

            # Get columns for this method
            cols = normalized[:, col_idxs]

            if method == "standard":
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                normalized[:, col_idxs] = torch.clamp(
                    (cols - means) / stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-standard":
                log_means = torch.stack([self.norm_params[var]["log_mean"] for var in self.method_groups[method]])
                log_stds = torch.stack([self.norm_params[var]["log_std"] for var in self.method_groups[method]])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                normalized[:, col_idxs] = torch.clamp(
                    (log_data - log_means) / log_stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-min-max":
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                ranges = maxs - mins
                ranges = torch.clamp(ranges, min=self.epsilon)
                normalized[:, col_idxs] = torch.clamp((log_data - mins) / ranges, 0.0, 1.0)

            elif method == "symlog":
                for i, var in enumerate(self.method_groups[method]):
                    params = self.norm_params[var]
                    col_idx = col_idxs[i]
                    normalized[:, col_idx] = self._symlog_transform(
                        profile[:, col_idx], 
                        params["threshold"], 
                        params["scale_factor"]
                    )

        return normalized

    def normalize_batch(self, batch: torch.Tensor) -> torch.Tensor:
        """
        Normalize a batch of samples using vectorized operations.

        Args:
            batch: Tensor of shape (batch_size, 2*n_species + n_globals + 1)
        Returns:
            Normalized batch tensor
        """
        if batch.device != self.device:
            batch = batch.to(self.device)

        normalized = batch.clone()

        # Apply normalization for each method group on sample columns (vectorized)
        for method, col_idxs in self.sample_col_indices.items():
            if not col_idxs or method == "none":
                continue

            cols = normalized[:, col_idxs]

            if method == "standard":
                var_list = [self.sample_col_vars[col] for col in col_idxs]
                means = torch.stack([self.norm_params[v]["mean"] for v in var_list])
                stds = torch.stack([self.norm_params[v]["std"] for v in var_list])
                normalized[:, col_idxs] = torch.clamp(
                    (cols - means) / stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-standard":
                var_list = [self.sample_col_vars[col] for col in col_idxs]
                log_means = torch.stack([self.norm_params[v]["log_mean"] for v in var_list])
                log_stds = torch.stack([self.norm_params[v]["log_std"] for v in var_list])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                normalized[:, col_idxs] = torch.clamp(
                    (log_data - log_means) / log_stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-min-max":
                var_list = [self.sample_col_vars[col] for col in col_idxs]
                mins = torch.stack([self.norm_params[v]["min"] for v in var_list])
                maxs = torch.stack([self.norm_params[v]["max"] for v in var_list])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                ranges = maxs - mins
                ranges = torch.clamp(ranges, min=self.epsilon)
                normalized[:, col_idxs] = torch.clamp((log_data - mins) / ranges, 0.0, 1.0)

            elif method == "symlog":
                for i, col_idx in enumerate(col_idxs):
                    var = self.sample_col_vars[col_idx]
                    params = self.norm_params[var]
                    normalized[:, col_idx] = self._symlog_transform(
                        batch[:, col_idx],
                        params["threshold"],
                        params["scale_factor"]
                    )

        return normalized

    def denormalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """
        Denormalize a complete profile tensor using vectorized operations.

        Args:
            profile: Normalized tensor of shape (timesteps, n_species + n_globals + 1)
        Returns:
            Denormalized profile tensor in original units
        """
        if profile.device != self.device:
            profile = profile.to(self.device)

        denormalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none":
                continue

            cols = denormalized[:, col_idxs]

            if method == "standard":
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                denormalized[:, col_idxs] = cols * stds + means

            elif method == "log-standard":
                log_means = torch.stack([self.norm_params[var]["log_mean"] for var in self.method_groups[method]])
                log_stds = torch.stack([self.norm_params[var]["log_std"] for var in self.method_groups[method]])
                log_data = cols * log_stds + log_means
                denormalized[:, col_idxs] = torch.pow(10.0, log_data)

            elif method == "log-min-max":
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = maxs - mins
                ranges = torch.clamp(ranges, min=self.epsilon)
                log_data = cols * ranges + mins
                denormalized[:, col_idxs] = torch.pow(10.0, log_data)

            elif method == "symlog":
                for i, var in enumerate(self.method_groups[method]):
                    params = self.norm_params[var]
                    col_idx = col_idxs[i]
                    denormalized[:, col_idx] = self._symlog_inverse(
                        profile[:, col_idx],
                        params["threshold"],
                        params["scale_factor"]
                    )

        return denormalized

    def _symlog_transform(self, data: torch.Tensor, threshold: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
        """Apply symlog transformation with optimized vectorized operations."""
        abs_data = torch.abs(data)
        sign_data = torch.sign(data)

        linear_part = data / threshold
        log_arg = torch.clamp(abs_data / threshold, min=self.epsilon)
        log_part = sign_data * (torch.log10(log_arg) + 1.0)

        result = torch.where(
            abs_data <= threshold,
            linear_part,
            log_part
        ) / scale

        return torch.clamp(result, -self.clamp_value, self.clamp_value)

    def _symlog_inverse(self, data: torch.Tensor, threshold: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
        """
        Apply inverse symlog transformation with robust overflow protection.
        Prevents numerical overflow for extreme values on A100 GPUs.
        """
        scaled_data = data * scale
        abs_scaled = torch.abs(scaled_data)
        sign_scaled = torch.sign(scaled_data)

        # Linear region: |scaled_data| <= 1
        linear_part = scaled_data * threshold
        
        # Log region: |scaled_data| > 1
        # Clamp exponent to prevent overflow
        exponent = abs_scaled - 1.0
        max_safe_exponent = FLOAT32_MAX_EXPONENT - 1  # Leave margin for safety
        
        # Log extreme values that would overflow
        extreme_mask = exponent > max_safe_exponent
        if extreme_mask.any():
            self.logger.warning(
                f"Clamping {extreme_mask.sum().item()} extreme values to prevent overflow "
                f"(max exponent: {exponent.max().item():.2f})"
            )
        
        clamped_exponent = torch.clamp(exponent, max=max_safe_exponent)
        
        # Compute log part with clamped exponent
        log_part = sign_scaled * threshold * torch.pow(10.0, clamped_exponent)

        # Combine linear and log regions
        result = torch.where(
            abs_scaled <= 1.0,
            linear_part,
            log_part
        )

        return result

