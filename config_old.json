{
  "precision": {
    "_comment": [
      "Central dtype control for data + model + loss.",
      "Supported dtype strings: bfloat16, float32, float64 (also bf16/fp32/fp64 aliases).",
      "compute_dtype + amp_mode determine Lightning precision:",
      "  - compute_dtype=bfloat16, amp_mode=mixed -> bf16-mixed (autocast bf16, weights typically fp32)",
      "  - compute_dtype=bfloat16, amp_mode=true  -> bf16-true  (pure bf16; requires model_dtype=input_dtype=bfloat16)",
      "  - compute_dtype=float32 -> 32-true",
      "  - compute_dtype=float64 -> 64-true",
      "model_dtype controls model parameter dtype (weights/buffers).",
      "input_dtype is the dtype used for tensors fed into the model (y, dt, g) inside the LightningModule.",
      "dataset_dtype is the dtype emitted by the Dataset when not preloading to device.",
      "preload_dtype is the dtype used when dataset.preload_to_device=true (tensors stored on accelerator).",
      "loss_dtype controls dtype inside the loss computation and loss buffers."
    ],
    "compute_dtype": "float32",
    "amp_mode": "true",
    "model_dtype": "float32",
    "input_dtype": "float32",
    "dataset_dtype": "float32",
    "preload_dtype": "float32",
    "loss_dtype": "float32"
  },
  "paths": {
    "raw_dir": "data/raw",
    "processed_dir": "data/processed",
    "work_dir": "models/v1_test"
  },
  "system": {
    "device": "auto",
    "log_level": "INFO",
    "seed": 1234
  },
  "runtime": {
    "_comment_checkpoint": [
      "null: no checkpoint (fresh)",
      "path: checkpoint file to load"
    ],
    "_comment_checkpointing": [
      "enabled: enable ModelCheckpoint callback",
      "every_n_epochs: checkpoint period",
      "monitor: metric name to monitor (val_loss, etc.)",
      "save_top_k: keep N best (by monitor)",
      "save_last: also keep last.ckpt if true"
    ],
    "_comment_load_weights_strict": [
      "true: state_dict must match",
      "false: allow missing/extra keys"
    ],
    "mode": "train",
    "checkpoint": null,
    "load_weights_strict": true,
    "accelerator": "auto",
    "devices": "auto",
    "strategy": "auto",
    "accumulate_grad_batches": 1,
    "deterministic": false,
    "enable_progress_bar": false,
    "gradient_clip_val": 0.0,
    "log_every_n_steps": 1000000,
    "checkpointing": {
      "enabled": true,
      "every_n_epochs": 1,
      "monitor": "val_loss",
      "save_top_k": 1,
      "save_last": true
    },
    "torch_compile": {
      "enabled": true,
      "backend": "inductor",
      "mode": "default",
      "dynamic": false,
      "fullgraph": true,
      "compile_forward_step": true,
      "compile_open_loop_unroll": false
    }
  },
  "data": {
    "global_variables": [
      "P",
      "T"
    ],
    "species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ]
  },
  "normalization": {
    "_comment": [
      "Used by preprocessing.py to write normalization.json.",
      "Species normalization is always log-standard: z = (log10(max(y, eps)) - mu) / sigma.",
      "Methods below apply to global variables only.",
      "",
      "epsilon: floor value for log10 to avoid -inf (applied to species and log-min-max globals).",
      "min_std: minimum standard deviation floor; prevents division by near-zero std.",
      "globals_default_method: fallback method for any global variable not listed in 'methods'.",
      "",
      "Supported method strings (with aliases resolved by _canonical_method):",
      "  identity  (aliases: none, '')        -> no transformation",
      "  standard  (default)                  -> z = (x - mean) / std",
      "  min-max   (aliases: minmax, min_max) -> z = (x - min) / (max - min)",
      "  log-min-max (aliases: logminmax, log_min_max, log-minmax) -> z = (log10(max(x,eps)) - log_min) / (log_max - log_min)",
      "",
      "Per-key stats written to normalization.json include:",
      "  species: log_mean, log_std, log_min, log_max, epsilon",
      "  globals: mean, std, min, max, log_min, log_max"
    ],
    "epsilon": 1e-30,
    "min_std": 1e-12,
    "globals_default_method": "standard",
    "methods": {
      "P": "log-min-max",
      "T": "min-max"
    }
  },
  "preprocessing": {
    "_comment": [
      "Used by preprocessing.py to sample and normalize trajectories into NPZ shards.",
      "Splits are deterministic by SHA-1 hash of (seed, file_name, traj_name).",
      "train_fraction is implied as 1 - val_fraction - test_fraction.",
      "",
      "Pipeline stages:",
      "  1. Discover raw HDF5 files matching raw_file_patterns under paths.raw_dir.",
      "  2. Reservoir-sample up to pool_size trajectory groups per file.",
      "  3. For each trajectory, attempt up to samples_per_source_trajectory resampled chunks.",
      "  4. Write physical shards (unnormalized) to _tmp_physical/.",
      "  5. Compute normalization stats from train physical shards (streaming Welford).",
      "  6. Normalize all splits and write final shards + normalization.json.",
      "  7. Write preprocessing_summary.json with counts and rejection breakdown."
    ],

    "_comment_raw_file_patterns": [
      "Glob patterns for discovering HDF5 files under paths.raw_dir.",
      "Can be a list of patterns or a single string.",
      "Default: ['*.h5', '*.hdf5']."
    ],
    "raw_file_patterns": [
      "*.h5",
      "*.hdf5"
    ],

    "_comment_dt": [
      "dt: base timestep (seconds). Used directly when dt_mode='fixed'.",
      "  When dt_mode='per_chunk', dt serves as the default for dt_min and dt_max if they are omitted.",
      "",
      "dt_mode: how dt is assigned per resampled chunk.",
      "  'fixed' (aliases: 'constant') -> every chunk uses dt=dt; dt_min and dt_max are forced to dt.",
      "  'per_chunk' (aliases: 'chunk', 'variable') -> dt is sampled per chunk from [dt_min, dt_max].",
      "",
      "dt_min, dt_max: range for per-chunk dt sampling. Must satisfy 0 < dt_min <= dt_max.",
      "  Also used for dt normalization: dt_norm = clip((log10(dt) - log10(dt_min)) / (log10(dt_max) - log10(dt_min)), 0, 1).",
      "",
      "dt_sampling: distribution for sampling dt when dt_mode='per_chunk'.",
      "  'loguniform' (default; triggered if 'log' appears in string) -> 10^uniform(log10(dt_min), log10(dt_max)).",
      "  'uniform' -> uniform(dt_min, dt_max)."
    ],
    "dt": 100.0,
    "dt_mode": "per_chunk",
    "dt_min": 10.0,
    "dt_max": 100.0,
    "dt_sampling": "loguniform",

    "_comment_n_steps": [
      "Number of uniformly-spaced time points per resampled chunk.",
      "Each chunk has shape [n_steps, n_species] for y and [n_steps-1] for dt.",
      "Larger n_steps captures longer trajectories but requires more memory per sample."
    ],
    "n_steps": 300,

    "_comment_t_min": [
      "Minimum start time for chunk placement (seconds).",
      "The actual start is max(t_min, first_positive_time_in_trajectory).",
      "Times below t_min * 0.5 are excluded from the 'valid' mask.",
      "t_start is sampled log-uniformly in [t_lo, t_hi] where t_hi = t_end - chunk_duration.",
      "Set very small to capture early-time chemistry; set larger to skip stiff transients."
    ],
    "t_min": 0.000,

    "_comment_output_trajectories_per_file": [
      "Target number of resampled chunks to extract per raw HDF5 file.",
      "Sampling stops once this many chunks have been successfully written.",
      "If the file has fewer viable trajectories, fewer chunks are produced."
    ],
    "output_trajectories_per_file": 5000,

    "_comment_shard_size": [
      "Maximum number of samples (resampled chunks) per NPZ shard file.",
      "Buffers are flushed to disk when this count is reached.",
      "Smaller shards improve random access and memory; larger shards reduce file count."
    ],
    "shard_size": 4096,

    "_comment_overwrite": [
      "If true, remove existing _tmp_physical/ and split directories before starting.",
      "If false, raise an error if output directories already exist (prevents accidental mixing)."
    ],
    "overwrite": true,
    "time_key": "t_time",

    "_comment_splits": [
      "val_fraction: fraction of trajectories assigned to the validation split.",
      "test_fraction: fraction of trajectories assigned to the test split.",
      "train_fraction is implicit: 1 - val_fraction - test_fraction.",
      "Assignment is deterministic via SHA-1(seed:filename:trajname) -> uniform [0,1).",
      "Backward-compat aliases: val_split, test_split (used if val_fraction/test_fraction absent)."
    ],
    "val_fraction": 0.1,
    "test_fraction": 0.1,

    "_comment_seed": [
      "RNG seed for all stochastic operations in preprocessing:",
      "  - reservoir sampling of trajectory groups from each HDF5 file",
      "  - shuffling the sampled pool",
      "  - dt sampling (per-chunk mode)",
      "  - t_start sampling (log-uniform within feasible window)",
      "  - split assignment hash (combined with file + trajectory names)",
      "Falls back to system.seed if not specified here."
    ],
    "seed": 1234,

    "_comment_pool_size": [
      "Maximum number of trajectory group keys to reservoir-sample per HDF5 file.",
      "Reservoir sampling avoids loading all keys into memory for very large files.",
      "Set >= total trajectories per file to sample exhaustively."
    ],
    "pool_size": 1000000,

    "_comment_samples_per_source_trajectory": [
      "Number of resampled chunks to attempt from each source trajectory.",
      "Each attempt independently samples a new dt (if per_chunk) and t_start.",
      "The first attempt (sidx=0) anchors t_start at the earliest feasible time;",
      "subsequent attempts sample randomly.",
      "Minimum value is 1 (clamped internally)."
    ],
    "samples_per_source_trajectory": 1,

    "_comment_max_chunk_attempts_per_source": [
      "Maximum random t_start candidates to try when placing a chunk within a trajectory.",
      "If no valid placement is found after this many attempts, the chunk is rejected",
      "with reason 'no_fit_chunk'.",
      "Higher values reduce rejections for trajectories with narrow feasible windows",
      "but increase per-trajectory processing time."
    ],
    "max_chunk_attempts_per_source": 200,

    "_comment_drop_below": [
      "Floor threshold for species mass fractions in raw (physical) space.",
      "If ANY value in y_raw falls below this, the entire trajectory is rejected",
      "with reason 'drop_below'.",
      "Set to match normalization.epsilon or smaller to avoid log10 issues downstream.",
      "Typical: 1e-30 to 1e-35."
    ],
    "drop_below": 1e-30
  },
  "dataset": {
    "windows_per_trajectory": 100,
    "preload_to_device": true,
    "shard_cache_size": 2
  },
  "model": {
    "type": "mlp",
    "activation": "silu",
    "dropout": 0.00,
    "layer_norm": true,
    "layer_norm_eps": 1e-05,
    "predict_delta": true,
    "mlp": {
      "hidden_dims": [1024, 1024, 1024, 1024, 1024, 1024],
      "residual": true
    },
    "autoencoder": {
      "latent_dim": 256,
      "encoder_hidden": 512,
      "decoder_hidden": 512,
      "dynamics_hidden": 512,
      "residual": true,
      "dynamics_residual": true
    }
  },
  "training": {
    "_comment_autoregressive": [
      "false: one-jump mode",
      "true: unroll for rollout_steps"
    ],
    "_comment_checkpoint_mode": [
      "none: fresh training, empty work_dir",
      "resume: restore model+optimizer+epoch",
      "weights_only: load weights, reset state"
    ],
    "_comment_curriculum": [
      "true: ramp rollout_steps over training",
      "mode: linear or cosine"
    ],
    "_comment_rollout_steps": [
      "1: required for one-jump mode",
      ">1: needs autoregressive enabled"
    ],
    "batch_size": 8192,
    "max_epochs": 200,
    "checkpoint_mode": "none",
    "num_workers": 0,
    "pin_memory": false,
    "persistent_workers": false,
    "prefetch_factor": null,
    "rollout_steps": 1,
    "loss": {
      "lambda_log10_mae": 1.0,
      "lambda_z_mse": 0.1
    },
    "optimizer": {
      "name": "adamw",
      "lr": 0.0001,
      "weight_decay": 0.0001,
      "exclude_norm_and_bias_from_weight_decay": true,
      "betas": [
        0.9,
        0.999
      ],
      "eps": 1e-08,
      "fused": true,
      "foreach": true
    },
    "scheduler": {
      "enabled": true,
      "type": "cosine_with_warmup",
      "warmup_epochs": 10,
      "min_lr_ratio": 0.01,
      "_comment_plateau": [
        "Keys below are only used when scheduler.type == 'plateau'."
      ],
      "factor": 0.5,
      "patience": 10,
      "threshold": 0.0001,
      "min_lr": 0.0,
      "mode": "min",
      "monitor": "val_loss"
    },
    "autoregressive_training": {
      "_comment_backward_per_step": [
        "true: backward each step, saves memory",
        "false: accumulate graph, faster"
      ],
      "_comment_detach": [
        "true: no BPTT, independent gradients",
        "false: full BPTT through sequence"
      ],
      "_comment_skip_steps": [
        "0: no warmup",
        "N: run N steps without gradient"
      ],
      "enabled": false,
      "skip_steps": 0,
      "detach_between_steps": true,
      "backward_per_step": true
    },
    "curriculum": {
      "enabled": false,
      "start_steps": 1,
      "end_steps": 1,
      "mode": "linear",
      "ramp_epochs": 0
    }
  }
}