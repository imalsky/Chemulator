================================================================================
FLOW-MAP DEEPONET: COMPLETE TECHNICAL GUIDE
TABLE OF CONTENTS

Introduction and Overview
Core Concepts: What is a Neural Operator?
Understanding MLPs (Multi-Layer Perceptrons)
The Complete Data Flow Pipeline
Codebase Architecture
Detailed File Reference
How to Run the Code
Configuration Guide
Performance and Optimization
Troubleshooting and Common Issues

================================================================================

INTRODUCTION AND OVERVIEW
================================================================================

Flow-map DeepONet is a neural operator framework for learning the evolution of
dynamical systems. Given a system's state at time t0, it predicts the state at
any future time t1 without solving differential equations.
Key Capabilities:

Learns mappings between infinite-dimensional function spaces
Handles multiple species/variables simultaneously
Predicts multiple future timesteps in parallel
Scales to large datasets via GPU acceleration
Supports distributed preprocessing via MPI
Includes production features: checkpointing, restart, validation

The Problem It Solves:
Many physical systems evolve according to differential equations:
dy/dt = f(y, t, parameters)
Traditional numerical methods require:

Knowing the exact form of f
Computing many intermediate timesteps
High computational cost for long time horizons

Flow-map DeepONet learns the solution operator directly from data:
y(t1) = Φ(y(t0), t1-t0, parameters)
This enables:

Single-step prediction to any future time
No need to know the governing equations
Amortized O(1) inference cost

================================================================================
2. CORE CONCEPTS: WHAT IS A NEURAL OPERATOR?
Traditional Neural Networks vs Neural Operators
Traditional NN:  Points → Points
Input: x ∈ ℝⁿ
Output: y ∈ ℝᵐ
Example: Classify an image
Neural Operator: Functions → Functions
Input: u(·) ∈ Function Space
Output: v(·) ∈ Function Space
Example: Map initial conditions to solutions
The DeepONet Architecture
DeepONet decomposes operator learning into two components:

BRANCH NETWORK: Encodes the input function

Input: Initial state y₀ and parameters θ
Output: Basis coefficients φ ∈ ℝᵖ
Learns "what" the initial condition represents


TRUNK NETWORK: Encodes the query location

Input: Time difference Δt
Output: Basis functions ψ(Δt) ∈ ℝᵖ
Learns "when" we want to evaluate


COMBINATION: Reconstructs the output

Element-wise product: φ ⊙ ψ
Linear projection to output dimension
Optional residual connection



Mathematical Foundation
The architecture is motivated by the universal approximation theorem for
operators. Any continuous operator G can be approximated as:
G(u)(y) ≈ Σᵢ₌₁ᵖ φᵢ(u) · ψᵢ(y)
Where:

u is the input function (initial condition)
y is the evaluation point (time)
p is the number of basis functions
φᵢ are branch network outputs
ψᵢ are trunk network outputs

================================================================================
3. UNDERSTANDING MLPs (MULTI-LAYER PERCEPTRONS)
What is an MLP?
An MLP is a fully-connected feedforward neural network. Each neuron in a layer
connects to every neuron in the next layer.
Structure of a 3-Layer MLP:
Input (d_in neurons)
     ↓
Hidden Layer 1 (h1 neurons)
- Linear: z₁ = W₁x + b₁
- Activation: a₁ = σ(z₁)
     ↓
Hidden Layer 2 (h2 neurons)
- Linear: z₂ = W₂a₁ + b₂
- Activation: a₂ = σ(z₂)
     ↓
Output Layer (d_out neurons)
- Linear: y = W₃a₂ + b₃
Key Components:

LINEAR TRANSFORMATION (Wx + b):

W: Weight matrix (learned parameters)
x: Input vector
b: Bias vector (learned parameters)
Performs affine transformation of input


ACTIVATION FUNCTIONS (σ):

ReLU: σ(x) = max(0, x)
GELU: σ(x) = x · Φ(x) where Φ is Gaussian CDF
Tanh: σ(x) = (eˣ - e⁻ˣ)/(eˣ + e⁻ˣ)
Introduces non-linearity for complex function approximation


DROPOUT (optional):

Randomly zeros neurons during training
Probability p of dropping each neuron
Prevents overfitting by reducing co-adaptation



Why MLPs Work:
UNIVERSAL APPROXIMATION: A sufficiently wide MLP with one hidden layer can
approximate any continuous function on a compact set to arbitrary accuracy.
In practice:

Deeper networks are more parameter-efficient
Multiple layers create hierarchical representations
Lower layers learn simple features
Higher layers combine them into complex patterns

In Flow-map DeepONet:
BRANCH MLP:
Input: [y₀, θ] concatenated (dimension S + G)
Hidden: 3 layers × 512 neurons (configurable)
Output: p basis coefficients
Role: Encode initial condition into latent space
TRUNK MLP:
Input: Δt (dimension 1)
Hidden: 2 layers × 512 neurons (configurable)
Output: p basis functions
Role: Learn time-dependent basis functions
================================================================================
4. THE COMPLETE DATA FLOW PIPELINE
STAGE 1: RAW DATA INGESTION
Input Format: HDF5 files containing trajectory data
data/raw/
├── simulation_001.h5
├── simulation_002.h5
└── ...
HDF5 Structure:
/trajectory_001/
├── t_time: [T] array of time points
├── species_1: [T] or [T,1] concentration over time
├── species_2: [T] or [T,1] concentration over time
└── attributes:
├── Re: Reynolds number (global parameter)
└── Pr: Prandtl number (global parameter)
STAGE 2: PREPROCESSING PIPELINE
The preprocessor (preprocessor.py) performs:

SCANNING & VALIDATION:

Read all HDF5 files
Check time monotonicity (must be strictly increasing)
Remove trajectories with NaN or invalid values
Verify all trajectories share the same time grid
Filter values below threshold (e.g., negative concentrations)


STATISTICS COMPUTATION:
For each variable in TRAINING data only:

Mean and standard deviation
Min and max values
Log-domain statistics (for log-normal distributions)
Time difference (Δt) statistics for normalization


DATA SPLITTING:
Deterministic split using hash-based assignment:

Train: 70% (configurable)
Validation: 15%
Test: 15%


SHARD WRITING:
Convert to NPZ format for fast loading:
data/processed/
├── train/
│   ├── shard_train_sim001_00000.npz
│   └── ...
├── validation/
│   └── ...
├── test/
│   └── ...
└── normalization.json
NPZ Contents:

x0: [N, S] initial conditions
globals: [N, G] global parameters
t_vec: [T] or [N, T] time vectors
y_mat: [N, T, S] full trajectories



STAGE 3: DATASET CONSTRUCTION
The dataset (dataset.py) implements:

LOADING PHASE:

Read all NPZ shards for the split
Optionally transfer to GPU memory
Apply normalization using statistics


SAMPLING STRATEGY:
Per epoch, for each trajectory:

Sample random anchor points i ∈ [0, T-1-min_steps]
Sample target points j ∈ (i+min_steps, i+max_steps]
Forms (y_i, Δt, y_j) training pairs


NORMALIZATION:

Species: Various methods (standard, min-max, log-standard)
Time differences: Log-min-max normalization
Global parameters: Standard normalization


BATCH ASSEMBLY:
Returns tuple for each batch:

y_i: [B, S] initial states
dt_norm: [B, K, 1] normalized time differences
y_j: [B, K, S] target states
g: [B, G] global parameters
aux: Metadata (indices)
k_mask: [B, K] validity mask (optional)



STAGE 4: TRAINING LOOP
The trainer (trainer.py) orchestrates:

FORWARD PASS:
model(y_i, dt_norm, g) → predictions

Branch network: [y_i, g] → φ
Trunk network: dt_norm → ψ
Combine: φ ⊙ ψ → linear → predictions
Add residual if configured


LOSS COMPUTATION:
MSE(predictions, y_j) with optional masking
BACKWARD PASS:

Compute gradients via backpropagation
Clip gradients if configured
Update weights via AdamW optimizer


LEARNING RATE SCHEDULE:

Linear warmup for initial epochs
Cosine annealing to min_lr


VALIDATION:

Run validation every epoch
Save best model based on validation loss
Log metrics to CSV



STAGE 5: OUTPUT ARTIFACTS
models/flowmap-deeponet/
├── best_model.pt       # Best validation weights
├── last.ckpt          # Full checkpoint for restart
├── config.json        # Configuration snapshot
└── train_log.csv      # Training metrics
================================================================================
5. CODEBASE ARCHITECTURE
Directory Structure:
project/
│
├── config/
│   └── config.jsonc          # Main configuration (JSON with comments)
│
├── data/
│   ├── raw/                  # Input HDF5 files
│   │   ├── simulation_001.h5
│   │   └── ...
│   │
│   └── processed/            # Preprocessed data
│       ├── train/
│       │   └── *.npz
│       ├── validation/
│       │   └── *.npz
│       ├── test/
│       │   └── *.npz
│       ├── normalization.json
│       └── preprocessing_summary.json
│
├── models/
│   └── flowmap-deeponet/     # Training outputs
│       ├── best_model.pt
│       ├── last.ckpt
│       ├── best.ckpt
│       ├── config.json
│       └── train_log.csv
│
└── src/
├── main.py               # Entry point
├── preprocessor.py       # Data preprocessing
├── preprocessor_utils.py # Preprocessing helpers
├── dataset.py            # PyTorch dataset
├── model.py              # Neural network
├── trainer.py            # Training loop
├── normalizer.py         # Normalization logic
├── hardware.py           # GPU configuration
├── utils.py              # General utilities
└── restart.py            # Restart handler
Design Principles:

SEPARATION OF CONCERNS:

Each module has a single responsibility
Clear interfaces between components
Minimal coupling between files


CONFIGURATION-DRIVEN:

Single source of truth (config.jsonc)
Runtime validation of parameters
Auto-detection where sensible


ROBUSTNESS:

Extensive error checking
Graceful handling of edge cases
Atomic file operations
Checkpointing for fault tolerance


PERFORMANCE:

GPU-resident data when possible
Vectorized operations
Mixed precision training
Efficient data formats (NPZ)


REPRODUCIBILITY:

Deterministic data splits
Seed management
Configuration snapshots
Comprehensive logging



================================================================================
6. DETAILED FILE REFERENCE
main.py (Entry Point)
Purpose: Orchestrates the entire training pipeline
Key Functions:

main(): Primary entry point
ensure_preprocessed_data(): Checks for/creates processed data
hydrate_config_from_processed(): Loads metadata from processed data
build_datasets_and_loaders(): Creates PyTorch datasets
build_model(): Instantiates the neural network

Flow:

Load configuration
Setup logging and hardware
Ensure preprocessed data exists
Create datasets and model
Initialize trainer
Run training loop

preprocessor.py (Data Preparation)
Purpose: Convert raw HDF5 to normalized NPZ shards
Classes:

DataPreprocessor: Main preprocessing pipeline

Key Methods:

run(): Execute full preprocessing
_scan_files(): Validate and inventory trajectories
_compute_dt_specification(): Calculate time normalization
_write_shards_and_collect_stats(): Create NPZ files
_finalize_manifest(): Generate normalization.json

Features:

MPI support for parallel processing
Automatic species variable detection
Robust error handling for corrupted data
Memory-efficient streaming processing

dataset.py (PyTorch Dataset)
Purpose: Efficient data loading and batch generation
Classes:

FlowMapPairsDataset: Main dataset class

Key Methods:

init(): Load all data to memory/GPU
set_epoch(): Deterministic per-epoch sampling
_sample_target_indices(): Generate time pairs
_gather_batch(): Assemble training batch

Features:

GPU-resident data for maximum throughput
Multiple sampling strategies
Multi-time predictions (K > 1)
Automatic normalization

model.py (Neural Network)
Purpose: Implement FlowMapDeepONet architecture
Classes:

FlowMapDeepONet: Main model
BranchNet: Process initial conditions
TrunkNet: Process time coordinates
build_mlp(): MLP factory function

Architecture:

Configurable depth and width
Multiple activation functions
Dropout regularization
Residual connections

trainer.py (Training Loop)
Purpose: Manage training process
Classes:

Trainer: Training orchestrator
CosineWarmupScheduler: Learning rate schedule

Features:

Mixed precision training (FP16/BF16)
Gradient clipping
Checkpoint saving/loading
SIGTERM handling for preemption
CSV logging
Validation loop

normalizer.py (Data Normalization)
Purpose: Centralized normalization logic
Classes:

NormalizationHelper: Normalization operations

Methods:

normalize(): Apply normalization
denormalize(): Inverse transformation
normalize_dt_from_phys(): Time difference normalization

Supports:

Standard: (x - mean) / std
Min-max: (x - min) / (max - min)
Log-standard: (log(x) - log_mean) / log_std
Log-min-max: (log(x) - log_min) / (log_max - log_min)

hardware.py (Hardware Configuration)
Purpose: Optimize hardware settings
Functions:

setup_device(): Select GPU/CPU
optimize_hardware(): Configure performance settings

Optimizations:

TensorFloat-32 (TF32) for Ampere GPUs
cuDNN autotuner
OpenMP thread configuration
Memory reporting

utils.py (General Utilities)
Purpose: Common helper functions
Functions:

setup_logging(): Configure logging
seed_everything(): Set random seeds
load_json_config(): Load JSONC files
dump_json(): Atomic JSON writing

Features:

JSONC (JSON with comments) support
Atomic file operations
Consistent formatting

preprocessor_utils.py (Preprocessing Helpers)
Purpose: Support functions for preprocessing
Classes:

WelfordAccumulator: Online statistics
RunningStatistics: Multi-domain statistics

Functions:

scan_hdf5_file_worker(): Parallel file scanning
deterministic_hash(): Reproducible splitting
get_normalization_flags(): Method requirements

restart.py (Restart Handler)
Purpose: Resume interrupted training
Functions:

main(): Restart entry point
pick_checkpoint(): Select best checkpoint
load_main_module(): Dynamic module loading

Usage:

Automatically finds latest checkpoint
Sets RESUME environment variable
Calls main.py with resume enabled

================================================================================
7. HOW TO RUN THE CODE
Installation

Create Python environment:
conda create -n flowmap python=3.10
conda activate flowmap
Install dependencies:
Core requirements
pip install torch numpy h5py
Optional for HPC
pip install mpi4py
Optional for better progress bars
pip install tqdm

Data Preparation

Organize HDF5 files:
mkdir -p data/raw
cp /path/to/simulations/*.h5 data/raw/
Verify HDF5 structure:
import h5py
with h5py.File('data/raw/sim001.h5', 'r') as f:
print(f.keys())  # Should show trajectory names
print(f['traj001'].keys())  # Should show variables

Configuration
Edit config/config.jsonc:
{
"paths": {
"raw_data_files": [],  // Leave empty for auto-detection
"processed_data_dir": "data/processed",
"work_dir": "models/flowmap-deeponet"
},
"data": {
"species_variables": [],  // Auto-detected if empty
"global_variables": ["Re", "Pr"],
"time_variable": "t_time"
},
"normalization": {
"default_method": "log-standard",
"methods": {
"species_1": "log-standard",
"species_2": "standard"
},
"epsilon": 1e-30,
"min_std": 1e-10,
"clamp_value": 10.0
},
"model": {
"p": 128,  // Basis dimension
"branch_width": 512,
"branch_depth": 3,
"trunk_layers": [512, 512],
"activation": "gelu"
},
"training": {
"epochs": 200,
"batch_size": 512,
"lr": 1e-3,
"pairs_per_traj": 64
}
}
Basic Training
Single GPU training
python src/main.py
The pipeline will:
1. Preprocess data (first run only)
2. Load datasets to GPU
3. Train model
4. Save checkpoints
Restarting After Interruption
Method 1: Using restart.py
python src/restart.py --work_dir models/flowmap-deeponet
Method 2: Environment variable
export RESUME=auto
python src/main.py
Method 3: Specific checkpoint
export RESUME=models/flowmap-deeponet/last.ckpt
python src/main.py
Multi-GPU Training (MPI)
Preprocessing with MPI (4 processes)
mpirun -n 4 python src/main.py
Note: Training itself uses single GPU; MPI accelerates preprocessing
PBS/Slurm Job Script
#!/bin/bash
#PBS -N flowmap_training
#PBS -l select=1:ncpus=8:ngpus=1:mem=32gb
#PBS -l walltime=24:00:00
#PBS -j oe
Load modules
module load cuda/11.8
module load python/3.10
Activate environment
cd $PBS_O_WORKDIR
source activate flowmap
Run training
python src/main.py
Submit continuation job
if [ -f models/flowmap-deeponet/last.ckpt ]; then
qsub -W depend=afterany:$PBS_JOBID restart.pbs
fi
Monitoring Training
Watch training progress
tail -f models/flowmap-deeponet/train_log.csv
Plot learning curves
python -c "
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('models/flowmap-deeponet/train_log.csv')
df[['train_loss', 'val_loss']].plot(logy=True)
plt.show()
"
================================================================================
8. CONFIGURATION GUIDE
Critical Parameters
MODEL ARCHITECTURE:
p: Basis dimension (64-256 typical)
- Higher = more expressive but slower
- Start with 128
branch_width: Hidden layer width (256-1024)
- Wider = more capacity
- Memory scales as O(width²)
branch_depth: Number of layers (2-5)
- Deeper = more hierarchical features
- Diminishing returns beyond 4
TRAINING:
batch_size: Samples per iteration
- Limited by GPU memory
- Larger = more stable gradients
- Try largest power of 2 that fits
lr: Learning rate (1e-4 to 1e-2)
- Higher for larger batches
- Lower for complex data
pairs_per_traj: Samples per trajectory per epoch
- Higher = better coverage
- Affects epoch length
min_steps/max_steps: Time gap constraints
- Larger range = harder problem
- Start narrow, gradually increase
NORMALIZATION:
methods: Per-variable normalization
- "standard": Gaussian data
- "log-standard": Log-normal data
- "min-max": Bounded data
- "log-min-max": Positive bounded data
DATA:
val_fraction: Validation set size (0.1-0.2)
test_fraction: Test set size (0.1-0.2)
use_fraction: Data subsampling (0-1)
- Use < 1 for debugging
Advanced Options
MULTI-TIME PREDICTIONS:
"dataset": {
"multi_time_per_anchor": true,
"times_per_anchor": 4  // Predict 4 times simultaneously
}
GPU OPTIMIZATION:
"dataset": {
"preload_train_to_gpu": true,  // Load entire dataset to GPU
"preload_val_to_gpu": true
}
MIXED PRECISION:
"mixed_precision": {
"mode": "bf16"  // or "fp16" for older GPUs
}
REGULARIZATION:
"model": {
"dropout": 0.1,  // Dropout probability
"branch_dropout": 0.1,  // Override for branch
"trunk_dropout": 0.05   // Override for trunk
}
FAULT TOLERANCE:
"training": {
"auto_resume": true,  // Auto-restart from checkpoint
"save_every_epochs": 5  // Checkpoint frequency
}
================================================================================
9. PERFORMANCE AND OPTIMIZATION
Memory Optimization
GPU MEMORY USAGE:

Dataset: N × T × S × 4 bytes (float32)
Model: ~10-50 MB depending on architecture
Gradients: Same size as model
Optimizer state: 2× model size (AdamW)
Batch: B × K × S × 4 bytes

REDUCING MEMORY:

Decrease batch_size
Use mixed precision (saves ~50%)
Reduce model width/depth
Don't preload to GPU
Use gradient accumulation

Speed Optimization
PREPROCESSING:

Use MPI for parallel scanning
Increase trajectories_per_shard
Use compressed NPZ (trades CPU for disk)

TRAINING:

Enable TF32 on Ampere GPUs
Use mixed precision training
Preload datasets to GPU
Increase batch size (if memory allows)
Use torch.compile (PyTorch 2.0+)

DATALOADER:

Set num_workers=0 for GPU-resident data
Use pin_memory=False with GPU data
Increase prefetch_factor for CPU data

Scaling Guidelines
SMALL DATASETS (< 1000 trajectories):

Increase pairs_per_traj (128-256)
Use smaller model (p=64)
Add dropout (0.2-0.3)
Longer training (500+ epochs)

LARGE DATASETS (> 10000 trajectories):

Decrease pairs_per_traj (16-32)
Use larger model (p=256)
Less regularization
Can train for fewer epochs

LONG TRAJECTORIES (T > 1000):

Increase max_steps gradually
Use hierarchical sampling
Consider trajectory chunking

MANY SPECIES (S > 100):

Use dimension reduction first
Increase model capacity
Consider multi-scale architecture

================================================================================
10. TROUBLESHOOTING AND COMMON ISSUES
Installation Issues
Problem: ImportError: No module named 'h5py'
Solution: pip install h5py
Problem: CUDA out of memory
Solution:

Reduce batch_size
Set preload_to_gpu: false
Use mixed precision
Reduce model size

Problem: MPI errors during preprocessing
Solution:

Check MPI installation: mpirun --version
Ensure all nodes have same environment
Try serial mode first (no MPI)

Data Issues
Problem: "No valid trajectories found"
Causes:

Incorrect time_variable name
Non-monotonic time data
Too many NaN values
Values below min_value_threshold
Debug:
Check HDF5 structure manually
Lower min_value_threshold
Check time monotonicity

Problem: "Time grid differs across files"
Solution:

Ensure all simulations use same time points
Interpolate to common grid if needed
Use subset with matching grids

Problem: Species auto-detection fails
Solution:

Manually specify in config:
"species_variables": ["u", "v", "w"]
Check HDF5 dataset names
Ensure consistent naming across files

Training Issues
Problem: Loss is NaN
Causes:

Learning rate too high
Gradient explosion
Numerical instability
Solutions:
Reduce learning rate
Enable gradient clipping
Check normalization statistics
Increase epsilon in normalization

Problem: Validation loss increases (overfitting)
Solutions:

Add dropout
Reduce model size
Increase training data
Add weight decay
Use data augmentation

Problem: Training is slow
Solutions:

Enable mixed precision
Increase batch size
Use GPU-resident datasets
Enable TF32 on A100
Reduce validation frequency

Problem: Can't resume from checkpoint
Debug:

Check checkpoint exists: ls models/*/last.ckpt
Verify checkpoint not corrupted:
python -c "import torch; torch.load('last.ckpt')"
Check config compatibility
Use restart.py for automatic detection

Memory Issues
Problem: "DataLoader worker (pid X) is killed by signal: Killed"
Cause: System OOM killer
Solutions:

Reduce num_workers
Decrease batch size
Don't use persistent_workers
Increase system RAM

Problem: GPU memory fragmentation
Symptoms: OOM despite having free memory
Solutions:

Set PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
Restart Python interpreter
Use gradient checkpointing

Performance Issues
Problem: GPU utilization is low
Diagnosis:
nvidia-smi dmon -s pucvmet
Causes:

CPU bottleneck in data loading
Small batch size
Slow data preprocessing
Solutions:
Preload to GPU
Increase batch size
Profile with PyTorch profiler

Problem: Preprocessing takes hours
Solutions:

Use MPI: mpirun -n 8 python src/main.py
Increase HDF5 chunk size
Use faster storage (SSD)
Reduce validation frequency during scan

Common Configuration Mistakes

Incompatible min_steps/max_steps:

max_steps must be >= min_steps
max_steps must be < T (trajectory length)


Wrong normalization method:

Use log-methods for positive-only data
Check data distribution first


Mismatched species variables:

Config doesn't match HDF5 structure
Different ordering in files


Too aggressive learning rate:

Start with 1e-4 and increase
Use warmup for large rates


Insufficient validation data:

Need at least batch_size samples
Increase val_fraction if needed



================================================================================
END OF GUIDE
For questions, bug reports, or contributions, please refer to the project
repository. This guide represents the system as of the last update and may
not reflect the most recent changes.