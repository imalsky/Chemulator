{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eccdae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                 Initialization                                 \n",
      "================================================================================\n",
      "Loading model and configuration artifacts...\n",
      "Pre-loading 14983 test profiles into memory...\n",
      "Pre-loading complete.\n",
      "Setup complete. Model loaded on device: MPS\n",
      "\n",
      "================================================================================\n",
      "                             Performance Benchmark                             \n",
      "================================================================================\n",
      "Batch Size:                  4096\n",
      "Device:                      MPS\n",
      "Average time per batch:      0.7808 ms\n",
      "Average time per single pred: 0.1906 µs (microseconds)\n",
      "\n",
      "================================================================================\n",
      "                          Single Prediction Validation                          \n",
      "================================================================================\n",
      "  - Predicting at time index 39 (t ≈ 1.0000e+04 s)\n",
      "\n",
      "Species  Predicted Value  True Value  Abs. Error  Rel. Error (%)\n",
      "   C2H2       1.1393e-01  1.5416e-01  4.0223e-02      2.6092e+01\n",
      "    CH4       7.2136e-02  8.1398e-02  9.2614e-03      1.1378e+01\n",
      "    CO2       1.3531e-12  1.9115e-12  5.5839e-13      2.9213e+01\n",
      "     CO       5.1327e-04  5.0093e-04  1.2335e-05      2.4624e+00\n",
      "    H2O       8.1341e-09  9.4394e-09  1.3053e-09      1.3828e+01\n",
      "     H2       7.1024e-01  7.1132e-01  1.0850e-03      1.5254e-01\n",
      "    HCN       2.5788e-02  2.6431e-02  6.4295e-04      2.4326e+00\n",
      "      H       8.8743e-05  8.2399e-05  6.3440e-06      7.6990e+00\n",
      "     N2       5.3715e-03  5.5660e-03  1.9442e-04      3.4929e+00\n",
      "    NH3       7.8624e-06  8.6824e-06  8.1999e-07      9.4443e+00\n",
      "     OH       5.1862e-14  6.7752e-14  1.5890e-14      2.3453e+01\n",
      "      O       5.0505e-18  5.3944e-18  3.4387e-19      6.3628e+00\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Setup: Add project source to the Python path ---\n",
    "# Ensure this path is correct for your project structure\n",
    "try:\n",
    "    project_root = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent\n",
    "    \n",
    "src_path = project_root / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    \n",
    "# It's good practice to handle potential import errors\n",
    "try:\n",
    "    from utils import load_config\n",
    "    from normalizer import DataNormalizer\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules from {src_path}.\")\n",
    "    print(\"Please ensure the script is run from the 'scripts' directory or adjust the project_root path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def print_header(title: str):\n",
    "    \"\"\"Prints a formatted, centered header to the console.\"\"\"\n",
    "    width = 80\n",
    "    padding = (width - len(title) - 2) // 2\n",
    "    print(\"\\n\" + \"=\" * width)\n",
    "    print(\" \" * padding, title, \" \" * padding)\n",
    "    print(\"=\" * width)\n",
    "\n",
    "def prepare_batch_from_memory(batch_size: int, all_test_data: list, species_vars: list, global_vars: list, device: torch.device):\n",
    "    \"\"\"\n",
    "    Creates a batch of input tensors by sampling from pre-loaded in-memory data.\n",
    "    \"\"\"\n",
    "    batch_inputs_list = []\n",
    "    selected_profiles = random.choices(all_test_data, k=batch_size)\n",
    "    \n",
    "    for norm_profile in selected_profiles:\n",
    "        initial_species = [norm_profile[key][0] for key in species_vars]\n",
    "        global_conds = [norm_profile[key] for key in global_vars]\n",
    "        final_norm_time = norm_profile[\"t_time\"][-1]\n",
    "        input_list = initial_species + global_conds + [final_norm_time]\n",
    "        batch_inputs_list.append(input_list)\n",
    "        \n",
    "    return torch.tensor(batch_inputs_list, dtype=torch.float32, device=device)\n",
    "\n",
    "def run_benchmark_and_validate():\n",
    "    \"\"\"\n",
    "    Main function to load artifacts, run a performance benchmark on Apple Silicon (MPS)\n",
    "    if available, and then validate a single prediction.\n",
    "    \"\"\"\n",
    "    # 1. LOAD ARTIFACTS\n",
    "    # --------------------------------------------------------------------------\n",
    "    print_header(\"Initialization\")\n",
    "    print(\"Loading model and configuration artifacts...\")\n",
    "    CONFIG_FILE = project_root / \"inputs/model_input_params.jsonc\"\n",
    "    DATA_ROOT = project_root / \"data\"\n",
    "    \n",
    "    config = load_config(CONFIG_FILE)\n",
    "    if not config:\n",
    "        print(\"Error: Could not load configuration. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    model_folder = DATA_ROOT / config[\"output_paths_config\"][\"fixed_model_foldername\"]\n",
    "    normalized_data_folder = DATA_ROOT / config[\"data_paths_config\"][\"normalized_profiles_foldername\"]\n",
    "    \n",
    "    # ============================ MPS CHANGE 1: START ============================\n",
    "    # Updated device selection logic for Apple Silicon\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    # ============================ MPS CHANGE 1: END ==============================\n",
    "    \n",
    "    model_path = model_folder / \"best_model_jit.pt\"\n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model file not found at {model_path}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Load the JIT model and move it to the selected device (CPU, CUDA, or MPS)\n",
    "    model = torch.jit.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    \n",
    "    norm_meta_path = normalized_data_folder / \"normalization_metadata.json\"\n",
    "    if not norm_meta_path.exists():\n",
    "        print(f\"Error: Normalization metadata not found at {norm_meta_path}. Exiting.\")\n",
    "        return\n",
    "    with norm_meta_path.open(\"r\") as f:\n",
    "        norm_metadata = json.load(f)\n",
    "\n",
    "    test_info_path = model_folder / \"test_set_info.json\"\n",
    "    if not test_info_path.exists():\n",
    "        print(f\"Error: Test set info file not found at {test_info_path}. Exiting.\")\n",
    "        return\n",
    "    with test_info_path.open(\"r\") as f:\n",
    "        test_filenames = json.load(f)[\"test_filenames\"]\n",
    "        \n",
    "    print(f\"Pre-loading {len(test_filenames)} test profiles into memory...\")\n",
    "    all_test_data_in_memory = []\n",
    "    for filename in test_filenames:\n",
    "        with (normalized_data_folder / filename).open(\"r\") as f:\n",
    "            all_test_data_in_memory.append(json.load(f))\n",
    "    print(\"Pre-loading complete.\")\n",
    "\n",
    "    species_vars = sorted(config[\"species_variables\"])\n",
    "    global_vars = sorted(config[\"global_variables\"])\n",
    "    print(f\"Setup complete. Model loaded on device: {device.type.upper()}\")\n",
    "    \n",
    "    # 2. RUN PERFORMANCE BENCHMARK\n",
    "    # --------------------------------------------------------------------------\n",
    "    print_header(\"Performance Benchmark\")\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WARMUP_RUNS = 50\n",
    "    NUM_TIMING_RUNS = 100\n",
    "\n",
    "    batch_tensor = prepare_batch_from_memory(BATCH_SIZE, all_test_data_in_memory, species_vars, global_vars, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(NUM_WARMUP_RUNS):\n",
    "            _ = model(batch_tensor)\n",
    "    \n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(NUM_TIMING_RUNS):\n",
    "            # For MPS, perf_counter is sufficient as PyTorch handles synchronization\n",
    "            start_time = time.perf_counter()\n",
    "            _ = model(batch_tensor)\n",
    "            end_time = time.perf_counter()\n",
    "            timings.append(end_time - start_time)\n",
    "\n",
    "    # 3. REPORT BENCHMARK RESULTS\n",
    "    # --------------------------------------------------------------------------\n",
    "    total_time = sum(timings)\n",
    "    avg_batch_time_ms = (total_time / NUM_TIMING_RUNS) * 1000\n",
    "    avg_prediction_time_us = (avg_batch_time_ms / BATCH_SIZE) * 1000\n",
    "\n",
    "    print(f\"{'Batch Size:':<28} {BATCH_SIZE}\")\n",
    "    print(f\"{'Device:':<28} {device.type.upper()}\")\n",
    "    print(f\"{'Average time per batch:':<28} {avg_batch_time_ms:.4f} ms\")\n",
    "    print(f\"{'Average time per single pred:':<28} {avg_prediction_time_us:.4f} µs (microseconds)\")\n",
    "\n",
    "    # 4. RUN SINGLE PREDICTION VALIDATION\n",
    "    # --------------------------------------------------------------------------\n",
    "    print_header(\"Single Prediction Validation\")\n",
    "    norm_profile = random.choice(all_test_data_in_memory)\n",
    "    \n",
    "    query_time_idx = random.randint(1, len(norm_profile[\"t_time\"]) - 1)\n",
    "    query_time_idx = len(norm_profile[\"t_time\"]) - 1\n",
    "\n",
    "\n",
    "    norm_time_to_predict = norm_profile[\"t_time\"][query_time_idx]\n",
    "    \n",
    "    initial_species = [norm_profile[key][0] for key in species_vars]\n",
    "    global_conds = [norm_profile[key] for key in global_vars]\n",
    "    \n",
    "    input_vector = torch.tensor(\n",
    "        initial_species + global_conds + [norm_time_to_predict], dtype=torch.float32, device=device\n",
    "    ).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        norm_prediction_tensor = model(input_vector).squeeze(0)\n",
    "\n",
    "    # Move the prediction back to the CPU for numpy/pandas operations\n",
    "    norm_prediction_tensor_cpu = norm_prediction_tensor.cpu()\n",
    "    \n",
    "    norm_true_values_tensor = torch.tensor([norm_profile[key][query_time_idx] for key in species_vars])\n",
    "    \n",
    "    # 5. DENORMALIZE AND REPORT VALIDATION RESULTS\n",
    "    # --------------------------------------------------------------------------\n",
    "    results = []\n",
    "    real_time_to_predict = DataNormalizer.denormalize(norm_time_to_predict, norm_metadata, \"t_time\")\n",
    "    print(f\"  - Predicting at time index {query_time_idx} (t ≈ {real_time_to_predict:.4e} s)\")\n",
    "\n",
    "    for i, key in enumerate(species_vars):\n",
    "        predicted_val = DataNormalizer.denormalize(norm_prediction_tensor_cpu[i], norm_metadata, key).item()\n",
    "        true_val = DataNormalizer.denormalize(norm_true_values_tensor[i], norm_metadata, key).item()\n",
    "        \n",
    "        results.append({\n",
    "            \"Species\": key.replace('_evolution', ''),\n",
    "            \"Predicted Value\": predicted_val,\n",
    "            \"True Value\": true_val,\n",
    "            \"Abs. Error\": abs(predicted_val - true_val),\n",
    "            \"Rel. Error (%)\": abs(predicted_val - true_val) / (true_val + 1e-20) * 100 if true_val != 0 else float('inf')\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    pd.options.display.float_format = '{:,.4e}'.format\n",
    "    print(\"\\n\" + df.to_string(index=False))\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmark_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3cab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
