2025-07-12 12:04:33 | INFO     | utils.utils - Logging system initialized
2025-07-12 12:04:33 | INFO     | __main__ - ================================================================================
2025-07-12 12:04:33 | INFO     | __main__ - Chemical Kinetics Neural Network Pipeline
2025-07-12 12:04:33 | INFO     | __main__ - Configuration: config/config.jsonc
2025-07-12 12:04:33 | INFO     | utils.utils - Random seed set to 42
2025-07-12 12:04:33 | INFO     | utils.hardware - Using Apple Silicon MPS device
2025-07-12 12:04:33 | WARNING  | utils.hardware - MPS backend has limited operator support
2025-07-12 12:04:33 | WARNING  | utils.hardware - torch.compile is disabled for MPS devices
2025-07-12 12:04:33 | WARNING  | utils.hardware - Disabling torch.compile for MPS device due to compatibility issues
2025-07-12 12:04:33 | INFO     | __main__ - Using cached, validated pre-processed dataset.
2025-07-12 12:04:33 | INFO     | __main__ - Starting model training...
2025-07-12 12:04:33 | INFO     | __main__ - Model: DEEPONET
2025-07-12 12:04:33 | INFO     | __main__ - Parameters: 1,555,854
2025-07-12 12:04:33 | INFO     | __main__ - Creating datasets...
2025-07-12 12:04:33 | WARNING  | data.dataset - No train_sample_indices found, using all samples
2025-07-12 12:04:33 | INFO     | data.dataset - Using pre-flattened format with 199,505,097 samples
2025-07-12 12:04:33 | INFO     | data.dataset - Dataset initialized: 199,505,097 samples (split: train)
2025-07-12 12:04:33 | WARNING  | data.dataset - No validation_sample_indices found, using all samples
2025-07-12 12:04:33 | INFO     | data.dataset - Using pre-flattened format with 199,505,097 samples
2025-07-12 12:04:33 | INFO     | data.dataset - Dataset initialized: 199,505,097 samples (split: validation)
2025-07-12 12:04:33 | WARNING  | data.dataset - No test_sample_indices found, using all samples
2025-07-12 12:04:33 | INFO     | data.dataset - Using pre-flattened format with 199,505,097 samples
2025-07-12 12:04:33 | INFO     | data.dataset - Dataset initialized: 199,505,097 samples (split: test)
2025-07-12 12:04:34 | INFO     | training.trainer - Optimizer: AdamW with lr=5.00e-04, weight_decay=1.00e-05
2025-07-12 12:04:34 | INFO     | training.trainer - Scheduler: cosine
2025-07-12 12:04:34 | INFO     | training.trainer - Loss function: huber
2025-07-12 12:04:34 | WARNING  | training.trainer - MPS doesn't support bfloat16, using float16 instead
2025-07-12 12:04:34 | INFO     | training.trainer - GradScaler disabled on MPS (not supported)
2025-07-12 12:04:34 | INFO     | training.trainer - AMP enabled with dtype=float16 on mps device
2025-07-12 12:04:34 | INFO     | training.trainer - Starting training...
2025-07-12 12:04:34 | INFO     | training.trainer - Train batches: 4058
2025-07-12 12:04:34 | INFO     | training.trainer - Val batches: 4058
2025-07-12 12:11:44 | INFO     | training.trainer - Epoch 001   0.0%  Loss 7.1401e-02
2025-07-12 12:18:07 | INFO     | training.trainer - Epoch 001   0.0%  Loss 1.1528e-01
2025-07-12 12:24:18 | INFO     | training.trainer - Epoch 001   0.1%  Loss 9.0021e-02
2025-07-12 12:30:20 | INFO     | training.trainer - Epoch 001   0.1%  Loss 1.3276e-01
2025-07-12 12:36:17 | INFO     | training.trainer - Epoch 001   0.1%  Loss 1.1148e-01
2025-07-12 12:42:23 | INFO     | training.trainer - Epoch 001   0.1%  Loss 9.5760e-02
2025-07-12 12:48:20 | INFO     | training.trainer - Epoch 001   0.2%  Loss 9.5854e-02
2025-07-12 12:54:22 | INFO     | training.trainer - Epoch 001   0.2%  Loss 9.6347e-02
2025-07-12 13:00:21 | INFO     | training.trainer - Epoch 001   0.2%  Loss 9.1206e-02
2025-07-12 13:06:17 | INFO     | training.trainer - Epoch 001   0.2%  Loss 8.4155e-02
2025-07-12 13:12:16 | INFO     | training.trainer - Epoch 001   0.3%  Loss 7.9485e-02
2025-07-12 13:18:15 | INFO     | training.trainer - Epoch 001   0.3%  Loss 6.4281e-02
2025-07-12 13:24:18 | INFO     | training.trainer - Epoch 001   0.3%  Loss 8.1871e-02
2025-07-12 13:30:20 | INFO     | training.trainer - Epoch 001   0.3%  Loss 7.6805e-02
2025-07-12 13:36:19 | INFO     | training.trainer - Epoch 001   0.4%  Loss 6.2713e-02
2025-07-12 13:42:30 | INFO     | training.trainer - Epoch 001   0.4%  Loss 5.1730e-02
