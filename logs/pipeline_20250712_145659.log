2025-07-12 14:56:59 | INFO     | utils.utils - Logging system initialized
2025-07-12 14:56:59 | INFO     | __main__ - ================================================================================
2025-07-12 14:56:59 | INFO     | __main__ - Chemical Kinetics Neural Network Pipeline
2025-07-12 14:56:59 | INFO     | __main__ - Configuration: config/config.jsonc
2025-07-12 14:56:59 | INFO     | utils.utils - Random seed set to 42
2025-07-12 14:56:59 | INFO     | utils.hardware - Using Apple Silicon MPS device
2025-07-12 14:56:59 | WARNING  | utils.hardware - MPS backend has limited operator support
2025-07-12 14:56:59 | WARNING  | utils.hardware - torch.compile is disabled for MPS devices
2025-07-12 14:56:59 | WARNING  | utils.hardware - Disabling torch.compile for MPS device due to compatibility issues
2025-07-12 14:56:59 | INFO     | __main__ - NPY shard format enabled for high-performance data loading
2025-07-12 14:56:59 | INFO     | __main__ - Using cached, validated pre-processed dataset.
2025-07-12 14:56:59 | INFO     | __main__ - Starting model training...
2025-07-12 14:56:59 | INFO     | __main__ - Model: DEEPONET
2025-07-12 14:56:59 | INFO     | __main__ - Parameters: 1,555,854
2025-07-12 14:56:59 | INFO     | __main__ - Creating datasets...
2025-07-12 14:56:59 | INFO     | __main__ - Using NPY shard datasets for high-performance loading
2025-07-12 14:56:59 | INFO     | data.dataset - Train split: 139,653,063 samples (indices mmap-loaded)
2025-07-12 14:56:59 | INFO     | data.dataset - NPYDataset ready: 139,653,063 samples, 20 shards, normalized=True
2025-07-12 14:56:59 | INFO     | data.dataset - Validation split: 29,925,720 samples (indices mmap-loaded)
2025-07-12 14:56:59 | INFO     | data.dataset - NPYDataset ready: 29,925,720 samples, 20 shards, normalized=True
2025-07-12 14:56:59 | INFO     | data.dataset - Test split: 29,926,314 samples (indices mmap-loaded)
2025-07-12 14:56:59 | INFO     | data.dataset - NPYDataset ready: 29,926,314 samples, 20 shards, normalized=True
2025-07-12 14:56:59 | INFO     | __main__ - NPY Dataset info: {'format': 'npy_shards', 'n_shards': 20, 'samples_per_shard': 10000000, 'total_samples': 139653063, 'split': 'train', 'normalized': True}
2025-07-12 14:56:59 | INFO     | data.dataset - DataLoader created with 12 workers, prefetch_factor=3
2025-07-12 14:56:59 | INFO     | data.dataset - DataLoader created with 12 workers, prefetch_factor=3
2025-07-12 14:56:59 | INFO     | data.dataset - DataLoader created with 12 workers, prefetch_factor=3
2025-07-12 14:57:00 | INFO     | training.trainer - Optimizer: AdamW with lr=5.00e-04, weight_decay=1.00e-05
2025-07-12 14:57:00 | INFO     | training.trainer - Scheduler: cosine
2025-07-12 14:57:00 | INFO     | training.trainer - Loss function: huber
2025-07-12 14:57:00 | WARNING  | training.trainer - MPS doesn't support bfloat16, using float16 instead
2025-07-12 14:57:00 | INFO     | training.trainer - GradScaler disabled on MPS (not supported)
2025-07-12 14:57:00 | INFO     | training.trainer - AMP enabled with dtype=float16 on mps device
2025-07-12 14:57:00 | INFO     | training.trainer - Starting training...
2025-07-12 14:57:00 | INFO     | training.trainer - Train batches: 2841
2025-07-12 14:57:00 | INFO     | training.trainer - Val batches: 608
2025-07-12 14:58:02 | INFO     | training.trainer - Epoch 001   0.0%  Loss 5.7134e-02
2025-07-12 14:58:39 | INFO     | training.trainer - Epoch 001   0.5%  Loss 9.6624e-02
2025-07-12 15:02:15 | INFO     | training.trainer - Epoch 001   0.9%  Loss 5.6321e-02
2025-07-12 15:05:22 | INFO     | training.trainer - Epoch 001   1.3%  Loss 2.4003e-02
2025-07-12 15:09:30 | INFO     | training.trainer - Epoch 001   1.7%  Loss 1.6346e-02
2025-07-12 15:12:28 | INFO     | training.trainer - Epoch 001   2.1%  Loss 1.3417e-02
2025-07-12 15:14:54 | INFO     | training.trainer - Epoch 001   2.6%  Loss 1.3266e-02
2025-07-12 15:17:31 | INFO     | training.trainer - Epoch 001   3.0%  Loss 1.8099e-02
2025-07-12 15:18:46 | INFO     | training.trainer - Epoch 001   3.4%  Loss 2.4119e-02
2025-07-12 15:20:59 | INFO     | training.trainer - Epoch 001   3.8%  Loss 1.9487e-02
2025-07-12 15:26:49 | INFO     | training.trainer - Epoch 001   4.3%  Loss 1.1538e-02
2025-07-12 15:30:18 | INFO     | training.trainer - Epoch 001   4.7%  Loss 1.1288e-02
2025-07-12 15:31:24 | INFO     | training.trainer - Epoch 001   5.1%  Loss 1.7758e-02
2025-07-12 15:32:17 | INFO     | training.trainer - Epoch 001   5.5%  Loss 1.9388e-02
2025-07-12 15:35:17 | INFO     | training.trainer - Epoch 001   5.9%  Loss 1.3813e-02
2025-07-12 15:38:38 | INFO     | training.trainer - Epoch 001   6.4%  Loss 1.7329e-02
2025-07-12 15:41:19 | INFO     | training.trainer - Epoch 001   6.8%  Loss 1.2802e-02
2025-07-12 15:43:08 | INFO     | training.trainer - Epoch 001   7.2%  Loss 1.5260e-02
2025-07-12 15:44:28 | INFO     | training.trainer - Epoch 001   7.6%  Loss 1.1840e-02
2025-07-12 15:45:17 | INFO     | training.trainer - Epoch 001   8.1%  Loss 1.7466e-02
2025-07-12 15:45:49 | INFO     | training.trainer - Epoch 001   8.5%  Loss 1.0143e-02
2025-07-12 15:46:20 | INFO     | training.trainer - Epoch 001   8.9%  Loss 1.0030e-02
2025-07-12 15:46:51 | INFO     | training.trainer - Epoch 001   9.3%  Loss 2.4478e-02
2025-07-12 15:47:31 | INFO     | training.trainer - Epoch 001   9.8%  Loss 4.9190e-02
2025-07-12 15:48:04 | INFO     | training.trainer - Epoch 001  10.2%  Loss 1.2126e-02
2025-07-12 15:48:34 | INFO     | training.trainer - Epoch 001  10.6%  Loss 9.3218e-03
2025-07-12 15:49:06 | INFO     | training.trainer - Epoch 001  11.0%  Loss 9.1471e-03
2025-07-12 15:49:51 | INFO     | training.trainer - Epoch 001  11.4%  Loss 9.3156e-03
2025-07-12 15:51:49 | INFO     | training.trainer - Epoch 001  11.9%  Loss 8.9160e-03
