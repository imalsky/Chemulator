{
  "normalization": {
    "methods": {
      "T": "min-max",
      "P": "log-min-max"
    },
    "default_method": "log-standard",
    "globals_default_method": "standard",
    "epsilon": 1e-30,
    "min_std": 1e-12
  },
  "data": {
    "species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    "global_variables": [
      "P",
      "T"
    ]
  },
  "dataset": {
    "windows_per_trajectory": 100,
    "preload_to_device": true,
    "shard_cache_size": 2,
    "use_mmap": true
  },
  "model": {
    "type": "mlp",
    "predict_delta": true,
    "activation": "silu",
    "dropout": 0.0,
    "layer_norm": true,
    "layer_norm_eps": 1e-05,
    "mlp": {
      "hidden_dims": [
        2048,
        2048,
        2048,
        2048,
        2048,
        2048
      ],
      "residual": true
    },
    "autoencoder": {
      "latent_dim": 256,
      "encoder_hidden": 512,
      "decoder_hidden": 512,
      "dynamics_hidden": 512,
      "residual": true,
      "dynamics_residual": true
    }
  },
  "training": {
    "_comment_autoregressive": [
      "false: one-jump mode, single step only",
      "true: unroll model for rollout_steps"
    ],
    "autoregressive_training": {
      "enabled": true,
      "_comment_skip_steps": [
        "0: no warmup",
        "N: run N steps without gradient first"
      ],
      "skip_steps": 1,
      "_comment_detach": [
        "true: no BPTT, independent step gradients",
        "false: full BPTT through sequence"
      ],
      "detach_between_steps": true,
      "_comment_backward_per_step": [
        "true: backward each step, saves memory",
        "false: accumulate graph, faster, more VRAM"
      ],
      "backward_per_step": true
    },
    "batch_size": 4096,
    "_comment_checkpoint_mode": [
      "none: fresh training, work_dir must be empty",
      "resume: restore model + optimizer + epoch",
      "weights_only: load weights, reset training state"
    ],
    "checkpoint_mode": "weights_only",
    "_comment_curriculum": [
      "true: ramp rollout_steps over training",
      "mode: linear or cosine interpolation"
    ],
    "curriculum": {
      "enabled": false,
      "mode": "linear",
      "ramp_epochs": 0,
      "start_steps": 2,
      "end_steps": 2
    },
    "loss": {
      "lambda_log10_mae": 1.0,
      "lambda_z_mse": 0.25
    },
    "max_epochs": 1500,
    "num_workers": 0,
    "optimizer": {
      "betas": [
        0.9,
        0.999
      ],
      "eps": 1e-08,
      "exclude_norm_and_bias_from_weight_decay": true,
      "foreach": true,
      "fused": true,
      "lr": 1e-05,
      "name": "adamw",
      "weight_decay": 0.0001
    },
    "persistent_workers": false,
    "pin_memory": false,
    "prefetch_factor": null,
    "_comment_rollout_steps": [
      "1: required for one-jump mode",
      ">1: needs autoregressive_training.enabled=true"
    ],
    "rollout_steps": 2,
    "_comment_scheduler": [
      "plateau: reduce LR when metric stalls",
      "cosine_with_warmup: warmup then cosine decay"
    ],
    "scheduler": {
      "enabled": true,
      "factor": 0.5,
      "min_lr": 1e-08,
      "mode": "min",
      "monitor": "val_loss",
      "patience": 5,
      "threshold": 1e-10,
      "type": "plateau"
    }
  },
  "runtime": {
    "accelerator": "auto",
    "accumulate_grad_batches": 1,
    "_comment_checkpoint": [
      "null: no checkpoint (fresh training)",
      "path: checkpoint file to load"
    ],
    "checkpoint": "models/one_jump/checkpoints/last.ckpt",
    "_comment_checkpointing": [
      "save_top_k: keep N best by monitor metric",
      "save_last: always keep last.ckpt"
    ],
    "checkpointing": {
      "enabled": true,
      "every_n_epochs": 1,
      "monitor": "val_loss",
      "save_top_k": 1,
      "save_last": true
    },
    "deterministic": false,
    "devices": "auto",
    "enable_progress_bar": true,
    "gradient_clip_val": 0.0,
    "_comment_load_weights_strict": [
      "true: state_dict must match exactly",
      "false: allow missing/extra keys"
    ],
    "load_weights_strict": true,
    "log_every_n_steps": 1000000,
    "mode": "train",
    "precision": "bf16-mixed",
    "strategy": "auto",
    "torch_compile": {
      "backend": "inductor",
      "compile_forward_step": true,
      "compile_open_loop_unroll": false,
      "dynamic": false,
      "enabled": false,
      "fullgraph": true,
      "mode": "reduce-overhead"
    }
  },
  "preprocessing": {
    "train_frac": 0.8,
    "val_frac": 0.1,
    "test_frac": 0.1,
    "random_seed": 1234,
    "chunk_size": 1000,
    "epsilon": 1e-30,
    "min_positive": 1e-30
  },
  "paths": {
    "raw_dir": "data/raw",
    "processed_dir": "data/processed",
    "work_dir": "models/auto"
  },
  "system": {
    "device": "auto",
    "log_level": "INFO",
    "seed": 1234
  }
}
