{
  "normalization": {
    "methods": {
      "T": "min-max",
      "P": "log-min-max"
    },
    "default_method": "log-standard",
    "globals_default_method": "standard",
    "epsilon": 1e-30,
    "min_std": 1e-12
  },
  "data": {
    "species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    "global_variables": ["P", "T"]
  },
  "dataset": {
    "windows_per_trajectory": 10,
    "preload_to_device": true,
    "shard_cache_size": 2,
    "use_mmap": false
  },
  "model": {
    "type": "mlp",
    "activation": "silu",
    "dropout": 0.0,
    "predict_delta": true,
    "mlp": {
      "hidden_dims": [2048, 2048, 2048, 2048, 2048, 2048],
      "residual": true
    },
    "autoencoder": {
      "latent_dim": 256,
      "encoder_hidden": [512, 512],
      "dynamics_hidden": [512, 512],
      "decoder_hidden": [512, 512],
      "residual": true,
      "dynamics_residual": true
    }
  },
  "training": {
    "max_epochs": 500,
    "batch_size": 1024,
    "lr": 0.0005,
    "weight_decay": 0.0001,

    "rollout_steps": 50,
    "_comment_rollout_steps": "Number of forward prediction steps used to compute the sequence loss (model is unrolled for this horizon). Larger values emphasize long-horizon stability but make optimization harder and can increase compute.",

    "burn_in_steps": 10,
    "_comment_burn_in_steps": "Number of initial steps where ground-truth states are fed to the model before free rollout. Establishes context/state; then the model predicts forward for rollout_steps.",

    "val_burn_in_steps": 10,
    "_comment_val_burn_in_steps": "Burn-in horizon used in validation. Keep equal to burn_in_steps for apples-to-apples metrics unless you intentionally want stricter validation.",

    "burn_in_noise_std": 0.0,
    "_comment_burn_in_noise_std": "Std-dev of Gaussian noise added to burn-in inputs (if implemented). Nonzero encourages robustness to state perturbations; can reduce exposure bias.",

    "burn_in_loss_weight": 0.0,
    "_comment_burn_in_loss_weight": "Relative weight on burn-in-step losses (if your loss separates burn-in vs rollout). 0.0 typically means you only optimize prediction accuracy on the rollout portion.",

    "grad_clip": 1.0,
    "_comment_grad_clip": "Gradient norm clipping threshold to stabilize training during long unrolls and avoid exploding gradients.",

    "precision": "bf16-mixed",
    "devices": "auto",
    "accelerator": "auto",
    "num_workers": 4,
    "pin_memory": true,
    "persistent_workers": true,
    "prefetch_factor": 2,
    "accumulate_grad_batches": 1,
    "num_sanity_val_steps": 0,
    "enable_progress_bar": true,
    "enable_model_summary": true,
    "resume": true,

    "teacher_forcing": {
      "start": 1.0,
      "end": 0.0,
      "decay_epochs": 50,
      "mode": "linear",
      "_comment__teacher_forcing": "Controls the probability of using ground-truth previous state vs the model’s previous prediction during rollout. start=1.0 means fully teacher-forced early; end=0.0 means fully free-running later. Decaying teacher forcing reduces exposure bias and trains the model to self-propagate."
    },

    "curriculum": {
      "enabled": true,
      "start_steps": 15,
      "ramp_epochs": 200,
      "_comment__curriculum": "Curriculum learning on rollout horizon. Typically begins with shorter rollouts (start_steps) to make optimization easier, then increases toward training.rollout_steps over ramp_epochs. Goal: improve long-horizon performance without destabilizing early training."
    },

    "long_rollout": {
      "enabled": true,
      "long_rollout_steps": 400,
      "long_ft_epochs": 10,
      "apply_to_validation": true,
      "apply_to_test": true,
      "_comment__long_rollout": "Optional second phase or periodic evaluation using much longer rollouts. long_rollout_steps sets the long horizon. long_ft_epochs often denotes fine-tuning epochs emphasizing the long horizon. apply_to_validation/test means metrics (and possibly loss) are computed on long rollouts for those splits."
    },

    "loss": {
      "lambda_phys": 0.0,
      "lambda_z": 1.0,
      "_comment__loss": "Weights for composite loss terms. Commonly lambda_phys weights a physical-space error term (e.g., MSE on physical units), while lambda_z weights a normalized/latent-space or z-scored term. Exact meaning depends on your training code’s loss implementation."
    },

    "scheduler": {
      "name": "cosine_warmup",
      "warmup_epochs": 10,
      "min_lr_ratio": 0.01,
      "_comment__scheduler": "Learning-rate schedule. 'cosine_warmup' typically warms up LR over warmup_epochs, then decays with a cosine curve toward lr * min_lr_ratio."
    },

    "checkpointing": {
      "enabled": true,
      "save_top_k": 1,
      "monitor": "val_loss",
      "mode": "min",
      "save_last": true,
      "every_n_epochs": 1,
      "_comment__checkpointing": "Model checkpoint policy. Keeps the best checkpoint by monitored metric (val_loss, minimized), also saves the last checkpoint, and evaluates/saves every_n_epochs."
    },

    "early_stopping": {
      "enabled": false,
      "monitor": "val_loss",
      "patience": 20,
      "mode": "min",
      "min_delta": 0.0,
      "verbose": true,
      "_comment__early_stopping": "Stops training if monitored metric fails to improve by at least min_delta for 'patience' epochs. Disabled here."
    }
  },
  "preprocessing": {
    "drop_below": 1e-30,
    "output_trajectories_per_file": 100000,
    "dt_sampling": "loguniform",
    "dt_mode": "per_chunk",
    "dt_min": 10,
    "dt_max": 1000,
    "n_steps": 500,
    "train_split": 0.8,
    "val_split": 0.1,
    "test_split": 0.1,
    "shard_size": 16384,
    "time_key": "time",
    "species_group": "species",
    "globals_group": "globals"
  },
  "paths": {
    "raw_data_dir": "data/raw",
    "processed_data_dir": "data/processed",
    "model_dir": "models",
    "work_dir": "models/v1"
  },
  "runtime": {
    "mode": "train",
    "checkpoint": null
  },
  "system": {
    "seed": 1234,
    "deterministic": false
  }
}
