{
  "run_name": "deeponet-arb-pairs",

  "paths": {
    "raw_data_files": [
      "data/raw/run9001-result.h5",
      "data/raw/run9002-result.h5",
      "data/raw/run9003-result.h5",
      "data/raw/run9004-result.h5",
      "data/raw/run9005-result.h5",
      "data/raw/run9006-result.h5",
      "data/raw/run9007-result.h5",
      "data/raw/run9008-result.h5",
      "data/raw/run9009-result.h5",
      "data/raw/run9010-result.h5",
      "data/raw/run11001-result.h5",
      "data/raw/run11002-result.h5",
      "data/raw/run11003-result.h5",
      "data/raw/run11004-result.h5",
      "data/raw/run11005-result.h5",
      "data/raw/run11006-result.h5",
      "data/raw/run11007-result.h5",
      "data/raw/run11008-result.h5",
      "data/raw/run11009-result.h5",
      "data/raw/run11010-result.h5",
      "data/raw/run11011-result.h5",
      "data/raw/run11012-result.h5",
      "data/raw/run11013-result.h5",
      "data/raw/run11014-result.h5",
      "data/raw/run11015-result.h5",
      "data/raw/run11016-result.h5",
      "data/raw/run11017-result.h5",
      "data/raw/run11018-result.h5",
      "data/raw/run11019-result.h5",
      "data/raw/run11020-result.h5",
      "data/raw/run11021-result.h5",
      "data/raw/run11022-result.h5",
      "data/raw/run11023-result.h5",
      "data/raw/run11024-result.h5",
      "data/raw/run11025-result.h5"
    ],
    "processed_data_dir": "data/processed-10-log-standard-big",
    "model_save_dir": "models",
    "log_dir": "logs"
  },

  "data": {
    "species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    "target_species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    "global_variables": ["P", "T"],
    "expected_globals": ["P", "T"],
    "time_variable": "t_time"
  },

  "preprocessing": {
    "min_value_threshold": 1e-25,
    "num_workers": 8,
    "hdf5_chunk_size": 16384,
    "trajectories_per_shard": 10000,
    "npz_compressed": false
  },

  "normalization": {
    "default_method": "log-standard",
    "methods": {
      "t_time": "log-min-max",
      "P": "log-min-max",
      "T": "standard"
    },
    "epsilon": 1e-30,
    "min_std": 1e-10,
    "clamp_value": 50.0
  },

  "model": {
    "species_dim": 12,
    "globals_dim": 2,

    "p": 64,

    // NEW: the model now expects fixed-width residual settings, not "branch_layers"
    "branch_width": 1024,
    "branch_depth": 4,

    // keep your trunk width/depth the same
    "trunk_layers": [1024, 1024, 1024, 1024],

    "predict_delta": true,
    "trunk_dedup": false,

    // optional: keep this if you want α learnable (supported)
    "branch_residual_learnable": false

    // NOTE: "branch_residual" and "branch_residual_alpha_init" are ignored by this model
  },

  "training": {
    // RENAMED/ADDED so main.py can read them directly
    "pairs_per_traj": 300,     // was pairwise.pairs_per_trajectory
    "min_steps": 1,           // was pairwise.min_steps
    "max_steps": 99,          // was pairwise.max_steps

    "epochs": 500,
    "batch_size": 16384,

    "lr": 1e-4,             // was learning_rate
    "min_lr": 1e-8,
    "weight_decay": 0.00005,

    // The rest are currently unused by the new trainer but harmless to keep:
    "betas": [0.9, 0.999],
    "gradient_clip": 1.0,
    "use_amp": true,          // ignored (trainer uses system.use_bfloat16 instead)
    "amp_dtype": "bfloat16",  // ignored (see system.use_bfloat16)
    "val_fraction": 0.15,
    "test_fraction": 0.15,
    "use_fraction": 1.0,

    // DataLoader workers (you had 0; GPU-resident dataset is fine with 0–4)
    "num_workers": 0,

    // Optional trainer limits; currently ignored by my trainer
    "max_steps_per_epoch": 1000,
    "max_val_batches": 100
  },

  "system": {
    "seed": 42,
    "cudnn_benchmark": true,
    "tf32": true,
    "cuda_memory_fraction": 0.90,
    "deterministic": false,

    // NEW: this is what flips AMP bf16 in the trainer and sets dataset dtype
    "use_bfloat16": true,

    // "dtype" is ignored by the new code path
    "dtype": "float32"
  }
}
