{
  // Data paths configuration
  "paths": {
    // IMPORTANT: Leave raw_data_files out to auto-scan ./data/raw for *.h5 / *.hdf5.
    // The current preprocessor ignores any "raw_data_dir" field; it is hardcoded to "data/raw".
    // "raw_data_files": [],

    // Repo-relative outputs (resolve under the checkout you qsub from)
    "processed_data_dir": "data/processed",
    "work_dir": "models/flowmap-deeponet"
  },

  // Variable definitions
  "data": {
    // Let preprocessing/hydration auto-detect and inject species_variables.
    // "species_variables": [],

    // Global conditioning parameters (constant per trajectory)
    "global_variables": ["P", "T"],

    // Name of time array in HDF5 groups
    "time_variable": "t_time"
  },

  // Preprocessing configuration
  "preprocessing": {
    // Drop any trajectory with values below threshold
    "min_value_threshold": 1e-30,

    // Trajectories per NPZ shard
    "trajectories_per_shard": 65536,

    // NPZ compression off for faster I/O
    "npz_compressed": false,

    // HDF5 scanning workers if not using MPI (ignored with MPI)
    "num_workers": 12,

    // 0 = auto-detect from dataset
    "hdf5_chunk_size": 0
  },

  // Normalization configuration
  "normalization": {
    "default_method": "log-standard",
    "methods": {
      "dt":     "log-min-max",
      "t_time": "log-min-max",
      "P":      "log-min-max",
      "T":      "standard"
    },
    "epsilon": 1e-30,
    "min_std": 1e-10,
    "clamp_value": 1e6
  },

  // Dataset configuration
  "dataset": {
    // Keep anchor diversity with large K
    "uniform_offset_sampling": false,

    "multi_time_per_anchor": true,
    "times_per_anchor": 64,

    // Only set true if time grid truly shared
    "share_times_across_batch": false,

    // Precompute Î”t table if shared grid is used
    "precompute_dt_table": true,

    // Require dt stats in normalization.json
    "require_dt_stats": true,

    // GPU-resident datasets for throughput (GH200 has the RAM for it)
    "preload_train_to_gpu": true,
    "preload_val_to_gpu": true,

    // With GPU-resident data, keep workers at 0
    "num_workers": 0,
    "num_workers_val": 0,

    "pin_memory": false,
    "pin_memory_val": false,

    "prefetch_factor": 2,
    "prefetch_factor_val": 2,

    "persistent_workers": false,
    "persistent_workers_val": false,

    // Runtime dtype for staged tensors
    "storage_dtype": "bf16"
  },

  // Model architecture configuration
  "model": {
    "p": 256,
    "branch_width": 1024,
    "branch_depth": 4,
    "trunk_layers": [1024, 1024, 1024, 1024],
    "predict_delta": false,
    "trunk_dedup": false,
    "dropout": 0.0,
    "activation": "leakyrelu"
  },

  // Training configuration
  "training": {
    "val_fraction": 0.15,
    "test_fraction": 0.10,
    "use_fraction": 1.0,

    "min_steps": 1,
    "max_steps": 99,

    "pairs_per_traj": 32,

    "batch_size": 2048,
    "epochs": 200,
    "lr": 5e-4,
    "weight_decay": 1e-5,

    "warmup_epochs": 10,
    "min_lr": 1e-7,
    "gradient_clip": 1.0,

    "torch_compile": true,

    "max_train_steps_per_epoch": 0,
    "max_val_batches": 0
  },

  // Mixed precision
  "mixed_precision": {
    "mode": "bf16"
  },

  // System configuration
  "system": {
    "seed": 42,
    "deterministic": false,
    "tf32": true,
    "cudnn_benchmark": true,

    // Target PyTorch intra-op threads (your hardware.py already sets this)
    "omp_threads": 12,

    // NPZ on disk
    "io_dtype": "float32"
  }
}
