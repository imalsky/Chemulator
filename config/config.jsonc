{
  // ===============================
  // PATHS
  // ===============================
  "paths": {
    "processed_data_dir": "data/processed",
    "work_dir": "models/big",
    "raw_data_files": []
  },
  // ===============================
  // Compile
  // ===============================
  "torch_compile": {
    "enable": true,
    "backend": "inductor",
    "mode": "default",
    "dynamic": false,
    "fullgraph": false
  },

  // ===============================
  // DATA SCHEMA
  // (auto-hydrated from processed dir if present)
  // ===============================
  "data": {
    "species_variables": [],          // leave [] to auto-resolve from normalization.json
    "global_variables": ["P", "T"],
    "time_variable": "t_time",
    "target_species": []              // [] => predict all species
  },

  // ===============================
  // DATASET / DATALOADER
  // ===============================
  "dataset": {
    // Multi-Δt targets per anchor
    "multi_time_per_anchor": true,
    "times_per_anchor": 64,
    "share_times_across_batch": true,

    // Time sampling / grid fast-paths
    "log_dt_sampling": true,
    "precompute_dt_table": true,
    "uniform_grid_rtol": 1e-6,
    "uniform_grid_atol": 1e-12,
    "snap_shared_grid": true,
    "assume_shared_grid": true,
    "skip_scan": true,
    "skip_validate_grids": true,
    "require_dt_stats": true,

    // Dataset storage/runtime dtype (controls on-GPU buffers)
    "storage_dtype": "bfloat16",

    // DataLoader plumbing (supports "auto" in code)
    "preload_train_to_gpu": true,
    "preload_val_to_gpu": true,
    "num_workers": "auto",
    "num_workers_val": "auto",
    "pin_memory": true,
    "pin_memory_val": true,
    "prefetch_factor": 2,
    "prefetch_factor_val": 2,
    "persistent_workers": false,
    "persistent_workers_val": false,

    "log_every_files": 1000
  },

  // ===============================
  // MODEL
  // ===============================
  "model": {
    // Architecture
    "latent_dim": 128,
    "encoder_hidden": [256, 512, 1024, 1536],
    "dynamics_hidden":[1536, 2048, 1536],
    "decoder_hidden": [1536, 1024, 512, 256],

    "vae_mode": true,

    // Act/Dropout
    "activation": "silu",
    "dropout": 0.025,

    // Behavior (matches current model.py)
    "dynamics_residual": false,
    "decoder_condition_on_g": false,      // FiLM gating on P,T
    "predict_logit_delta": false,        // residual in logit (nat-log prob) space before softmax
    "allow_partial_simplex": false       // set true only if predicting a subset that must sum to 1
  },

  // ===============================
  // TRAINING
  // ===============================
  "training": {
    // Batching / sampling (used by main.build_datasets_and_loaders and preprocessor)
    "batch_size": 2048,
    "val_batch_size": 2048,
    "pairs_per_traj": 16,
    "pairs_per_traj_val": 16,

    "beta_kl": 1e-3,

    // Splits for preprocessing (required by preprocessor)
    "val_fraction": 0.1,
    "test_fraction": 0.1,
    "use_fraction": 1.0,

    // Allowed step-span (required by preprocessor; default-safe if you don’t care)
    "min_steps": 1,
    "max_steps": null,            // optional; omit for full range

    // Optimizer / schedule
    "lr": 1e-4,
    "min_lr": 1e-8,
    "weight_decay": 1e-5,
    "warmup_epochs": 10,
    "epochs": 200,
    "gradient_clip": 10.0,

    // Loss
    "loss_mode": "adaptive_stiff", //"adaptive_stiff"
    "adaptive_stiff_loss": {
      "lambda_mse": 0.25,
      "lambda_phys": 1.0,
      "time_edge_gain": 1.0
    },

    "ema": { "enable": true, "decay": 0.9999},            // EMA callback (minimal)
    "swa": { "enable": true, "epoch_start": 0.8 },         // SWA callback (optional)
    "model_soup": { "enable": true, "top_k": 3 },         // Average best-k ckpts post-fit

    // Aux losses (names/structure that trainer reads)
    "auxiliary_losses": {
      "rollout_enabled": true,
      "rollout_weight": 0.25,
      "rollout_horizon": 8,
      "rollout_warmup_epochs": 10,
      "rollout_teacher_forcing": {
        "mode": "linear",
        "start_p": 1.0,
        "end_p": 0.0,
        "end_epoch": 10
      },
      "semigroup": {
        "enabled": true,
        "weight": 0.1,
        "warmup_epochs": 5
      }
    },

    // Lightning’s final value will be set automatically; keep a default here
    "accumulate_grad_batches": 1
  },

  // ===============================
  // LIGHTNING RUNTIME
  // ===============================
  "lightning": {
    "accelerator": "auto",
    "devices": 1,
    "precision": "bf16-mixed",
    "strategy": "auto",
    "num_sanity_val_steps": 2,
    "early_stop_patience": 30
    // "resume_from": "path/to/last.ckpt"   // optional; used by Trainer wrapper
  },

  // ===============================
  // PREPROCESSING
  // ===============================
  "preprocessing": {
    "trajectories_per_shard": 32768,
    "npz_compressed": false,
    "min_value_threshold": 1e-30, // Intentionally drop very small values to sanitize the data
    "hdf5_chunk_size": 0,
    "num_workers": 0,
    "skip_first_timestep": false
  },

  // ===============================
  // NORMALIZATION
  // ===============================
  "normalization": {
    "default_method": "log-standard",
    "methods": {
      "t_time": "log-min-max",
      "dt": "log-min-max",
      "T": "standard",
      "P": "log-min-max"
    },
    "epsilon": 1e-30,
    "min_std": 1e-10
  },


  // ===============================
  // SYSTEM / HARDWARE
  // ===============================
  "system": {
    "seed": 42,
    "tf32": true,
    "cudnn_benchmark": true,
    "deterministic": false,
    "linalg_library": null,
    "omp_threads": null,
    "io_dtype": "float32",
    "overwrite_work_dir": true
  }
}