{
  "paths": {
    // Input HDF5s for preprocessing
    "raw_data_files": [
      "data/raw/run9001-result.h5"
    ],

    // Output directory for NPZ shards + normalization.json
    "processed_data_dir": "data/processed-flowmap",

    // Where checkpoints/logs go (trainer writes here)
    "work_dir": "models/deeponet-flowmap"
  },

  "data": {
    // Full species list (targets)
    "species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    // If you use target!=species later, trainer/model still infer dims from this list
    "target_species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    // Global/conditioning features
    "global_variables": ["P", "T"],

    // Name of the time column in HDF5 files
    "time_variable": "t_time"
  },

  "preprocessing": {
    // Drop an entire trajectory if ANY species value is < threshold or non-finite
    "min_value_threshold": 1e-25,

    // Shard sizing and NPZ compression
    "trajectories_per_shard": 1024,
    "npz_compressed": false,

    // Parallel file processing (process-based; 0 means serial)
    "num_workers": 5,

    // Optional chunk length along time axis for HDF5 reads (0 → auto)
    "hdf5_chunk_size": 0
  },

  "normalization": {
    // Default per-key normalization unless overridden below
    "default_method": "log-standard",

    // Per-key overrides. IMPORTANT: keep "dt" as log-min-max for flow-map trunk.
    "methods": {
      "dt": "log-min-max",
      "t_time": "log-min-max",
      "P": "log-min-max",
      "T": "standard"
      // add per-species overrides here if needed, else default_method applies
    },

    // Numerical safety
    "epsilon": 1e-25,
    "min_std": 1e-8,
    "clamp_value": 1e6
  },

  "dataset": {
    // Multi-time-per-anchor controls
    "multi_time_per_anchor": true,
    "times_per_anchor": 32,              // K
    "share_times_across_batch": false,   // set true only if Δt grid truly aligns

    // Δt normalization table and manifest requirements
    "precompute_dt_table": true,
    "require_dt_stats": true,

    // Stage tensors to device memory for throughput (A100 has headroom)
    "preload_train_to_gpu": true,
    "preload_val_to_gpu": true,

    // DataLoader worker settings (ignored when dataset lives on GPU)
    "num_workers": 0,
    "num_workers_val": 0,
    "pin_memory": false,
    "pin_memory_val": false,
    "prefetch_factor": 2,
    "prefetch_factor_val": 2,
    "persistent_workers": false,
    "persistent_workers_val": false,

    // On-device storage dtype for staged NPZ data (keep time-sensitive inputs at fp32)
    "storage_dtype": "float32"           // "float32" | "bf16" | "fp16"
  },

  "model": {
    // Basis size and MLP widths/depths
    "p": 256,
    "branch_width": 1024,
    "branch_depth": 4,
    "trunk_layers": [512, 512, 512],

    // Predict residual y_i + f(…)
    "predict_delta": true,

    // Pre-LN residual α learnable inside branch blocks (usually false)
    "branch_residual_learnable": false,

    // Evaluate trunk once per unique Δt in batch if duplicates exist
    "trunk_dedup": false
  },

  "training": {
    // Preprocessor split fractions
    "val_fraction": 0.15,
    "test_fraction": 0.10,

    // Pair sampling: j - i ∈ [min_steps, max_steps], j > i, with T typically = 100
    "min_steps": 1,
    "max_steps": 99,

    // How many anchors per trajectory per epoch
    "pairs_per_traj": 64,

    // Batch sizes
    "batch_size": 512,
    "val_batch_size": 512,

    // Optimizer & schedule (AdamW; betas are defaults inside Trainer)
    "epochs": 100,
    "lr": 5e-4,
    "weight_decay": 1e-5,
    "warmup_epochs": 0,
    "min_lr": 1e-7,

    // Execution controls
    "gradient_clip": 0.0,
    "torch_compile": true,
    "max_train_steps_per_epoch": 0,      // 0 = no cap
    "max_val_batches": 0                 // 0 = full validation
  },

  "mixed_precision": {
    // Trainer compute dtype
    "mode": "bf16"                       // "bf16" | "fp16" | "none"
  },

  "system": {
    // Seeding and deterministic behavior
    "seed": 42,
    "deterministic": false,

    // CUDA/cuDNN knobs
    "tf32": true,
    "cudnn_benchmark": true,

    // Optional cap on CUDA memory fraction for this process (0.0–1.0); omit or set 0 to skip
    "cuda_memory_fraction": 0.0,

    // OpenMP thread pool for BLAS/linear algebra on CPU
    "omp_threads": 12,

    // NPZ on-disk dtype emitted by the preprocessor
    "io_dtype": "float32"                // "float32" | "float64"
  }
}
