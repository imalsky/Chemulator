{
  // ---------- Paths ----------
  "paths": {
    // Where preprocessed shards + manifests live (normalization.json, shard_index.json, …)
    "processed_data_dir": "data/processed",

    // Where checkpoints, logs, CSV, etc. are written
    "work_dir": "models/trained_model_optimized_v1",

    // Optional: explicit list of HDF5 files. Leave [] to auto-scan data/raw/*.h5(hdf5)
    "raw_data_files": []
  },

  // ---------- Data schema (authoritative values are re-hydrated from normalization.json) ----------
  "data": {
    // Species variables - will be auto-detected from HDF5 if null
    //"species_variables": null,

    // Globals that condition the branch (constant per trajectory)
    "global_variables": ["P", "T"],

    // Target species - null means predict all
    //"target_species": null,

    // Name of the absolute time dataset in the raw HDF5
    "time_variable": "t_time"
  },

  // ---------- Normalization applied everywhere (preprocessor + runtime) ----------
  "normalization": {
    "default_method": "log-standard",
    "methods": {
      "dt":     "log-min-max",
      "t_time": "log-min-max",
      "P":      "log-min-max",
      "T":      "standard"
    },
    "epsilon": 1e-30,           // More stable for float32
    "min_std": 1e-10,
    "clamp_value": 1e10          // Reduced from 10.0 to avoid saturation
  },

  // ---------- Preprocessing configuration ----------
  "preprocessing": {
    // Drop any trajectory with values below threshold
    "min_value_threshold": 0,

    // Trajectories per NPZ shard
    "trajectories_per_shard": 65536,

    // NPZ compression off for faster I/O
    "npz_compressed": false,

    // HDF5 scanning workers if not using MPI (ignored with MPI)
    "num_workers": 50,

    // 0 = auto-detect from dataset
    "hdf5_chunk_size": 0
  },

  // ---------- Dataset (pair sampling & loader behavior) ----------
  "dataset": {
    // Require dt-spec (log-min-max over Δt) in normalization.json
    "require_dt_stats": true,

    // If all trajectories share the exact same time grid, precompute a [T,T] Δt table
    "precompute_dt_table": true,

    // Sample multiple target times per anchor (better coverage)
    "multi_time_per_anchor": true,
    "times_per_anchor": 64,           // Increased from 4 for better stiff coverage

    // If grids are shared, reuse the same K offsets across the batch
    "share_times_across_batch": false,
    "uniform_offset_sampling": false,

    "preload_train_to_gpu": true,
    "preload_val_to_gpu": true,

    // Loader knobs (ignored when GPU-resident; workers forced to 0)
    "num_workers": 0,
    "pin_memory": false,
    "prefetch_factor": 2,
    "persistent_workers": false
  },

  // ---------- Training ----------
  "training": {
    // Loss configuration
    "loss_mode": "adaptive_stiff",    // Best for stiff systems
    "adaptive_stiff_loss": {
      "lambda_phys": 1.0,             // Weight for physical-space error
      "lambda_z": 0.1,                // Stabilizer in normalized space
      "epsilon_phys": 1e-25,          // Numerical floor
      "use_fractional": false,        // false = MAE(log10)
      "time_edge_gain": 2.0,          // Upweight trajectory edges
      "conservation_penalty": 0    // Added conservation penalty
    },
    "loss_epsilon": 1e-20,
    "loss_rel_cap": 10.0,            // Cap fractional error if using frac_l1_phys

    "epochs": 200,

    "val_fraction": 0.1,              // 10% for validation
    "test_fraction": 0.1,             // 10% for test
    "use_fraction": 1.0,              // Use all available data

    // Batch sizes
    "batch_size": 1024,
    "val_batch_size": 1024,

    // Sampling parameters
    "pairs_per_traj": 8,            
    "pairs_per_traj_val": 8,        // Keep validation moderate

    // Δt index bounds in steps
    "min_steps": 1,
    "max_steps": 99,                 // Assuming T=100; adjust to T-1

    // Optimizer settings
    "lr": 1e-4,                      // Reduced from 5e-4
    "min_lr": 1e-7,
    "warmup_epochs": 10,
    "weight_decay": 3e-5,            // Slightly increased
    "gradient_clip": 1.0,

    // Performance options
    "torch_compile": false,
    "max_train_steps_per_epoch": 0,
    "max_val_batches": 0,

    // Restart policy
    "resume": null,
    "auto_resume": true
  },

  // ---------- Model (DeepONet) ----------
  "model": {
    // Residual configuration
    "predict_delta": false,           // Keep false when using predict_delta_log_phys
    "predict_delta_log_phys": true,   // Correct residual in log-physical space

    // Architecture - balanced for performance and VRAM
    "p": 1024,                        // Increased basis dimension
    "branch_width": 1024,             // Reduced width
    "branch_depth": 4,               // Increased depth
    "trunk_layers": [512, 512, 512, 512], // Added layer, reduced width

    // Activation
    "activation": "leakyrelu",            // Generally more stable than 

    // Mild dropout
    "dropout": 0.0,                  // Global fallback
    "branch_dropout": 0.00,          // Reduced from 0.1
    "trunk_dropout": 0.00,           // Reduced from 0.1

    // Keep trunk per-time
    "trunk_dedup": false
  },

  // ---------- Mixed precision ----------
  "mixed_precision": {
    // BF16 is stable on modern GPUs
    "mode": "bf16"
  },

  // ---------- System / hardware ----------
  "system": {
    "seed": 42,

    // Storage and runtime precision
    "dtype": "float32",
    "io_dtype": "float32",

    // CPU threading
    "omp_threads": 8,

    // GPU optimizations
    "tf32": true,                   // Fast math for Ampere+
    "cudnn_benchmark": true,         // Auto-tune convolutions

    // Reproducibility
    "deterministic": false           // Keep false for performance
  }
}
