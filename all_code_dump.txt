===== /Users/imalsky/Desktop/Chemulator/test/plot_benchmark.py =====
#!/usr/bin/env python3
"""
Benchmark inference performance for single samples and various batch sizes.
Highly optimized for measuring real-world inference latency.
"""

import json
import sys
import time
from pathlib import Path
import logging

sys.path.append(str(Path(__file__).parent.parent))

import numpy as np
import torch
import matplotlib.pyplot as plt
from contextlib import contextmanager

# Ensure we can use high-precision timing
if sys.platform == "win32":
    from time import perf_counter as timer
else:
    from time import perf_counter as timer

def get_latest_model_dir():
    """Get the most recent model directory."""
    models_dir = Path(__file__).parent.parent / 'data' / 'models'
    model_dirs = [d for d in models_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    if not model_dirs:
        raise ValueError("No model directories found!")
    
    return max(model_dirs, key=lambda d: d.stat().st_mtime)

@contextmanager
def cuda_timer():
    """Context manager for accurate GPU timing."""
    if torch.cuda.is_available():
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        torch.cuda.synchronize()
        start.record()
        yield
        end.record()
        torch.cuda.synchronize()
        elapsed = start.elapsed_time(end) / 1000.0  # Convert to seconds
        return elapsed
    else:
        start = timer()
        yield
        return timer() - start

def load_model(model_dir, device):
    """Load the exported model for inference."""
    logging.info(f"Loading model from: {model_dir}")
    
    # Load config
    config_path = model_dir / 'config.json'
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Try exported model first (fastest)
    exported_path = model_dir / 'exported_model.pt'
    
    if exported_path.exists():
        logging.info("Loading exported model for inference...")
        try:
            # Try torch.export first
            import torch.export
            exported_program = torch.export.load(str(exported_path))
            model = exported_program.module()
            logging.info("Loaded via torch.export")
        except:
            # Fall back to JIT
            model = torch.jit.load(str(exported_path), map_location=device)
            logging.info("Loaded via torch.jit")
    else:
        # Fall back to creating model from config
        logging.warning("No exported model found, creating from config (slower)")
        from src.models.model import create_model
        model = create_model(config, device)
        
        # Load checkpoint
        checkpoint_path = model_dir / 'best_model.pt'
        if checkpoint_path.exists():
            checkpoint = torch.load(checkpoint_path, map_location=device)
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
            model.load_state_dict(state_dict)
    
    model.eval()
    model = model.to(device)
    
    # Get dimensions
    n_species = len(config['data']['species_variables'])
    n_globals = len(config['data']['global_variables'])
    input_dim = n_species + n_globals + 1
    
    return model, input_dim, config

def benchmark_inference(model, input_dim, device, warmup_runs=100, test_runs=1000):
    """Benchmark single sample and batched inference."""
    results = {}
    
    # Test batch sizes
    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
    
    logging.info("Starting inference benchmark...")
    
    # Warmup
    logging.info(f"Warming up with {warmup_runs} runs...")
    dummy_input = torch.randn(128, input_dim, device=device)
    with torch.no_grad():
        for _ in range(warmup_runs):
            _ = model(dummy_input)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # Benchmark each batch size
    for batch_size in batch_sizes:
        logging.info(f"Benchmarking batch size {batch_size}...")
        
        # Create input
        test_input = torch.randn(batch_size, input_dim, device=device)
        
        # Time CPU execution
        cpu_times = []
        gpu_times = []
        
        with torch.no_grad():
            # CPU timing
            for _ in range(test_runs):
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                
                start = timer()
                output = model(test_input)
                
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                
                cpu_times.append(timer() - start)
            
            # GPU timing (if available)
            if device.type == 'cuda':
                for _ in range(test_runs):
                    start_event = torch.cuda.Event(enable_timing=True)
                    end_event = torch.cuda.Event(enable_timing=True)
                    
                    start_event.record()
                    output = model(test_input)
                    end_event.record()
                    
                    torch.cuda.synchronize()
                    gpu_times.append(start_event.elapsed_time(end_event) / 1000.0)
        
        # Calculate statistics
        cpu_times = np.array(cpu_times[10:])  # Skip first few for stability
        cpu_mean = np.mean(cpu_times)
        cpu_std = np.std(cpu_times)
        cpu_median = np.median(cpu_times)
        cpu_p99 = np.percentile(cpu_times, 99)
        
        results[batch_size] = {
            'cpu_mean': cpu_mean,
            'cpu_std': cpu_std,
            'cpu_median': cpu_median,
            'cpu_p99': cpu_p99,
            'throughput': batch_size / cpu_mean,
            'latency_per_sample': cpu_mean / batch_size
        }
        
        if gpu_times:
            gpu_times = np.array(gpu_times[10:])
            results[batch_size]['gpu_mean'] = np.mean(gpu_times)
            results[batch_size]['gpu_median'] = np.median(gpu_times)
            results[batch_size]['gpu_throughput'] = batch_size / np.mean(gpu_times)
    
    return results

def plot_benchmark_results(results, model_dir, device):
    """Create benchmark visualization plots."""
    batch_sizes = sorted(results.keys())
    
    # Extract metrics
    latencies = [results[bs]['latency_per_sample'] * 1000 for bs in batch_sizes]  # Convert to ms
    throughputs = [results[bs]['throughput'] for bs in batch_sizes]
    total_times = [results[bs]['cpu_mean'] * 1000 for bs in batch_sizes]  # Convert to ms
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Latency per sample vs batch size
    ax1 = axes[0, 0]
    ax1.plot(batch_sizes, latencies, 'b-o', linewidth=2, markersize=8)
    ax1.set_xlabel('Batch Size')
    ax1.set_ylabel('Latency per Sample (ms)')
    ax1.set_title('Inference Latency per Sample vs Batch Size')
    ax1.set_xscale('log', base=2)
    ax1.grid(True, alpha=0.3)
    
    # Add annotations for key points
    for i, (bs, lat) in enumerate(zip(batch_sizes[:5], latencies[:5])):
        ax1.annotate(f'{lat:.2f} ms', (bs, lat), textcoords="offset points", 
                     xytext=(0,10), ha='center', fontsize=9)
    
    # 2. Throughput vs batch size
    ax2 = axes[0, 1]
    ax2.plot(batch_sizes, throughputs, 'g-s', linewidth=2, markersize=8)
    ax2.set_xlabel('Batch Size')
    ax2.set_ylabel('Throughput (samples/second)')
    ax2.set_title('Inference Throughput vs Batch Size')
    ax2.set_xscale('log', base=2)
    ax2.grid(True, alpha=0.3)
    
    # 3. Total inference time vs batch size
    ax3 = axes[1, 0]
    ax3.plot(batch_sizes, total_times, 'r-^', linewidth=2, markersize=8)
    ax3.set_xlabel('Batch Size')
    ax3.set_ylabel('Total Inference Time (ms)')
    ax3.set_title('Total Batch Inference Time')
    ax3.set_xscale('log', base=2)
    ax3.set_yscale('log')
    ax3.grid(True, alpha=0.3)
    
    # 4. Performance summary table
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    # Create summary statistics
    single_sample = results[1]
    optimal_batch = max(results.items(), key=lambda x: x[1]['throughput'])
    
    summary_text = f"""Performance Summary
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Device: {device}
    
    Single Sample (Batch=1):
      Mean Latency: {single_sample['cpu_mean']*1000:.2f} ms
      P99 Latency: {single_sample['cpu_p99']*1000:.2f} ms
      Throughput: {single_sample['throughput']:.0f} samples/sec
    
    Optimal Batch Size: {optimal_batch[0]}
      Throughput: {optimal_batch[1]['throughput']:.0f} samples/sec
      Latency/sample: {optimal_batch[1]['latency_per_sample']*1000:.2f} ms
      Speedup: {optimal_batch[1]['throughput']/single_sample['throughput']:.1f}x
    
    Latency Range:
      Best: {min(latencies):.2f} ms/sample
      Worst: {max(latencies):.2f} ms/sample
    """
    
    if 'gpu_mean' in single_sample:
        summary_text += f"""
    GPU Timing (Batch=1):
      Kernel Time: {single_sample['gpu_mean']*1000:.2f} ms
      CPU Overhead: {(single_sample['cpu_mean']-single_sample['gpu_mean'])*1000:.2f} ms
    """
    
    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes,
             fontsize=11, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.5))
    
    plt.suptitle(f'Inference Performance Benchmark - {model_dir.name}', 
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # Save plot
    plots_dir = model_dir / 'plots'
    plots_dir.mkdir(exist_ok=True)
    save_path = plots_dir / 'inference_benchmark.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Benchmark plot saved to: {save_path}")
    
    # Create a detailed timing distribution plot
    fig2, ax = plt.subplots(figsize=(10, 6))
    
    # Plot timing distributions for key batch sizes
    key_batch_sizes = [1, 8, 32, 128]
    positions = np.arange(len(key_batch_sizes))
    
    means = [results[bs]['cpu_mean']*1000 for bs in key_batch_sizes]
    stds = [results[bs]['cpu_std']*1000 for bs in key_batch_sizes]
    
    bars = ax.bar(positions, means, yerr=stds, capsize=10, 
                   color=['blue', 'green', 'orange', 'red'],
                   alpha=0.7, edgecolor='black', linewidth=2)
    
    ax.set_xticks(positions)
    ax.set_xticklabels([f'Batch {bs}' for bs in key_batch_sizes])
    ax.set_ylabel('Inference Time (ms)')
    ax.set_title('Inference Time Distribution for Different Batch Sizes')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, mean, std in zip(bars, means, stds):
        height = bar.get_height()
        ax.annotate(f'{mean:.1f}±{std:.1f} ms',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')
    
    plt.tight_layout()
    save_path2 = plots_dir / 'inference_timing_distribution.png'
    plt.savefig(save_path2, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Timing distribution plot saved to: {save_path2}")

def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Get latest model directory
    model_dir = get_latest_model_dir()
    logging.info(f"Benchmarking model: {model_dir.name}")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")
    
    # Set optimization flags
    if device.type == 'cuda':
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    # Load model
    model, input_dim, config = load_model(model_dir, device)
    
    # Run benchmark
    results = benchmark_inference(model, input_dim, device)
    
    # Plot results
    plot_benchmark_results(results, model_dir, device)
    
    # Print summary
    print(f"\nBenchmark Complete!")
    print(f"Single sample latency: {results[1]['cpu_mean']*1000:.2f} ms")
    print(f"Peak throughput: {max(r['throughput'] for r in results.values()):.0f} samples/sec")

if __name__ == '__main__':
    main()

===== /Users/imalsky/Desktop/Chemulator/test/plot_training.py =====
#!/usr/bin/env python3
"""
Plot comprehensive training logs and metrics from model training.
Shows loss curves, learning rates, and training statistics.
"""

import json
import sys
from pathlib import Path
import logging

sys.path.append(str(Path(__file__).parent.parent))

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from datetime import datetime

# Configure matplotlib
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (16, 12)
plt.rcParams['font.size'] = 11

def get_latest_model_dir():
    """Get the most recent model directory."""
    models_dir = Path(__file__).parent.parent / 'data' / 'models'
    model_dirs = [d for d in models_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    if not model_dirs:
        raise ValueError("No model directories found!")
    
    return max(model_dirs, key=lambda d: d.stat().st_mtime)

def load_training_log(model_dir):
    """Load training log and config."""
    log_path = model_dir / 'training_log.json'
    config_path = model_dir / 'config.json'
    
    if not log_path.exists():
        raise FileNotFoundError(f"No training log found at {log_path}")
    
    with open(log_path, 'r') as f:
        training_log = json.load(f)
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    return training_log, config

def calculate_statistics(values):
    """Calculate running statistics for a series."""
    if not values:
        return []
    
    window_size = min(10, len(values) // 4)
    if window_size < 1:
        return values
    
    # Calculate moving average
    moving_avg = []
    for i in range(len(values)):
        start = max(0, i - window_size + 1)
        window = values[start:i+1]
        moving_avg.append(np.mean(window))
    
    return moving_avg

def plot_comprehensive_metrics(training_log, config, model_dir):
    """Create comprehensive training plots."""
    epochs_data = training_log['epochs']
    
    if not epochs_data:
        logging.warning("No epoch data found in training log!")
        return
    
    # Extract data
    epochs = [e['epoch'] for e in epochs_data]
    train_losses = [e['train_loss'] for e in epochs_data]
    val_losses = [e.get('val_loss', None) for e in epochs_data]
    learning_rates = [e.get('lr', 0) for e in epochs_data]
    epoch_times = [e.get('epoch_time', 0) for e in epochs_data]
    
    # Filter out None values from validation losses
    val_epochs = [ep for ep, vl in zip(epochs, val_losses) if vl is not None]
    val_losses_clean = [vl for vl in val_losses if vl is not None]
    
    # Create figure with subplots
    fig = plt.figure(figsize=(16, 12))
    gs = gridspec.GridSpec(3, 2, height_ratios=[2, 1, 1], hspace=0.3, wspace=0.25)
    
    # 1. Main loss plot (top, spanning both columns)
    ax1 = fig.add_subplot(gs[0, :])
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.8)
    if val_losses_clean:
        ax1.plot(val_epochs, val_losses_clean, 'r-', label='Validation Loss', linewidth=2, alpha=0.8)
        
        # Add moving averages
        train_ma = calculate_statistics(train_losses)
        val_ma = calculate_statistics(val_losses_clean)
        ax1.plot(epochs, train_ma, 'b--', alpha=0.5, label='Training MA', linewidth=1)
        if len(val_ma) == len(val_epochs):
            ax1.plot(val_epochs, val_ma, 'r--', alpha=0.5, label='Validation MA', linewidth=1)
    
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    
    # Add log scale option if losses span multiple orders of magnitude
    if max(train_losses) / min(train_losses) > 100:
        ax1.set_yscale('log')
    
    # 2. Learning rate plot
    ax2 = fig.add_subplot(gs[1, 0])
    ax2.plot(epochs, learning_rates, 'g-', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Learning Rate', fontsize=12)
    ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.set_yscale('log')
    
    # 3. Training time per epoch
    ax3 = fig.add_subplot(gs[1, 1])
    ax3.plot(epochs, epoch_times, 'orange', linewidth=2)
    ax3.set_xlabel('Epoch', fontsize=12)
    ax3.set_ylabel('Time (seconds)', fontsize=12)
    ax3.set_title('Training Time per Epoch', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    
    # Add cumulative time on secondary axis
    ax3_twin = ax3.twinx()
    cumulative_time = np.cumsum(epoch_times)
    ax3_twin.plot(epochs, cumulative_time / 3600, 'm--', alpha=0.7, linewidth=1.5)
    ax3_twin.set_ylabel('Cumulative Time (hours)', fontsize=12, color='m')
    ax3_twin.tick_params(axis='y', labelcolor='m')
    
    # 4. Loss improvement rate
    ax4 = fig.add_subplot(gs[2, 0])
    if len(train_losses) > 1:
        loss_improvement = np.diff(train_losses)
        ax4.plot(epochs[1:], loss_improvement, 'c-', linewidth=1.5, alpha=0.8)
        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Epoch', fontsize=12)
        ax4.set_ylabel('Loss Change', fontsize=12)
        ax4.set_title('Training Loss Change per Epoch', fontsize=14, fontweight='bold')
        ax4.grid(True, alpha=0.3)
    
    # 5. Training statistics summary
    ax5 = fig.add_subplot(gs[2, 1])
    ax5.axis('off')
    
    # Calculate statistics
    total_time = sum(epoch_times) / 3600  # in hours
    final_train_loss = train_losses[-1]
    best_val_loss = min(val_losses_clean) if val_losses_clean else None
    best_epoch = val_epochs[val_losses_clean.index(best_val_loss)] if best_val_loss else None
    
    # Model info
    model_type = config['model']['type']
    prediction_mode = config['prediction']['mode']
    batch_size = config['training']['batch_size']
    
    stats_text = f"""Training Summary
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Model Type: {model_type}
    Prediction Mode: {prediction_mode}
    Batch Size: {batch_size:,}
    Total Epochs: {len(epochs)}
    Total Training Time: {total_time:.2f} hours
    Average Time/Epoch: {np.mean(epoch_times):.1f} seconds
    
    Final Training Loss: {final_train_loss:.6f}
    Best Validation Loss: {best_val_loss:.6f if best_val_loss else 'N/A'}
    Best Epoch: {best_epoch if best_epoch else 'N/A'}
    
    Initial Learning Rate: {learning_rates[0]:.2e}
    Final Learning Rate: {learning_rates[-1]:.2e}
    
    Loss Reduction: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%
    """
    
    ax5.text(0.1, 0.9, stats_text, transform=ax5.transAxes, 
             fontsize=11, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.5))
    
    # Add title with model name and timestamp
    model_name = model_dir.name
    fig.suptitle(f'Training Metrics - {model_name}', fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    
    # Save plot
    plots_dir = model_dir / 'plots'
    plots_dir.mkdir(exist_ok=True)
    save_path = plots_dir / 'training_logs.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Training log plot saved to: {save_path}")
    
    # Also save a simplified loss-only plot
    fig2, ax = plt.subplots(figsize=(10, 6))
    ax.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2.5)
    if val_losses_clean:
        ax.plot(val_epochs, val_losses_clean, 'r-', label='Validation Loss', linewidth=2.5)
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Loss', fontsize=12)
    ax.set_title('Training Progress', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    if max(train_losses) / min(train_losses) > 100:
        ax.set_yscale('log')
    
    plt.tight_layout()
    save_path2 = plots_dir / 'loss_curves.png'
    plt.savefig(save_path2, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Simple loss plot saved to: {save_path2}")

def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Get latest model directory
    model_dir = get_latest_model_dir()
    logging.info(f"Analyzing model: {model_dir.name}")
    
    # Load training log
    training_log, config = load_training_log(model_dir)
    
    # Create plots
    plot_comprehensive_metrics(training_log, config, model_dir)
    
    # Print summary
    epochs_data = training_log['epochs']
    if epochs_data:
        print(f"\nTraining completed with {len(epochs_data)} epochs")
        print(f"Final training loss: {epochs_data[-1]['train_loss']:.6f}")
        if 'val_loss' in epochs_data[-1] and epochs_data[-1]['val_loss'] is not None:
            print(f"Final validation loss: {epochs_data[-1]['val_loss']:.6f}")

if __name__ == '__main__':
    main()

===== /Users/imalsky/Desktop/Chemulator/test/plot_predictions.py =====
#!/usr/bin/env python3
"""
Test model predictions on a randomly selected test profile.
Plots true vs predicted abundances for all species.
"""

import json
import sys
from pathlib import Path
import random
import logging

# Add parent directory to path to import from src
sys.path.append(str(Path(__file__).parent.parent))

import numpy as np
import torch
import matplotlib.pyplot as plt
import h5py
import re
import hashlib

from src.models.model import create_model
from src.data.normalizer import NormalizationHelper

# Configure matplotlib for better plots
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 12

def get_latest_model_dir():
    """Get the most recent model directory."""
    models_dir = Path(__file__).parent.parent / 'data' / 'models'
    
    # List all model directories
    model_dirs = [d for d in models_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    if not model_dirs:
        raise ValueError("No model directories found!")
    
    # Sort by modification time and return the latest
    latest = max(model_dirs, key=lambda d: d.stat().st_mtime)
    return latest

def load_model_and_config(model_dir):
    """Load model, config, and normalization helper."""
    logging.info(f"Loading model from: {model_dir}")
    
    # Load config
    config_path = model_dir / 'config.json'
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load normalization stats
    processed_dir = Path(config['paths']['processed_data_dir']) / f"mode_{config['prediction']['mode']}"
    norm_stats_path = processed_dir / 'normalization.json'
    with open(norm_stats_path, 'r') as f:
        norm_stats = json.load(f)
    
    norm_helper = NormalizationHelper(
        stats=norm_stats,
        device=device,
        species_vars=config['data']['species_variables'],
        global_vars=config['data']['global_variables'],
        time_var=config['data']['time_variable'],
        config=config
    )
    
    # Try to load exported model first, fall back to checkpoint
    exported_path = model_dir / 'exported_model.pt'
    checkpoint_path = model_dir / 'best_model.pt'
    
    if exported_path.exists():
        logging.info("Loading exported model...")
        try:
            import torch.export
            exported_program = torch.export.load(str(exported_path))
            model = exported_program.module()
        except:
            # Fallback to JIT
            model = torch.jit.load(str(exported_path))
    elif checkpoint_path.exists():
        logging.info("Loading from checkpoint...")
        model = create_model(config, device)
        checkpoint = torch.load(checkpoint_path, map_location=device)
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        # Remove _orig_mod prefix if present
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
        model.load_state_dict(state_dict)
    else:
        raise FileNotFoundError("No model file found!")
    
    model.eval()
    model = model.to(device)
    
    return model, config, norm_helper, device

def find_random_test_profile(config):
    """Find a random test profile from the dataset."""
    # Get test split info
    processed_dir = Path(config['paths']['processed_data_dir']) / f"mode_{config['prediction']['mode']}"
    shard_index_path = processed_dir / 'shard_index.json'
    
    with open(shard_index_path, 'r') as f:
        shard_index = json.load(f)
    
    # Get a random test shard
    test_shards = shard_index['splits']['test']['shards']
    if not test_shards:
        raise ValueError("No test shards found!")
    
    random_shard = random.choice(test_shards)
    shard_path = processed_dir / 'test' / random_shard['filename']
    
    # Load shard data
    shard_data = np.load(shard_path)
    
    # Pick a random sample
    random_idx = random.randint(0, shard_data.shape[0] - 1)
    sample = shard_data[random_idx]
    
    # Extract initial conditions (normalized)
    n_species = len(config['data']['species_variables'])
    n_globals = len(config['data']['global_variables'])
    
    initial_conditions_norm = sample[:n_species + n_globals]
    
    # Now we need to find the corresponding full profile in the raw data
    # This is tricky, so we'll reconstruct from multiple time samples if available
    
    # For now, let's generate predictions at multiple time points
    time_points = np.logspace(-8, 2, 100)  # 100 time points from 1e-8 to 100
    
    return initial_conditions_norm, time_points

def make_predictions(model, initial_conditions_norm, time_points_raw, config, norm_helper, device):
    """Generate predictions for multiple time points."""
    n_species = len(config['data']['species_variables'])
    predictions = []
    
    # Normalize time points
    time_tensor = torch.tensor(time_points_raw, dtype=torch.float32).unsqueeze(1)
    time_profile = torch.zeros(len(time_points_raw), 1)
    time_profile[:, 0] = time_tensor.squeeze()
    
    # Create a dummy profile just for time normalization
    dummy_profile = torch.zeros(len(time_points_raw), len(config['data']['species_variables']) + len(config['data']['global_variables']) + 1)
    dummy_profile[:, -1] = time_tensor.squeeze()
    
    norm_profile = norm_helper.normalize_profile(dummy_profile)
    time_points_norm = norm_profile[:, -1]
    
    # Batch predictions
    batch_size = 100
    initial_conditions_tensor = torch.tensor(initial_conditions_norm, dtype=torch.float32, device=device)
    
    for i in range(0, len(time_points_norm), batch_size):
        end_idx = min(i + batch_size, len(time_points_norm))
        batch_times = time_points_norm[i:end_idx].to(device)
        
        # Prepare input batch
        batch_size_actual = end_idx - i
        batch_initials = initial_conditions_tensor.unsqueeze(0).expand(batch_size_actual, -1)
        batch_input = torch.cat([batch_initials, batch_times.unsqueeze(1)], dim=1)
        
        with torch.no_grad():
            batch_predictions = model(batch_input)
        
        predictions.append(batch_predictions.cpu())
    
    predictions = torch.cat(predictions, dim=0).numpy()
    
    # Denormalize predictions if in absolute mode
    if config['prediction']['mode'] == 'absolute':
        # Create full profile for denormalization
        full_profile_norm = torch.zeros(len(time_points_raw), n_species + len(config['data']['global_variables']) + 1)
        full_profile_norm[:, :n_species] = torch.tensor(predictions)
        full_profile_norm[:, n_species:n_species+len(config['data']['global_variables'])] = torch.tensor(initial_conditions_norm[n_species:])
        full_profile_norm[:, -1] = time_points_norm
        
        denorm_profile = norm_helper.denormalize_profile(full_profile_norm)
        predictions = denorm_profile[:, :n_species].numpy()
        
        # Also denormalize initial conditions for plotting
        initial_profile = torch.zeros(1, n_species + len(config['data']['global_variables']) + 1)
        initial_profile[0, :n_species + len(config['data']['global_variables'])] = torch.tensor(initial_conditions_norm)
        initial_denorm = norm_helper.denormalize_profile(initial_profile)
        initial_species = initial_denorm[0, :n_species].numpy()
    else:
        # Ratio mode - need to denormalize differently
        initial_profile = torch.zeros(1, n_species + len(config['data']['global_variables']) + 1)
        initial_profile[0, :n_species + len(config['data']['global_variables'])] = torch.tensor(initial_conditions_norm)
        initial_denorm = norm_helper.denormalize_profile(initial_profile)
        initial_species = initial_denorm[0, :n_species].numpy()
        
        # Convert ratio predictions to absolute
        predictions = norm_helper.denormalize_ratio_predictions(
            torch.tensor(predictions),
            torch.tensor(initial_species)
        ).numpy()
    
    return predictions, initial_species

def plot_predictions(time_points, predictions, initial_species, species_names, model_dir):
    """Create and save the prediction plot."""
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # Use a colormap with enough distinct colors
    colors = plt.cm.tab20(np.linspace(0, 1, len(species_names)))
    
    # Plot each species
    for i, (species, color) in enumerate(zip(species_names, colors)):
        # Plot predictions
        ax.plot(time_points, predictions[:, i], '-', color=color, linewidth=2.5, 
                label=f'{species}', alpha=0.8)
        
        # Mark initial condition
        ax.scatter(time_points[0], initial_species[i], color=color, s=100, 
                   edgecolor='black', linewidth=2, zorder=5)
    
    ax.set_xscale('log')
    ax.set_yscale('log')
    ax.set_xlabel('Time (s)', fontsize=14)
    ax.set_ylabel('Abundance', fontsize=14)
    ax.set_title('Model Predictions for Random Test Sample', fontsize=16)
    ax.grid(True, which='both', alpha=0.3)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., 
              frameon=True, fancybox=True, shadow=True)
    
    # Set reasonable axis limits
    ax.set_xlim(time_points.min(), time_points.max())
    y_min = max(predictions.min(), 1e-30)
    y_max = predictions.max() * 10
    ax.set_ylim(y_min, y_max)
    
    plt.tight_layout()
    
    # Save plot
    plots_dir = model_dir / 'plots'
    plots_dir.mkdir(exist_ok=True)
    save_path = plots_dir / 'test_predictions.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Plot saved to: {save_path}")

def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Get latest model directory
    model_dir = get_latest_model_dir()
    logging.info(f"Using model: {model_dir.name}")
    
    # Load model and config
    model, config, norm_helper, device = load_model_and_config(model_dir)
    
    # Find a random test profile
    initial_conditions_norm, time_points = find_random_test_profile(config)
    
    # Make predictions
    predictions, initial_species = make_predictions(
        model, initial_conditions_norm, time_points, config, norm_helper, device
    )
    
    # Plot results
    species_names = config['data']['species_variables']
    plot_predictions(time_points, predictions, initial_species, species_names, model_dir)
    
    # Print some statistics
    logging.info(f"Initial conditions (P={initial_conditions_norm[-2]:.2e}, T={initial_conditions_norm[-1]:.2e})")
    logging.info(f"Time range: {time_points.min():.2e} - {time_points.max():.2e} seconds")
    logging.info(f"Species range: {predictions.min():.2e} - {predictions.max():.2e}")

if __name__ == '__main__':
    main()

===== /Users/imalsky/Desktop/Chemulator/test/plot_errors.py =====
#!/usr/bin/env python3
"""
Analyze and plot model error distributions across test set.
Shows per-species error statistics and distributions.
"""

import json
import sys
from pathlib import Path
import logging

sys.path.append(str(Path(__file__).parent.parent))

import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from scipy import stats as scipy_stats
import seaborn as sns

from src.models.model import create_model
from src.data.normalizer import NormalizationHelper
from src.data.dataset import NPYDataset

# Configure matplotlib
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (16, 12)
plt.rcParams['font.size'] = 10

def get_latest_model_dir():
    """Get the most recent model directory."""
    models_dir = Path(__file__).parent.parent / 'data' / 'models'
    model_dirs = [d for d in models_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    if not model_dirs:
        raise ValueError("No model directories found!")
    
    return max(model_dirs, key=lambda d: d.stat().st_mtime)

def load_model_and_data(model_dir):
    """Load model, config, and test dataset."""
    logging.info(f"Loading model from: {model_dir}")
    
    # Load config
    config_path = model_dir / 'config.json'
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load normalization stats
    processed_dir = Path(config['paths']['processed_data_dir']) / f"mode_{config['prediction']['mode']}"
    norm_stats_path = processed_dir / 'normalization.json'
    with open(norm_stats_path, 'r') as f:
        norm_stats = json.load(f)
    
    norm_helper = NormalizationHelper(
        stats=norm_stats,
        device=device,
        species_vars=config['data']['species_variables'],
        global_vars=config['data']['global_variables'],
        time_var=config['data']['time_variable'],
        config=config
    )
    
    # Load model
    exported_path = model_dir / 'exported_model.pt'
    checkpoint_path = model_dir / 'best_model.pt'
    
    if exported_path.exists():
        logging.info("Loading exported model...")
        try:
            import torch.export
            exported_program = torch.export.load(str(exported_path))
            model = exported_program.module()
        except:
            model = torch.jit.load(str(exported_path))
    elif checkpoint_path.exists():
        logging.info("Loading from checkpoint...")
        model = create_model(config, device)
        checkpoint = torch.load(checkpoint_path, map_location=device)
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
        model.load_state_dict(state_dict)
    else:
        raise FileNotFoundError("No model file found!")
    
    model.eval()
    model = model.to(device)
    
    # Load test dataset
    test_dataset = NPYDataset(
        shard_dir=processed_dir,
        split_name='test',
        config=config,
        device=device
    )
    
    return model, config, norm_helper, test_dataset, device

def compute_errors(model, test_dataset, config, norm_helper, device, max_samples=10000):
    """Compute prediction errors on test set."""
    n_species = len(config['data']['species_variables'])
    prediction_mode = config['prediction']['mode']
    
    # Limit samples for memory efficiency
    n_samples = min(len(test_dataset), max_samples)
    logging.info(f"Computing errors on {n_samples} test samples...")
    
    # Storage for errors
    absolute_errors = []
    relative_errors = []
    log_errors = []
    
    # Batch processing
    batch_size = min(1000, n_samples)
    
    with torch.no_grad():
        for i in range(0, n_samples, batch_size):
            end_idx = min(i + batch_size, n_samples)
            
            # Get batch
            inputs_list = []
            targets_list = []
            
            for j in range(i, end_idx):
                inp, tgt = test_dataset[j]
                inputs_list.append(inp)
                targets_list.append(tgt)
            
            inputs = torch.stack(inputs_list)
            targets = torch.stack(targets_list)
            
            if not test_dataset.gpu_cache:
                inputs = inputs.to(device)
                targets = targets.to(device)
            
            # Predictions
            predictions = model(inputs)
            
            # Denormalize for error calculation
            if prediction_mode == 'absolute':
                # Create full profiles for denormalization
                n_globals = len(config['data']['global_variables'])
                
                # Predicted profile
                pred_profile = torch.zeros(predictions.shape[0], n_species + n_globals + 1, device=device)
                pred_profile[:, :n_species] = predictions
                pred_profile[:, n_species:n_species+n_globals] = inputs[:, n_species:n_species+n_globals]
                pred_profile[:, -1] = inputs[:, -1]
                
                # Target profile  
                target_profile = torch.zeros_like(pred_profile)
                target_profile[:, :n_species] = targets
                target_profile[:, n_species:n_species+n_globals] = inputs[:, n_species:n_species+n_globals]
                target_profile[:, -1] = inputs[:, -1]
                
                # Denormalize
                pred_denorm = norm_helper.denormalize_profile(pred_profile)[:, :n_species]
                target_denorm = norm_helper.denormalize_profile(target_profile)[:, :n_species]
                
            else:  # ratio mode
                # Get initial species (denormalized)
                initial_profile = torch.zeros(predictions.shape[0], n_species + len(config['data']['global_variables']) + 1, device=device)
                initial_profile[:, :n_species + len(config['data']['global_variables'])] = inputs[:, :n_species + len(config['data']['global_variables'])]
                initial_denorm = norm_helper.denormalize_profile(initial_profile)
                initial_species = initial_denorm[:, :n_species]
                
                # Convert predictions
                pred_denorm = norm_helper.denormalize_ratio_predictions(predictions, initial_species)
                target_denorm = norm_helper.denormalize_ratio_predictions(targets, initial_species)
            
            # Calculate errors
            abs_err = torch.abs(pred_denorm - target_denorm).cpu().numpy()
            absolute_errors.append(abs_err)
            
            # Relative error (avoid division by zero)
            target_cpu = target_denorm.cpu().numpy()
            rel_err = abs_err / (np.abs(target_cpu) + 1e-30)
            relative_errors.append(rel_err)
            
            # Log error (for positive values)
            pred_cpu = pred_denorm.cpu().numpy()
            mask = (pred_cpu > 0) & (target_cpu > 0)
            log_err = np.full_like(pred_cpu, np.nan)
            log_err[mask] = np.abs(np.log10(pred_cpu[mask]) - np.log10(target_cpu[mask]))
            log_errors.append(log_err)
            
            if (i + batch_size) % 1000 == 0:
                logging.info(f"Processed {i + batch_size}/{n_samples} samples")
    
    # Concatenate all errors
    absolute_errors = np.concatenate(absolute_errors, axis=0)
    relative_errors = np.concatenate(relative_errors, axis=0)
    log_errors = np.concatenate(log_errors, axis=0)
    
    return absolute_errors, relative_errors, log_errors

def plot_error_distributions(absolute_errors, relative_errors, log_errors, species_names, model_dir):
    """Create comprehensive error distribution plots."""
    n_species = len(species_names)
    
    # Create figure with subplots
    fig = plt.figure(figsize=(18, 14))
    gs = gridspec.GridSpec(4, 3, height_ratios=[1.5, 1, 1, 1], hspace=0.4, wspace=0.3)
    
    # 1. Overall error distributions (top row)
    ax1 = fig.add_subplot(gs[0, 0])
    ax2 = fig.add_subplot(gs[0, 1])
    ax3 = fig.add_subplot(gs[0, 2])
    
    # Absolute error distribution
    all_abs_errors = absolute_errors.flatten()
    all_abs_errors = all_abs_errors[all_abs_errors < np.percentile(all_abs_errors, 99)]  # Remove outliers
    ax1.hist(all_abs_errors, bins=50, alpha=0.7, color='blue', edgecolor='black')
    ax1.set_xlabel('Absolute Error')
    ax1.set_ylabel('Count')
    ax1.set_title('Overall Absolute Error Distribution')
    ax1.set_yscale('log')
    
    # Relative error distribution
    all_rel_errors = relative_errors.flatten()
    all_rel_errors = all_rel_errors[all_rel_errors < np.percentile(all_rel_errors, 99)]
    ax2.hist(all_rel_errors, bins=50, alpha=0.7, color='green', edgecolor='black')
    ax2.set_xlabel('Relative Error')
    ax2.set_ylabel('Count')
    ax2.set_title('Overall Relative Error Distribution')
    ax2.set_yscale('log')
    
    # Log error distribution
    all_log_errors = log_errors.flatten()
    all_log_errors = all_log_errors[~np.isnan(all_log_errors)]
    if len(all_log_errors) > 0:
        ax3.hist(all_log_errors, bins=50, alpha=0.7, color='red', edgecolor='black')
        ax3.set_xlabel('Log10 Error')
        ax3.set_ylabel('Count')
        ax3.set_title('Overall Log10 Error Distribution')
        ax3.set_yscale('log')
    
    # 2. Per-species error statistics (box plots)
    ax4 = fig.add_subplot(gs[1, :])
    
    # Prepare data for box plot
    rel_error_data = []
    for i in range(n_species):
        species_errors = relative_errors[:, i]
        # Remove extreme outliers for visualization
        species_errors = species_errors[species_errors < np.percentile(species_errors, 95)]
        rel_error_data.append(species_errors)
    
    bp = ax4.boxplot(rel_error_data, labels=species_names, patch_artist=True)
    for patch in bp['boxes']:
        patch.set_facecolor('lightblue')
    
    ax4.set_ylabel('Relative Error')
    ax4.set_title('Per-Species Relative Error Distribution')
    ax4.set_yscale('log')
    ax4.grid(True, alpha=0.3)
    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    # 3. Error statistics table
    ax5 = fig.add_subplot(gs[2:, :])
    ax5.axis('off')
    
    # Calculate statistics
    stats_data = []
    headers = ['Species', 'Mean Abs Err', 'Median Abs Err', 'Mean Rel Err', 'Median Rel Err', '95% Rel Err']
    
    for i, species in enumerate(species_names):
        abs_err = absolute_errors[:, i]
        rel_err = relative_errors[:, i]
        
        stats_data.append([
            species,
            f'{np.mean(abs_err):.2e}',
            f'{np.median(abs_err):.2e}',
            f'{np.mean(rel_err):.2%}',
            f'{np.median(rel_err):.2%}',
            f'{np.percentile(rel_err, 95):.2%}'
        ])
    
    # Add overall statistics
    stats_data.append([''] * len(headers))  # Empty row
    stats_data.append([
        'OVERALL',
        f'{np.mean(absolute_errors):.2e}',
        f'{np.median(absolute_errors):.2e}',
        f'{np.mean(relative_errors):.2%}',
        f'{np.median(relative_errors):.2%}',
        f'{np.percentile(relative_errors, 95):.2%}'
    ])
    
    # Create table
    table = ax5.table(cellText=stats_data, colLabels=headers, loc='center', cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    
    # Style the table
    for i in range(len(headers)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # Highlight overall row
    for i in range(len(headers)):
        table[(len(stats_data), i)].set_facecolor('#FFC107')
        table[(len(stats_data), i)].set_text_props(weight='bold')
    
    # Add title
    fig.suptitle(f'Error Analysis - {model_dir.name}', fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    
    # Save plot
    plots_dir = model_dir / 'plots'
    plots_dir.mkdir(exist_ok=True)
    save_path = plots_dir / 'error_distributions.png'
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Error distribution plot saved to: {save_path}")
    
    # Create a second plot showing error vs magnitude
    fig2, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()
    
    # Sample 4 species for detailed plots
    species_indices = np.linspace(0, n_species-1, min(4, n_species), dtype=int)
    
    for idx, species_idx in enumerate(species_indices):
        ax = axes[idx]
        
        # Get errors and targets for this species
        abs_err = absolute_errors[:, species_idx]
        rel_err = relative_errors[:, species_idx]
        
        # Create 2D histogram
        hist, xedges, yedges = np.histogram2d(
            np.log10(abs_err + 1e-30), 
            np.log10(rel_err + 1e-30),
            bins=50
        )
        
        im = ax.imshow(hist.T, origin='lower', aspect='auto', 
                       extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],
                       cmap='viridis')
        
        ax.set_xlabel('Log10(Absolute Error)')
        ax.set_ylabel('Log10(Relative Error)')
        ax.set_title(f'{species_names[species_idx]}')
        plt.colorbar(im, ax=ax, label='Count')
    
    plt.suptitle('Error Correlation Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    save_path2 = plots_dir / 'error_correlation.png'
    plt.savefig(save_path2, dpi=150, bbox_inches='tight')
    plt.close()
    
    logging.info(f"Error correlation plot saved to: {save_path2}")

def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Get latest model directory
    model_dir = get_latest_model_dir()
    logging.info(f"Analyzing model: {model_dir.name}")
    
    # Load model and data
    model, config, norm_helper, test_dataset, device = load_model_and_data(model_dir)
    
    # Compute errors
    absolute_errors, relative_errors, log_errors = compute_errors(
        model, test_dataset, config, norm_helper, device
    )
    
    # Plot distributions
    species_names = config['data']['species_variables']
    plot_error_distributions(absolute_errors, relative_errors, log_errors, species_names, model_dir)
    
    # Print summary
    print(f"\nError Analysis Summary:")
    print(f"Mean absolute error: {np.mean(absolute_errors):.2e}")
    print(f"Median absolute error: {np.median(absolute_errors):.2e}")
    print(f"Mean relative error: {np.mean(relative_errors):.2%}")
    print(f"Median relative error: {np.median(relative_errors):.2%}")

if __name__ == '__main__':
    main()

===== /Users/imalsky/Desktop/Chemulator/config/config.jsonc =====
{
    // ===== OPTIMIZED CONFIGURATION FOR A100 GPU =====
    // This configuration maximizes performance on NVIDIA A100 80GB
    
    // ===== FILE PATHS =====
    "paths": {
        // Raw HDF5 data files to process
        "raw_data_files": [
            "data/raw/run9001-result.h5",
            "data/raw/run9002-result.h5",
            "data/raw/run9003-result.h5",
            "data/raw/run9004-result.h5",
            "data/raw/run9005-result.h5",
            "data/raw/run9006-result.h5",
            "data/raw/run9007-result.h5",
            "data/raw/run9008-result.h5",
            "data/raw/run9009-result.h5",
            "data/raw/run9010-result.h5"
        ],
        // Directory for processed NPY shards
        "processed_data_dir": "data/processed",
        // Directory for saved models
        "model_save_dir": "data/models",
        // Directory for logs
        "log_dir": "logs"
    },
    
    // ===== DATA CONFIGURATION =====
    "data": {
        // Chemical species to predict (order matters!)
        "species_variables": [
            "C2H2_evolution",
            "CH4_evolution",
            "CO2_evolution",
            "CO_evolution",
            "H2O_evolution",
            "H2_evolution",
            "HCN_evolution",
            "H_evolution",
            "N2_evolution",
            "NH3_evolution",
            "OH_evolution",
            "O_evolution"
        ],
        // Global parameters (initial conditions)
        "global_variables": ["P_init", "T_init"],
        // Time variable name in HDF5 files
        "time_variable": "t_time"
    },
    
    // ===== PREPROCESSING SETTINGS =====
    "preprocessing": {
        // Number of samples per NPY shard file
        "shard_size": 10000000,
        // Minimum species concentration threshold
        "min_value_threshold": 1e-25,
        // Compression type: null for raw npy files (faster I/O)
        "compression": null,
        // Number of parallel workers for preprocessing
        "num_workers": 8,
        // Enable parallel preprocessing
        "parallel_enabled": true
    },
    
    // ===== NORMALIZATION SETTINGS =====
    "normalization": {
        // Default normalization method for all variables
        "default_method": "log-min-max",
        
        // Override methods for specific variables
        "methods": {
            "T_init": "standard",
            "P_init": "log-min-max",
            "t_time": "log-min-max"
        },
        
        // Epsilon for numerical stability
        "epsilon": 1e-38,
        // Minimum standard deviation to prevent division by tiny values
        "min_std": 1e-10,
        // Clamp normalized values to [-clamp_value, clamp_value]
        "clamp_value": 50.0
    },
    
    // ===== MODEL ARCHITECTURE =====
    "model": {
        // Model type: "deeponet" or "siren"
        "type": "deeponet",
        
        // Activation function: "gelu", "relu", "silu", "tanh"
        "activation": "gelu",
        
        // Dropout rate (0.0 = no dropout)
        "dropout": 0.0,
        
        // Output scaling factor (1.0 = no scaling)
        "output_scale": 1.0,
        
        // DeepONet-specific parameters
        "branch_layers": [512, 512, 512, 512],
        "trunk_layers": [128, 128, 128, 128],
        "basis_dim": 64,
        
        // SIREN-specific parameters (when type="siren")
        "hidden_dims": [512, 512, 512, 512],
        "omega_0": 30.0
    },
    
    // ===== FiLM CONDITIONING =====
    "film": {
        // Enable Feature-wise Linear Modulation
        "enabled": true,
        // Hidden layers for FiLM networks
        "hidden_dims": [64, 64, 64, 64, 64],
        // Activation for FiLM networks
        "activation": "relu"
    },
    
    // ===== PREDICTION SETTINGS =====
    "prediction": {
        // Prediction mode: "absolute" or "ratio"
        "mode": "absolute",
        
        // Optional output clamping
        "output_clamp": null
    },
    
    // ===== TRAINING PARAMETERS - OPTIMIZED FOR A100 =====
    "training": {
        // Data splitting
        "val_fraction": 0.15,
        "test_fraction": 0.15,
        "use_fraction": 1.0,
        
        // Training duration
        "epochs": 300,
        
        // BATCH SIZE - MAXIMIZED FOR A100 80GB
        "batch_size": 32768,  
        "gradient_accumulation_steps": 1,
        
        // GPU CACHING - CRITICAL FOR PERFORMANCE
        "gpu_cache_dataset": true,  // Force GPU caching
        
        // DATALOADER SETTINGS - OPTIMIZED FOR GPU
        "num_workers": 0,  // MUST be 0 for GPU caching
        "pin_memory": false,  // Not needed with GPU cache
        "persistent_workers": false,
        "prefetch_factor": null,
        "drop_last": true,
        
        // Learning rate - scaled with batch size
        "learning_rate": 5e-4,  // 4x original due to 8x batch size (sqrt scaling)
        "weight_decay": 1e-5,
        "betas": [0.9, 0.999],
        "eps": 1e-8,
        "gradient_clip": 5.0,  // Increased for stability with large batches
        
        // Scheduler settings
        "scheduler": "cosine",
        "scheduler_params": {
            "T_0": 30,  // Restart every 10 epochs
            "T_mult": 2,
            "eta_min": 1e-8
        },
        
        // Loss and training settings
        "loss": "mse",
        "huber_delta": 0.5,
        "use_amp": true,
        "amp_dtype": "bfloat16",  // Better for A100 than float16
        "early_stopping_patience": 5000,
        "min_delta": 1e-8,
        
        // Reduced logging/memory clearing for performance
        "log_interval": 50,  // Less frequent logging
        "save_interval": 5,
        "empty_cache_interval": 50000,  // Almost never
        
        // HPO-specific settings
        "hpo_min_epochs": 10,
        "hpo_max_epochs": 40
    },
    
    // ===== SYSTEM/HARDWARE SETTINGS - A100 OPTIMIZED =====
    "system": {
        // Random seed for reproducibility
        "seed": 42,
        
        // PyTorch optimizations for A100
        "use_torch_compile": true,
        "compile_mode": "max-autotune",  // Maximum optimization
        "use_torch_export": true,
        
        // CUDA optimizations for A100
        "cudnn_benchmark": true,
        "tf32": true,  // Use TensorFloat-32 on A100
        "cuda_memory_fraction": 0.95  // Use most GPU memory
    },
    
    // ===== HYPERPARAMETER OPTIMIZATION =====
    "optuna": {
        // Enable Optuna integration
        "enabled": true,
        
        // Hyperband settings
        "algorithm": "hyperband",
        "hyperband_min_resource": 10,
        "hyperband_max_resource": 50,
        "hyperband_reduction_factor": 3,
        
        // Target number of trials
        "n_trials": 100
    }
}

// ===== A100 OPTIMIZATION NOTES =====
// 
// Key changes from CPU/small GPU config:
// 1. batch_size: 131072 (8x larger) - A100 can handle this easily
// 2. gpu_cache_dataset: true - All data loaded to GPU memory
// 3. num_workers: 0 - No CPU workers needed with GPU cache
// 4. learning_rate: 2e-3 - Scaled with sqrt(batch_size_ratio)
// 5. compile_mode: "max-autotune" - Maximum compilation optimization
// 6. cuda_memory_fraction: 0.95 - Use almost all GPU memory
// 7. amp_dtype: "bfloat16" - Better numerical stability than float16
// 
// Expected performance:
// - GPU utilization: >95%
// - Training speed: ~1-2 minutes per epoch
// - Memory usage: ~20-30GB GPU memory
// 
// If you run out of memory, reduce batch_size to 65536

===== /Users/imalsky/Desktop/Chemulator/src/main.py =====
#!/usr/bin/env python3
import logging
import sys
import time
from pathlib import Path
import torch
from typing import Dict, Any, Union
import hashlib
import json
import os

# =================================================================
# 1. CONFIGURE LOGGING FIRST
# We set this up at the very top so even the prologue can use it.
# =================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(name)s - %(message)s",
    datefmt="%Y%m%d_%H%M%S",
    stream=sys.stdout
)

# =================================================================
# 2. PROLOGUE FOR MAXIMUM PARALLELISM
# This block now uses the logging system to report its actions.
# =================================================================
try:
    # SLURM_CPUS_PER_TASK is the ground truth for the number of cores allocated
    n_cores_str = os.environ.get("SLURM_CPUS_PER_TASK")
    source = "SLURM"
    
    if n_cores_str is None:
        # Fallback for local execution or other schedulers
        n_cores_str = str(os.cpu_count() or 1)
        source = "os.cpu_count()"

    n_cores = int(n_cores_str)
    
    logging.info(f"CORE PROLOGUE: Detected {n_cores} cores via {source}.")
    
    # Set environment variables to force thread counts
    os.environ["OMP_NUM_THREADS"] = str(n_cores)
    os.environ["MKL_NUM_THREADS"] = str(n_cores)
    logging.info(f"CORE PROLOGUE: Set OMP_NUM_THREADS and MKL_NUM_THREADS to {n_cores}.")
    
    # Set PyTorch's internal thread counts
    torch.set_num_threads(n_cores)
    
    # Final verification log
    logging.info(f"CORE PROLOGUE: Verification: torch.get_num_threads() now reports {torch.get_num_threads()} threads.")

except Exception as e:
    logging.error(f"CORE PROLOGUE: Failed to set thread counts: {e}", exc_info=True)


# =================================================================
# 3. REST OF THE APPLICATION
# (The rest of the file is unchanged)
# =================================================================
# Set multiprocessing sharing strategy
import torch.multiprocessing
try:
    torch.multiprocessing.set_sharing_strategy('file_system')
    logging.info("SUCCESS: Set multiprocessing sharing strategy to 'file_system'.")
except RuntimeError:
    logging.warning("Could not set multiprocessing sharing strategy.")

import numpy as np
from utils.hardware import setup_device, optimize_hardware
from utils.utils import setup_logging, seed_everything, ensure_directories, load_json_config, save_json, load_json
from data.preprocessor import DataPreprocessor
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer, diagnose_performance
from data.normalizer import NormalizationHelper


class ChemicalKineticsPipeline:
    """Optimized training pipeline for A100 GPU."""
    def __init__(self, config_or_path: Union[Path, Dict[str, Any]]):
        """Initialize the pipeline."""
        if isinstance(config_or_path, (Path, str)):
            self.config = load_json_config(Path(config_or_path))
        elif isinstance(config_or_path, dict):
            self.config = config_or_path
        else:
            raise TypeError(f"config_or_path must be a Path, str, or dict")
        
        # Get prediction mode
        self.prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        
        # Setup paths
        self.setup_paths()
        
        # Setup logging
        log_file = self.log_dir / f"pipeline_{self.prediction_mode}_{time.strftime('%Y%m%d_%H%M%S')}.log"
        setup_logging(log_file=log_file)
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Chemical Kinetics Pipeline initialized - Mode: {self.prediction_mode}")
        
        seed_everything(self.config["system"]["seed"])
        
        # Setup hardware
        self.device = setup_device()
        optimize_hardware(self.config["system"], self.device)
        
    def setup_paths(self):
        """Create directory structure."""
        paths = self.config["paths"]
        
        # Create run directory
        model_type = self.config["model"]["type"]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.run_save_dir = Path(paths["model_save_dir"]) / f"{model_type}_{self.prediction_mode}_{timestamp}"
        
        # Convert paths
        self.raw_data_files = [Path(f) for f in paths["raw_data_files"]]
        
        # Mode-specific processed directory
        base_processed_dir = Path(paths["processed_data_dir"])
        self.processed_dir = base_processed_dir / f"mode_{self.prediction_mode}"
        
        self.log_dir = Path(paths["log_dir"])
        
        # Create directories
        ensure_directories(self.processed_dir, self.run_save_dir, self.log_dir)

    def _compute_data_hash(self) -> str:
        """
        Compute a hash of data-critical parameters.
        """
        data_params = {
            "raw_files": sorted([str(f) for f in self.raw_data_files]),
            "species_variables": self.config["data"]["species_variables"],
            "global_variables": self.config["data"]["global_variables"],
            "time_variable": self.config["data"]["time_variable"],
            "min_value_threshold": self.config["preprocessing"]["min_value_threshold"],
            "use_fraction": self.config["training"]["use_fraction"],
            "prediction_mode": self.prediction_mode,
            "normalization_methods": self.config["normalization"].get("methods", {}),
            "default_norm_method": self.config["normalization"]["default_method"],
        }
        
        hash_str = json.dumps(data_params, sort_keys=True)
        return hashlib.sha256(hash_str.encode()).hexdigest()[:16]

    def normalize_only(self):
        """Run only the data preprocessing and normalization step."""
        self.logger.info("Running data normalization only...")
        
        # Check if data already exists
        current_hash = self._compute_data_hash()
        hash_file = self.processed_dir / "data_hash.json"
        
        regenerate = True
        if hash_file.exists():
            saved_hash_data = load_json(hash_file)
            if saved_hash_data.get("hash") == current_hash:
                self.logger.info("Data already preprocessed with matching hash. Skipping regeneration.")
                regenerate = False
            else:
                self.logger.info("Data hash mismatch. Regenerating data...")
        
        if regenerate:
            preprocessor = DataPreprocessor(
                raw_files=self.raw_data_files,
                output_dir=self.processed_dir,
                config=self.config
            )
            
            missing = [p for p in self.raw_data_files if not p.exists()]
            if missing:
                raise FileNotFoundError(f"Missing raw data files: {missing}")
            
            # Process to shards and compute normalization
            preprocessor.process_to_npy_shards()
            
            # Save the hash
            save_json({
                "hash": current_hash,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "mode": self.prediction_mode
            }, hash_file)
            
            self.logger.info(f"Data normalization complete. Files saved to: {self.processed_dir}")

    def preprocess_data(self):
        """Preprocess data if needed."""
        self.logger.info(f"Preprocessing data for {self.prediction_mode} mode...")
        self.normalize_only()

    def _log_memory_status(self):
        """Log current memory status."""
        import psutil
        
        # CPU memory
        mem = psutil.virtual_memory()
        self.logger.info(f"System memory: {mem.total/1024**3:.1f}GB total, "
                         f"{mem.available/1024**3:.1f}GB available ({mem.percent:.1f}% used)")
        
        # GPU memory
        if self.device.type == "cuda":
            free_mem, total_mem = torch.cuda.mem_get_info(self.device.index)
            used_mem = total_mem - free_mem
            self.logger.info(f"GPU memory: {total_mem/1024**3:.1f}GB total, "
                             f"{free_mem/1024**3:.1f}GB free, "
                             f"{used_mem/1024**3:.1f}GB used")

    def train_model(self):
        """Train the neural network model."""
        self.logger.info("Starting model training...")

        # Ensure data is preprocessed
        self.preprocess_data()

        # Save config for this run
        save_json(self.config, self.run_save_dir / "config.json")

        # Create model
        model = create_model(self.config, self.device)

        # Log model info
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"Model: {self.config['model']['type']} - Parameters: {total_params:,}")

        # Load normalization stats
        norm_stats = load_json(self.processed_dir / "normalization.json")
        norm_helper = NormalizationHelper(
            norm_stats,
            self.device,
            self.config["data"]["species_variables"],
            self.config["data"]["global_variables"],
            self.config["data"]["time_variable"],
            self.config
        )
        
        # Log memory status before creating datasets
        self._log_memory_status()

        # Create datasets
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="train",
            config=self.config,
            device=self.device
        )
        
        # **FIXED** Safely log cache status to prevent crash
        cache_info = train_dataset.get_cache_info()
        if cache_info.get("type") == "gpu":
            self.logger.info(
                f"✓ GPU caching active for '{train_dataset.split_name}': {cache_info.get('size_gb', 0):.1f}GB loaded."
            )
        elif cache_info.get("type") == "cpu":
            self.logger.warning(
                f"CPU fallback active for '{train_dataset.split_name}'. Reason: {cache_info.get('message', 'N/A')}"
            )
        else:
            self.logger.error(
                f"Cache status error for '{train_dataset.split_name}': {cache_info.get('status', 'unknown')}"
            )

        val_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="validation",
            config=self.config,
            device=self.device
        ) if self.config["training"]["val_fraction"] > 0 else None
        
        test_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="test",
            config=self.config,
            device=self.device
        ) if self.config["training"]["test_fraction"] > 0 else None
        
        # Initialize trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device,
            norm_helper=norm_helper
        )

        # Train model
        best_val_loss = trainer.train()
        
        # Evaluate on test set
        test_loss = trainer.evaluate_test()
        
        self.logger.info(f"Training complete! Best validation loss: {best_val_loss:.6f}")
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        # Save results
        results = {
            "best_val_loss": best_val_loss,
            "test_loss": test_loss,
            "model_path": str(self.run_save_dir / "best_model.pt"),
            "training_time": trainer.total_training_time,
        }
        
        save_json(results, self.run_save_dir / "results.json")
    
    def run_diagnostics(self, num_batches=20):
        """Run performance diagnostics."""
        self.logger.info("Running performance diagnostics...")
        
        # Ensure data is preprocessed
        self.preprocess_data()
        
        # Create model
        model = create_model(self.config, self.device)
        
        # Load norm stats
        norm_stats = load_json(self.processed_dir / "normalization.json")
        norm_helper = NormalizationHelper(
            norm_stats, self.device,
            self.config["data"]["species_variables"],
            self.config["data"]["global_variables"],
            self.config["data"]["time_variable"],
            self.config
        )
        
        # Create training dataset only
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="train",
            config=self.config,
            device=self.device
        )
        
        # Create minimal trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=None,
            test_dataset=None,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device,
            norm_helper=norm_helper
        )
        
        # Run diagnostics
        times = diagnose_performance(trainer, num_batches=num_batches)
        
        # Print summary
        print("\n" + "="*60)
        print("PERFORMANCE DIAGNOSTICS COMPLETE")
        print("="*60)
        print(f"Batch size: {self.config['training']['batch_size']}")
        print(f"Device: {self.device}")
        print(f"Model compilation: {self.config['system'].get('use_torch_compile', False)}")
        print(f"AMP enabled: {self.config['training'].get('use_amp', False)}")
        print(f"GPU cache: {train_dataset.get_cache_info()['type']}")
    
    def run(self):
        """Execute the full training pipeline."""
        try:
            self.train_model()
            self.logger.info("Pipeline completed successfully!")
            self.logger.info(f"Results saved in: {self.run_save_dir}")
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            sys.exit(1)

def main():
    """Main entry point."""
    import argparse
    parser = argparse.ArgumentParser(description="Chemical Kinetics Neural Network Training")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/config.jsonc"),
        help="Path to configuration file"
    )
    
    # Operation mode arguments
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument(
        "--normalize",
        action="store_true",
        help="Only preprocess and normalize the data"
    )
    mode_group.add_argument(
        "--train",
        action="store_true",
        help="Train a model using the configuration"
    )
    mode_group.add_argument(
        "--tune",
        action="store_true",
        help="Run hyperparameter optimization"
    )
    mode_group.add_argument(
        "--diagnose",
        action="store_true",
        help="Run performance diagnostics"
    )
    
    # Additional arguments
    parser.add_argument(
        "--trials",
        type=int,
        default=100,
        help="Number of Optuna trials for hyperparameter optimization"
    )
    parser.add_argument(
        "--study-name",
        type=str,
        default="chemical_kinetics_opt",
        help="Name for Optuna study"
    )
    parser.add_argument(
        "--diagnose-batches",
        type=int,
        default=20,
        help="Number of batches for diagnostics"
    )
    
    args = parser.parse_args()
    
    # Validate config path
    if not args.config.exists():
        print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
        sys.exit(1)
    
    # Execute based on mode
    if args.normalize:
        # Just normalize data
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.normalize_only()
        print("\nData normalization complete!")
        
    elif args.train:
        # Train model
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.run()
        
    elif args.diagnose:
        # Run diagnostics
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.run_diagnostics(num_batches=args.diagnose_batches)
        
    elif args.tune:
        # Run hyperparameter optimization
        try:
            import optuna
        except ImportError:
            print("Installing optuna...")
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "optuna"])
        
        from hyperparameter_tuning import optimize
        
        print(f"Starting hyperparameter optimization with {args.trials} trials...")
        study = optimize(
            config_path=args.config,
            n_trials=args.trials,
            n_jobs=1,
            study_name=args.study_name
        )
        
        # Print results
        print("\n" + "="*60)
        print("Optimization Complete")
        print("="*60)
        print(f"Best validation loss: {study.best_value:.6f}")
        print(f"Best trial: {study.best_trial.number}")
        print("\nBest parameters:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")


if __name__ == "__main__":
    main()

===== /Users/imalsky/Desktop/Chemulator/src/hyperparameter_tuning.py =====
#!/usr/bin/env python3
"""
Hyperparameter tuning for chemical kinetics models using Optuna.
Optimized for ~40 hour runtime with aggressive but smart pruning.
"""

import copy
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional, Callable

import optuna
from optuna.samplers import TPESampler
from optuna.pruners import HyperbandPruner
import torch

from main import ChemicalKineticsPipeline
from utils.hardware import setup_device, optimize_hardware
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer
from data.normalizer import NormalizationHelper
from utils.utils import setup_logging, seed_everything, ensure_directories, load_json_config, save_json, load_json


class OptunaPruningCallback:
    """Callback to report intermediate values to Optuna for pruning."""
    def __init__(self, trial: optuna.Trial, min_epochs: int = 10):
        self.trial = trial
        self.min_epochs = min_epochs
        
    def __call__(self, epoch: int, val_loss: float) -> bool:
        """
        Report intermediate value to Optuna and check if should prune.
        Only allows pruning after min_epochs to avoid conflict with cosine warmup.
        """
        self.trial.report(val_loss, epoch)
        
        # Don't prune during warmup period
        if epoch < self.min_epochs:
            return False
            
        if self.trial.should_prune():
            return True
        return False


class OptunaTrialRunner:
    """Manages the execution of a single Optuna trial."""
    def __init__(self, base_config_path: Path):
        self.base_config_path = base_config_path
        self.base_config = load_json_config(base_config_path)
        self.device = setup_device()
        self.logger = logging.getLogger(__name__)
        self._pipelines = {}
        
        # Preprocess data for all possible modes upfront
        self._prepare_all_modes()

    def _prepare_all_modes(self):
        """Ensure data is preprocessed for all possible prediction modes."""
        mode = self.base_config["prediction"]["mode"]
        self.logger.info(f"Preparing data for '{mode}' mode...")
        
        pipeline = ChemicalKineticsPipeline(self.base_config)
        pipeline.normalize_only()
        
        self._pipelines[mode] = OptunaPipeline(self.base_config, pipeline.processed_dir)

    def run_trial(self, trial: optuna.Trial) -> float:
        """Configures and runs a single trial."""
        config = suggest_model_config(trial, self.base_config)
        prediction_mode = config["prediction"]["mode"]
        pipeline = self._pipelines[prediction_mode]
        return pipeline.execute_trial(config, trial)


class OptunaPipeline:
    """Holds datasets and executes the training for a specific prediction mode."""
    def __init__(self, config: Dict[str, Any], processed_dir: Path):
        self.config = config
        self.device = setup_device()
        self.logger = logging.getLogger(f"OptunaPipeline_{config['prediction']['mode']}")
        
        self.processed_dir = processed_dir
        self.model_save_root = Path(self.config["paths"]["model_save_dir"])
        
        norm_stats_path = self.processed_dir / "normalization.json"
        if not norm_stats_path.exists():
            raise FileNotFoundError(f"Normalization stats not found in {norm_stats_path}")
        norm_stats = load_json(norm_stats_path)
        
        self.norm_helper = NormalizationHelper(
            stats=norm_stats, device=self.device,
            species_vars=self.config["data"]["species_variables"],
            global_vars=self.config["data"]["global_variables"],
            time_var=self.config["data"]["time_variable"],
            config=self.config
        )
        self._load_datasets()

    def _load_datasets(self):
        """Load split-specific datasets."""
        self.logger.info(f"Loading datasets from: {self.processed_dir}")
        
        self.train_dataset = NPYDataset(self.processed_dir, "train", self.config, self.device)
        self.val_dataset = NPYDataset(self.processed_dir, "validation", self.config, self.device)
        
        self.logger.info(
            f"Datasets loaded: train={len(self.train_dataset)}, "
            f"val={len(self.val_dataset)}"
        )

    def execute_trial(self, config: Dict[str, Any], trial: optuna.Trial) -> float:
        """Runs a single trial's training and evaluation with pruning."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        trial_id = f"trial_{trial.number:04d}_{config['prediction']['mode']}"
        save_dir = self.model_save_root / "optuna" / f"{timestamp}_{trial_id}"
        ensure_directories(save_dir)
        
        try:
            seed_everything(config["system"]["seed"])
            optimize_hardware(config["system"], self.device)
            model = create_model(config, self.device)
            
            # Use dynamic epoch allocation from Hyperband
            n_epochs = trial.user_attrs.get("n_epochs", config["training"]["hpo_max_epochs"])
            config["training"]["epochs"] = n_epochs
            
            # Create pruning callback with minimum epochs
            min_epochs = config["training"]["hpo_min_epochs"]
            pruning_callback = OptunaPruningCallback(trial, min_epochs)
            
            # Log trial configuration
            self.logger.info(f"Trial {trial.number}: {n_epochs} epochs allocated by Hyperband")
            self.logger.info(f"Learning rate: {config['training']['learning_rate']:.2e}")
            
            trainer = PrunableTrainer(
                model=model, train_dataset=self.train_dataset,
                val_dataset=self.val_dataset, test_dataset=None,
                config=config, save_dir=save_dir, device=self.device,
                norm_helper=self.norm_helper, epoch_callback=pruning_callback
            )
            
            best_val_loss = trainer.train()

            trial.set_user_attr("full_config", config)
            trial.set_user_attr("final_lr", trainer.optimizer.param_groups[0]['lr'])
            save_json(config, save_dir / "config.json")
            
            self.logger.info(f"Trial {trial.number} completed. Best loss: {best_val_loss:.6f}, "
                             f"Final LR: {trainer.optimizer.param_groups[0]['lr']:.2e}")
            
            return best_val_loss
            
        except optuna.TrialPruned:
            self.logger.info(f"Trial {trial.number} pruned.")
            raise
        except Exception as e:
            self.logger.error(f"Trial {trial.number} failed: {e}", exc_info=True)
            return float("inf")
        finally:
            if self.device.type == "cuda":
                torch.cuda.empty_cache()


class PrunableTrainer(Trainer):
    """Extended Trainer that supports epoch callbacks for Optuna pruning."""
    def __init__(self, *args, epoch_callback: Optional[Callable[[int, float], bool]] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_callback = epoch_callback
        
    def _run_training_loop(self):
        """Main training loop with pruning support."""
        best_train_loss = float("inf")
        
        for epoch in range(1, self.train_config["epochs"] + 1):
            self.current_epoch = epoch
            epoch_start = time.time()
            train_loss, train_metrics = self._train_epoch()
            val_loss, val_metrics = self._validate()

            if self.scheduler and not self.scheduler_step_on_batch:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) and self.has_validation:
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            epoch_time = time.time() - epoch_start
            self.total_training_time += epoch_time
            self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)

            if self.epoch_callback:
                loss_for_pruning = val_loss if self.has_validation and val_loss != float("inf") else train_loss
                
                if self.epoch_callback(epoch, loss_for_pruning):
                    self.logger.info(f"Trial pruned at epoch {epoch} with loss {loss_for_pruning:.6f}")
                    raise optuna.TrialPruned()

            if self.has_validation:
                if val_loss < (self.best_val_loss - self.min_delta):
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1

                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            else:
                if train_loss < (best_train_loss - self.min_delta):
                    best_train_loss = train_loss
                    self.best_val_loss = train_loss
                    self.best_epoch = epoch
                    self._save_best_model()


def suggest_model_config(trial: optuna.Trial, base_config: Dict[str, Any]) -> Dict[str, Any]:
    """Suggests a valid model and training configuration for a trial."""
    config = copy.deepcopy(base_config)

    config["prediction"]["mode"] = "absolute"
    
    # Model architecture choice
    model_type = trial.suggest_categorical("model_type", ["deeponet", "siren"])
    config["model"]["type"] = model_type
    
    # Common hyperparameters - EXPANDED SEARCH SPACE
    config["model"]["activation"] = trial.suggest_categorical("activation", ["gelu", "silu", "relu", "tanh"])
    #config["training"]["learning_rate"] = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    #config["training"]["batch_size"] = trial.suggest_categorical("batch_size", [16384])
    #config["training"]["weight_decay"] = trial.suggest_float("weight_decay", 1e-6, 1e-3, log=True)
    config["model"]["dropout"] = trial.suggest_float("dropout", 0.0, 0.1, step=0.05)
    

    if model_type == "deeponet":
        # More flexible architecture search
        n_branch = trial.suggest_int("n_branch_layers", 2, 6)
        branch_layers = []
        for i in range(n_branch):
            if i == 0:
                width = trial.suggest_categorical(f"branch_layer_{i}", [256, 384, 512, 1024])
            else:
                # Allow different widths per layer
                width = trial.suggest_categorical(f"branch_layer_{i}", [128, 256, 384, 512, 1024])
            branch_layers.append(width)
        config["model"]["branch_layers"] = branch_layers
        
        n_trunk = trial.suggest_int("n_trunk_layers", 2, 5)
        trunk_layers = []
        for i in range(n_trunk):
            width = trial.suggest_categorical(f"trunk_layer_{i}", [64, 128, 192])
            trunk_layers.append(width)
        config["model"]["trunk_layers"] = trunk_layers
        
        config["model"]["basis_dim"] = trial.suggest_categorical("basis_dim", [64, 128, 256])
        config["model"]["output_scale"] = trial.suggest_categorical("output_scale", [0.1, 1.0, 10.0])
        
    # Siren
    else: 
        n_layers = trial.suggest_int("n_hidden_layers", 3, 6)
        hidden_dims = []
        for i in range(n_layers):
            if i == 0:
                width = trial.suggest_categorical(f"hidden_dim_{i}", [256, 384, 512, 1024, 2048])
            else:
                width = trial.suggest_categorical(f"hidden_dim_{i}", [128, 256, 384, 1024, 2048])
            hidden_dims.append(width)
        config["model"]["hidden_dims"] = hidden_dims
        config["model"]["omega_0"] = trial.suggest_float("omega_0", 10.0, 50.0)

    # FiLM configuration
    use_film = trial.suggest_categorical("use_film", [True, False])
    config["film"]["enabled"] = use_film
    if use_film:
        film_layers = trial.suggest_int("film_layers", 1, 5)
        film_widths = []
        for i in range(film_layers):
            width = trial.suggest_categorical(f"film_width_{i}", [32, 64, 128])
            film_widths.append(width)
        config["film"]["hidden_dims"] = film_widths
        config["film"]["activation"] = trial.suggest_categorical("film_activation", ["gelu", "relu"])

    # Loss function
    config["training"]["loss"] = trial.suggest_categorical("loss", ["mse"])
    if config["training"]["loss"] == "huber":
        config["training"]["huber_delta"] = trial.suggest_float("huber_delta", 0.1, 2.0)

    # Gradient clipping
    config["training"]["gradient_clip"] = trial.suggest_categorical("gradient_clip", [1.0])

    return config


def optimize(config_path: Path, n_trials: int = 25, n_jobs: int = 1,
             study_name: str = "chemulator_hpo", pruner: Optional[optuna.pruners.BasePruner] = None):
    """
    Main function to run Optuna optimization with Hyperband for 40-hoaur runtime.
    """
    logger = logging.getLogger(__name__)
    base_config = load_json_config(config_path)
    
    # Create trial runner which will prepare data
    trial_runner = OptunaTrialRunner(config_path)
    objective = trial_runner.run_trial

    # Use Hyperband pruner for efficient resource allocation
    if pruner is None:
        min_resource = base_config["training"]["hpo_min_epochs"]
        max_resource = base_config["training"]["hpo_max_epochs"]
        
        pruner = HyperbandPruner(
            min_resource=min_resource,
            max_resource=max_resource,
            reduction_factor=3
        )
        
        logger.info(f"Using Hyperband pruner: min={min_resource} epochs, max={max_resource} epochs")

    study = optuna.create_study(
        direction="minimize",
        sampler=TPESampler(seed=42, n_startup_trials=5),
        pruner=pruner,
        study_name=study_name,
        storage=f"sqlite:///{study_name}.db",
        load_if_exists=True
    )

    # Log expected runtime
    logger.info(f"Starting optimization with target {n_trials} trials")
    logger.info(f"Expected runtime: ~40 hours with aggressive pruning")

    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)

    # Save Results
    results_dir = Path("optuna_results")
    ensure_directories(results_dir)
    timestamp = time.strftime("%Y%m%d_%H%M%S")

    best_config = study.best_trial.user_attrs.get("full_config", {})
    if not best_config:
        logger.warning("Could not retrieve full config from user_attrs.")

    # Compute statistics
    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
    
    epoch_distribution = {}
    for t in completed_trials + pruned_trials:
        n_epochs = t.user_attrs.get("n_epochs", "unknown")
        epoch_distribution[n_epochs] = epoch_distribution.get(n_epochs, 0) + 1

    best_results = {
        "best_value": study.best_value,
        "best_params": study.best_trial.params,
        "best_config": best_config,
        "n_trials_completed": len(completed_trials),
        "n_trials_pruned": len(pruned_trials),
        "epoch_distribution": epoch_distribution,
        "best_trial_final_lr": study.best_trial.user_attrs.get("final_lr", "unknown"),
        "study_db": f"{study_name}.db"
    }
    
    save_json(best_results, results_dir / f"best_config_{study_name}_{timestamp}.json")
    
    print("\n" + "="*60)
    print("OPTIMIZATION COMPLETE")
    print("="*60)
    print(f"Best validation loss: {best_results['best_value']:.6f}")
    print(f"Best trial final LR: {best_results['best_trial_final_lr']}")
    print(f"Trials: {best_results['n_trials_completed']} completed, {best_results['n_trials_pruned']} pruned")
    print("\nEpoch distribution:")
    for epochs, count in sorted(epoch_distribution.items()):
        print(f"  {epochs} epochs: {count} trials")
    print("\nBest parameters:")
    for key, value in best_results['best_params'].items():
        print(f"  {key}: {value}")
    
    return study

===== /Users/imalsky/Desktop/Chemulator/src/training/trainer.py =====
#!/usr/bin/env python3
"""
Optimized training pipeline for chemical kinetics models on A100 GPU.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
from contextlib import nullcontext
import math

import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau

from models.model import export_model
from data.normalizer import NormalizationHelper


class Trainer:
    """Optimized trainer for A100 GPU."""
    def __init__(self, model: nn.Module, train_dataset, val_dataset, test_dataset,
                config: Dict[str, Any], save_dir: Path, device: torch.device,
                norm_helper: NormalizationHelper):
        self.logger = logging.getLogger(__name__)
        
        self.model = model
        self.config = config
        self.save_dir = save_dir
        self.device = device
        self.norm_helper = norm_helper
        
        # Extract config sections
        self.train_config = config["training"]
        self.system_config = config["system"]
        self.prediction_config = config.get("prediction", {})
        
        # Prediction mode
        self.prediction_mode = self.prediction_config.get("mode", "absolute")
        self.output_clamp = self.prediction_config.get("output_clamp")
        
        self._validate_trainer_config()
        
        # Dataset info
        self.n_species = len(config["data"]["species_variables"])
        self.n_globals = len(config["data"]["global_variables"])
        
        # Check for validation data
        self.has_validation = val_dataset is not None and len(val_dataset) > 0
        if not self.has_validation:
            self.logger.warning("No validation data – using training loss for checkpointing")
        
        # Create data loaders
        self._setup_dataloaders(train_dataset, val_dataset, test_dataset)

        # Training state
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_loss = float("inf")
        self.best_epoch = -1
        self.total_training_time = 0
        self.patience_counter = 0
        
        # Training parameters
        self.log_interval = self.train_config.get("log_interval", 100)
        self.early_stopping_patience = self.train_config["early_stopping_patience"]
        self.min_delta = self.train_config["min_delta"]
        self.gradient_accumulation_steps = self.train_config["gradient_accumulation_steps"]
        
        # Setup training components
        self._setup_optimizer()
        self._setup_scheduler()
        self._setup_loss()
        self._setup_amp()
        
        # Training history
        self.training_history = {
            "config": config,
            "prediction_mode": self.prediction_mode,
            "epochs": []
        }


    def _validate_trainer_config(self):
        """Validate trainer configuration for correctness."""
        # Fix 2: Validate ratio mode requirements
        if self.prediction_mode == "ratio":
            # Check model compatibility
            model_type = self.config["model"]["type"]
            if model_type != "deeponet":
                raise ValueError(
                    f"Training in 'ratio' mode requires 'deeponet' model, "
                    f"but '{model_type}' was specified. Please use 'deeponet' or switch to 'absolute' mode."
                )
            
            # Fix 3: Check for ratio statistics
            if not hasattr(self.norm_helper, 'ratio_stats') or self.norm_helper.ratio_stats is None:
                raise ValueError(
                    "Training in 'ratio' mode requires ratio statistics from preprocessing. "
                    "Ensure data was preprocessed with prediction.mode='ratio' in config. "
                    "Current normalization data does not contain ratio_stats."
                )
            
            self.logger.info("Ratio mode validation passed: using DeepONet with ratio statistics")
        
        # Validate output clamping configuration
        if self.output_clamp is not None:
            if isinstance(self.output_clamp, (list, tuple)):
                if len(self.output_clamp) != 2:
                    raise ValueError("output_clamp must be None, a single value (min), or a tuple/list of (min, max)")
            elif not isinstance(self.output_clamp, (int, float)):
                raise ValueError("output_clamp must be None, a number, or a tuple/list of two numbers")

    def _setup_dataloaders(self, train_dataset, val_dataset, test_dataset):
        """Setup data loaders for GPU-cached data."""
        from data.dataset import create_dataloader
        
        self.train_loader = create_dataloader(
            train_dataset,
            self.config,
            shuffle=True,
            device=self.device,
            drop_last=True
        ) if train_dataset else None
        
        self.val_loader = create_dataloader(
            val_dataset,
            self.config,
            shuffle=False,
            device=self.device,
            drop_last=False
        ) if val_dataset and len(val_dataset) > 0 else None
        
        self.test_loader = create_dataloader(
            test_dataset,
            self.config,
            shuffle=False,
            device=self.device,
            drop_last=False
        ) if test_dataset and len(test_dataset) > 0 else None
    
    def _setup_optimizer(self):
        """Setup AdamW optimizer with safe feature detection."""
        # Separate parameters for weight decay
        decay_params = []
        no_decay_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            if param.dim() == 1 or "bias" in name or "norm" in name.lower():
                no_decay_params.append(param)
            else:
                decay_params.append(param)
        
        param_groups = [
            {"params": decay_params, "weight_decay": self.train_config["weight_decay"]},
            {"params": no_decay_params, "weight_decay": 0.0}
        ]
        
        # Safely check for fused optimizer support
        optimizer_kwargs = {
            "lr": self.train_config["learning_rate"],
            "betas": tuple(self.train_config.get("betas", [0.9, 0.999])),
            "eps": self.train_config.get("eps", 1e-8),
        }
        
        # Only use fused if available and on CUDA
        if self.device.type == "cuda" and hasattr(torch.optim.AdamW, "fused"):
            try:
                # Test if fused parameter actually works
                test_opt = torch.optim.AdamW([torch.zeros(1)], fused=True)
                optimizer_kwargs["fused"] = True
                self.logger.info("Using fused AdamW optimizer")
            except Exception:
                self.logger.info("Fused AdamW not available, using standard implementation")
        
        self.optimizer = AdamW(param_groups, **optimizer_kwargs)
    
    def _setup_scheduler(self):
        """Setup learning rate scheduler."""
        scheduler_type = self.train_config.get("scheduler", "none").lower()

        if scheduler_type == "none" or not self.train_loader:
            self.scheduler = None
            self.scheduler_step_on_batch = False
            return

        steps_per_epoch = len(self.train_loader) // self.gradient_accumulation_steps
        
        # **FIXED**: Guard against division-by-zero or T_0=0 errors on small datasets.
        if steps_per_epoch == 0:
            self.logger.warning(
                f"Number of batches ({len(self.train_loader)}) is smaller than "
                f"gradient_accumulation_steps ({self.gradient_accumulation_steps}). "
                f"Scheduler will step once per epoch."
            )
            steps_per_epoch = 1

        params = self.train_config.get("scheduler_params", {})

        if scheduler_type == "cosine":
            T_0_epochs = params.get("T_0", 10)
            T_0_steps = T_0_epochs * steps_per_epoch

            self.scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=T_0_steps,
                T_mult=params.get("T_mult", 2),
                eta_min=params.get("eta_min", 1e-8),
            )
            self.scheduler_step_on_batch = True

        elif scheduler_type == "plateau":
            if not self.has_validation:
                self.logger.warning("Plateau scheduler requires validation data")
                self.scheduler = None
                self.scheduler_step_on_batch = False
                return
                
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                factor=params.get("factor", 0.5),
                patience=params.get("patience", 10),
                min_lr=params.get("min_lr", 1e-7),
            )
            self.scheduler_step_on_batch = False
        else:
            raise ValueError(f"Unknown scheduler '{scheduler_type}'")
    
    def _setup_loss(self):
        """Setup loss function."""
        loss_type = self.train_config["loss"]
        
        if loss_type == "mse":
            self.criterion = nn.MSELoss()
        elif loss_type == "mae":
            self.criterion = nn.L1Loss()
        elif loss_type == "huber":
            self.criterion = nn.HuberLoss(delta=self.train_config.get("huber_delta", 0.5))
        else:
            raise ValueError(f"Unknown loss: {loss_type}")
    
    def _setup_amp(self):
        """Setup automatic mixed precision for A100."""
        self.use_amp = self.train_config.get("use_amp", True)
        
        # Get dtype
        dtype_str = str(self.train_config.get("amp_dtype", "bfloat16")).lower()
        self.amp_dtype = torch.bfloat16 if dtype_str == "bfloat16" else torch.float16
        
        # GradScaler only for float16
        self.scaler = GradScaler(enabled=(self.amp_dtype == torch.float16))
    
    def _compute_loss(self, outputs: torch.Tensor, 
                    targets: torch.Tensor,
                    inputs: torch.Tensor) -> torch.Tensor:
        """
        Compute loss with proper output clamping.
        
        output_clamp can be:
        - None: no clamping
        - Single value: clamp minimum only (backward compatibility)
        - Tuple/list of (min, max): clamp both sides
        """
        if self.output_clamp is not None and self.prediction_mode == "absolute":
            if isinstance(self.output_clamp, (list, tuple)):
                # Two-sided clamp
                outputs = torch.clamp(outputs, min=self.output_clamp[0], max=self.output_clamp[1])
            else:
                # Single value - clamp minimum only (backward compatibility)
                outputs = torch.clamp(outputs, min=self.output_clamp)
                self.logger.warning(
                    "Using single-sided output clamping (min only). "
                    "Consider using (min, max) tuple for two-sided clamping."
                )
        
        return self.criterion(outputs, targets)



    def train(self) -> float:
        """Execute the training loop."""
        if not self.train_loader:
            self.logger.error("Training loader not available")
            return float("inf")

        self.logger.info(f"Starting training...")
        self.logger.info(f"Train batches: {len(self.train_loader)}")
        if self.has_validation:
            self.logger.info(f"Val batches: {len(self.val_loader)}")


        if self.system_config.get("use_torch_compile", False):
            self.logger.info("Compiling model with torch.compile...")
            self.logger.warning("    This is a one-time process that can take several minutes.")


        try:
            self._run_training_loop()
            
            self.logger.info(
                f"Training completed. Best validation loss: {self.best_val_loss:.6f} "
                f"at epoch {self.best_epoch}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}", exc_info=True)
            raise
            
        finally:
            # Save training history
            save_path = self.save_dir / "training_log.json"
            with open(save_path, 'w') as f:
                json.dump(self.training_history, f, indent=2)
        
        return self.best_val_loss
    
    def _run_training_loop(self):
        """Main training loop - optimized for GPU."""
        best_train_loss = float("inf")
        
        for epoch in range(1, self.train_config["epochs"] + 1):
            self.current_epoch = epoch
            epoch_start = time.time()

            # Train
            train_loss, train_metrics = self._train_epoch()
            
            # Validate
            val_loss, val_metrics = self._validate()

            # Update scheduler
            if self.scheduler and not self.scheduler_step_on_batch:
                if isinstance(self.scheduler, ReduceLROnPlateau) and self.has_validation:
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            # Log epoch
            epoch_time = time.time() - epoch_start
            self.total_training_time += epoch_time
            self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)

            # Save best model
            if self.has_validation:
                if val_loss < (self.best_val_loss - self.min_delta):
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1

                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            else:
                # Use training loss if no validation
                if train_loss < (best_train_loss - self.min_delta):
                    best_train_loss = train_loss
                    self.best_val_loss = train_loss
                    self.best_epoch = epoch
                    self._save_best_model()


    def _train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """Optimized training epoch for GPU."""
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        
        accumulation_steps = self.gradient_accumulation_steps
        
        is_gpu_cached = hasattr(self.train_loader.dataset, 'gpu_cache') and self.train_loader.dataset.gpu_cache is not None
        
        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            if not is_gpu_cached:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)
                loss = loss / accumulation_steps
            
            if self.scaler.is_enabled():
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(self.train_loader):
                if self.train_config["gradient_clip"] > 0:
                    if self.scaler.is_enabled():
                        self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.train_config["gradient_clip"]
                    )
                
                if self.scaler.is_enabled():
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.optimizer.zero_grad(set_to_none=True)
                
                if self.scheduler and self.scheduler_step_on_batch:
                    self.scheduler.step()
                
                self.global_step += 1
            
            batch_size = inputs.size(0)
            total_loss += loss.item() * accumulation_steps * batch_size
            total_samples += batch_size
        
        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
        return avg_loss, {}

    @torch.inference_mode()
    def _validate(self) -> Tuple[float, Dict[str, float]]:
        """Optimized validation for GPU."""
        if not self.has_validation or self.val_loader is None:
            return float("inf"), {}
        
        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        
        is_gpu_cached = hasattr(self.val_loader.dataset, 'gpu_cache') and self.val_loader.dataset.gpu_cache is not None
        
        for inputs, targets in self.val_loader:
            if not is_gpu_cached:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
            
            # **FIXED**: device_type is now dynamic, based on the detected hardware.
            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)
            
            batch_size = inputs.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size
        
        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        return avg_loss, {}

    @torch.inference_mode()
    def evaluate_test(self) -> float:
        """Evaluate on test set."""
        if not self.test_loader:
            self.logger.warning("No test data available")
            return float("inf")

        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        
        is_gpu_cached = hasattr(self.test_loader.dataset, 'gpu_cache') and self.test_loader.dataset.gpu_cache is not None

        for inputs, targets in self.test_loader:
            if not is_gpu_cached:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

            # **FIXED**: device_type is now dynamic, based on the detected hardware.
            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)

            batch_size = inputs.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size

        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        self.logger.info(f"Test loss: {avg_loss:.6f}")
        return avg_loss

    def _log_epoch(self, train_loss, val_loss, train_metrics, val_metrics, epoch_time):
        """Log epoch results."""
        lr = self.optimizer.param_groups[0]['lr']
        log_entry = {
            "epoch": self.current_epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "epoch_time": epoch_time,
            "lr": lr,
        }
        self.training_history["epochs"].append(log_entry)
        
        val_str = f"Val loss: {val_loss:.3e}" if self.has_validation else "Val loss: N/A"
        self.logger.info(
            f"Epoch {self.current_epoch}/{self.train_config['epochs']} "
            f"Train loss: {train_loss:.3e} {val_str} "
            f"Time: {epoch_time:.1f}s LR: {lr:.2e}"
        )
    
    def _save_best_model(self):
        """Save the best model checkpoint."""
        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict() if self.scheduler else None,
            "best_val_loss": self.best_val_loss,
            "config": self.config
        }
        
        checkpoint_path = self.save_dir / "best_model.pt"
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"Saved best model checkpoint to {checkpoint_path}")

        # Export model if enabled
        if self.system_config.get("use_torch_export", False):
            example_loader = self.val_loader or self.train_loader
            if example_loader:
                example_inputs, _ = next(iter(example_loader))
                self.logger.info(f"Exporting model with example input shape: {example_inputs.shape}")
                export_path = self.save_dir / "exported_model.pt"
                export_model(self.model, example_inputs, export_path)


def diagnose_performance(trainer, num_batches=20):
    """Diagnose training performance bottlenecks."""
    import torch.profiler
    
    logger = logging.getLogger(__name__)
    logger.info("Running performance diagnostics...")
    
    trainer.model.train()
    times = {
        'forward': [],
        'backward': [],
        'optimizer': [],
        'total': []
    }
    
    # Warmup
    for i, (inputs, targets) in enumerate(trainer.train_loader):
        if i >= 3:
            break
        outputs = trainer.model(inputs)
        loss = trainer._compute_loss(outputs, targets, inputs)
        loss.backward()
        trainer.optimizer.step()
        trainer.optimizer.zero_grad()
    
    torch.cuda.synchronize()
    
    # Actual timing
    for i, (inputs, targets) in enumerate(trainer.train_loader):
        if i >= num_batches:
            break
        
        torch.cuda.synchronize()
        batch_start = torch.cuda.Event(enable_timing=True)
        forward_end = torch.cuda.Event(enable_timing=True)
        backward_end = torch.cuda.Event(enable_timing=True)
        optimizer_end = torch.cuda.Event(enable_timing=True)
        
        batch_start.record()
        
        # Forward
        outputs = trainer.model(inputs)
        loss = trainer._compute_loss(outputs, targets, inputs)
        forward_end.record()
        
        # Backward
        loss.backward()
        backward_end.record()
        
        # Optimizer
        trainer.optimizer.step()
        trainer.optimizer.zero_grad()
        optimizer_end.record()
        
        torch.cuda.synchronize()
        
        # Record times
        times['forward'].append(batch_start.elapsed_time(forward_end))
        times['backward'].append(forward_end.elapsed_time(backward_end))
        times['optimizer'].append(backward_end.elapsed_time(optimizer_end))
        times['total'].append(batch_start.elapsed_time(optimizer_end))
    
    # Report results
    for key, values in times.items():
        avg_time = sum(values) / len(values)
        logger.info(f"{key}: {avg_time:.2f} ms/batch")
    
    # Calculate throughput
    batch_size = trainer.train_config['batch_size']
    avg_total = sum(times['total']) / len(times['total']) / 1000  # Convert to seconds
    throughput = batch_size / avg_total
    logger.info(f"Throughput: {throughput:.0f} samples/second")
    
    return times

===== /Users/imalsky/Desktop/Chemulator/src/utils/utils.py =====
#!/usr/bin/env python3
"""
Simplified utility functions for the chemical kinetics pipeline.
"""

import json
import logging
import os
import random
import sys
from pathlib import Path
from typing import Any, Dict, Union

import numpy as np
import torch


def setup_logging(level: int = logging.INFO, log_file: Path = None) -> None:
    """Configure logging for the application."""
    format_string = "%(asctime)s | %(levelname)-8s | %(name)s - %(message)s"
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    # Clear existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create formatter
    formatter = logging.Formatter(format_string, datefmt="%Y-%m-%d %H:%M:%S")
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler
    if log_file is not None:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)


def seed_everything(seed: int) -> None:
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    os.environ["PYTHONHASHSEED"] = str(seed)
    
    logger = logging.getLogger(__name__)
    logger.info(f"Random seed set to {seed}")


def load_json_config(path: Union[str, Path]) -> Dict[str, Any]:
    """Load JSON configuration file."""
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"Configuration file not found: {path}")
    
    # Try json5 first for comment support
    try:
        import json5
        with open(path, 'r', encoding='utf-8') as f:
            config = json5.load(f)
    except ImportError:
        # Fallback to standard json
        with open(path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    return config


def save_json(data: Dict[str, Any], path: Union[str, Path], indent: int = 2) -> None:
    """Save dictionary to JSON file."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Custom encoder for numpy/torch types
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.Tensor):
                return obj.cpu().numpy().tolist()
            elif isinstance(obj, Path):
                return str(obj)
            return super().default(obj)
    
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, cls=NumpyEncoder)


def load_json(path: Union[str, Path]) -> Dict[str, Any]:
    """Load JSON file."""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def ensure_directories(*paths: Union[str, Path]) -> None:
    """Create directories if they don't exist."""
    for path in paths:
        Path(path).mkdir(parents=True, exist_ok=True)

===== /Users/imalsky/Desktop/Chemulator/src/utils/hardware.py =====
#!/usr/bin/env python3
"""
Hardware detection and optimization utilities.
"""

import logging
import os
from typing import Dict, Any

import torch


def setup_device() -> torch.device:
    """Detect and configure the best available compute device."""
    logger = logging.getLogger(__name__)
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"Using CUDA device: {gpu_name} ({gpu_memory:.1f} GB)")
        
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("Using Apple Silicon MPS device")
        
    else:
        device = torch.device("cpu")
        logger.info(f"Using CPU device ({os.cpu_count()} cores)")
    
    return device


def optimize_hardware(config: Dict[str, Any], device: torch.device) -> None:
    """Apply hardware-specific optimizations with safe feature detection."""
    logger = logging.getLogger(__name__)
    
    # CUDA optimizations
    if device.type == "cuda":
        # Enable TensorFloat-32 for faster matmul
        if config.get("tf32", True):
            if hasattr(torch.backends.cuda, "matmul"):
                torch.backends.cuda.matmul.allow_tf32 = True
                logger.info("TensorFloat-32 enabled for matmul")
            if hasattr(torch.backends.cudnn, "allow_tf32"):
                torch.backends.cudnn.allow_tf32 = True
                logger.info("TensorFloat-32 enabled for cuDNN")
        
        # Enable cuDNN autotuner
        if config.get("cudnn_benchmark", True) and hasattr(torch.backends.cudnn, "benchmark"):
            torch.backends.cudnn.benchmark = True
            logger.info("cuDNN autotuner enabled")
        
        # Set memory fraction - safely check if API exists
        memory_fraction = config.get("cuda_memory_fraction", 0.9)
        if memory_fraction < 1.0 and hasattr(torch.cuda, "set_per_process_memory_fraction"):
            try:
                torch.cuda.set_per_process_memory_fraction(memory_fraction)
                logger.info(f"CUDA memory fraction set to {memory_fraction}")
            except Exception as e:
                logger.warning(f"Could not set CUDA memory fraction: {e}")
    
    # Set number of threads for CPU operations
    torch.set_num_threads(min(32, os.cpu_count() or 1))
    logger.info(f"Using {torch.get_num_threads()} CPU threads")

===== /Users/imalsky/Desktop/Chemulator/src/models/model.py =====
#!/usr/bin/env python3
"""
Optimized model definitions for chemical kinetics neural networks.
Includes better compilation and A100-specific optimizations.
"""

import logging
import math
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

import torch
import torch.nn as nn
from torch.export import Dim


class FiLMLayer(nn.Module):
    """Feature-wise Linear Modulation layer - optimized version."""
    
    def __init__(self, condition_dim: int, feature_dim: int, 
                 hidden_dims: List[int], activation: Union[str, List[str]] = "gelu", 
                 use_beta: bool = True):
        super().__init__()
        
        self.use_beta = use_beta
        self.feature_dim = feature_dim
        out_multiplier = 2 if use_beta else 1
        
        # Handle activation specification
        if isinstance(activation, str):
            activations = [activation] * len(hidden_dims)
        else:
            if len(activation) != len(hidden_dims):
                raise ValueError(f"Number of activations ({len(activation)}) must match hidden_dims ({len(hidden_dims)})")
            activations = activation
        
        # Build FiLM MLP
        layers = []
        prev_dim = condition_dim
        
        # Hidden layers with per-layer activation
        for dim, act in zip(hidden_dims, activations):
            layers.extend([
                nn.Linear(prev_dim, dim),
                self._get_activation(act)
            ])
            prev_dim = dim
        
        # Output layer
        layers.append(nn.Linear(prev_dim, out_multiplier * feature_dim))
        
        self.film_net = nn.Sequential(*layers)
        
        # Initialize to identity mapping
        self._initialize_identity()
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(inplace=True),  # Inplace for memory efficiency
            "tanh": nn.Tanh(),
            "silu": nn.SiLU(inplace=True),
            "leakyrelu": nn.LeakyReLU(0.2, inplace=True),
            "elu": nn.ELU(inplace=True)
        }
        if name.lower() not in activations:
            raise ValueError(f"Unknown activation: {name}")
        return activations[name.lower()]
    
    def _initialize_identity(self):
        """Initialize FiLM to identity mapping."""
        with torch.no_grad():
            final_layer = self.film_net[-1]
            # Small weights
            final_layer.weight.data.normal_(0, 0.02)
            # Set bias for gamma=1, beta=0
            if self.use_beta:
                final_layer.bias.data[:self.feature_dim] = 1.0
                final_layer.bias.data[self.feature_dim:] = 0.0
            else:
                final_layer.bias.data.fill_(1.0)
    
    def forward(self, features: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """Apply FiLM modulation - optimized."""
        # Generate parameters
        params = self.film_net(condition)
        
        if self.use_beta:
            gamma = params[:, :self.feature_dim]
            beta = params[:, self.feature_dim:]
        else:
            gamma = params
            beta = 0
        
        # Handle different feature dimensions efficiently
        if features.dim() == 2:
            # Simple 2D case
            return features * gamma + beta
        else:
            # Reshape for broadcasting
            shape = [gamma.size(0)] + [1] * (features.dim() - 2) + [self.feature_dim]
            gamma = gamma.view(*shape)
            if self.use_beta:
                beta = beta.view(*shape)
            return features * gamma + beta


class FiLMSIREN(nn.Module):
    """SIREN with FiLM conditioning - optimized for A100."""
    def __init__(self, config: Dict[str, Any]):
        super().__init__()

        # Extract dimensions
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]

        # SIREN parameters
        self.omega_0 = config["model"].get("omega_0", 30.0)
        
        # Dropout
        dropout_rate = config["model"].get("dropout", 0.0)
        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None

        # FiLM configuration
        film_config = config.get("film", {})
        self.use_film = film_config.get("enabled", True)

        # Input dimension
        input_dim = self.num_species + self.num_globals + 1

        # Build network layers
        self.layers = nn.ModuleList()
        self.film_layers = nn.ModuleList() if self.use_film else None

        prev_dim = input_dim
        condition_dim = self.num_species + self.num_globals

        # Get FiLM activations
        film_activations = film_config.get("activations", film_config.get("activation", "gelu"))
        if isinstance(film_activations, str):
            film_activations = [film_activations] * len(self.hidden_dims)

        for i, dim in enumerate(self.hidden_dims):
            # Main layer
            self.layers.append(nn.Linear(prev_dim, dim))

            # FiLM layer
            if self.use_film:
                layer_activation = film_activations[i] if isinstance(film_activations, list) else film_activations
                self.film_layers.append(
                    FiLMLayer(
                        condition_dim=condition_dim,
                        feature_dim=dim,
                        hidden_dims=film_config.get("hidden_dims", [128, 128]),
                        activation=layer_activation,
                        use_beta=True
                    )
                )

            prev_dim = dim

        # Output layer
        self.output_layer = nn.Linear(prev_dim, self.num_species)

        # Initialize SIREN weights
        self._initialize_siren_weights()
    
    def _initialize_siren_weights(self):
        """Initialize weights following SIREN paper."""
        with torch.no_grad():
            # First layer
            if len(self.layers) > 0:
                fan_in = self.layers[0].in_features
                self.layers[0].weight.uniform_(-1.0 / fan_in, 1.0 / fan_in)
            
            # Hidden layers
            for layer in self.layers[1:]:
                fan_in = layer.in_features
                bound = math.sqrt(6.0 / fan_in) / self.omega_0
                layer.weight.uniform_(-bound, bound)
            
            # Output layer
            fan_in = self.output_layer.in_features
            bound = math.sqrt(6.0 / fan_in) / self.omega_0
            self.output_layer.weight.uniform_(-bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass - optimized."""
        # Extract initial conditions for FiLM
        initial_conditions = x[:, :-1]  # All but time
        
        # Process through layers
        h = x
        for i, layer in enumerate(self.layers):
            # Linear transformation
            h = layer(h)
            
            # Apply FiLM before activation
            if self.use_film and self.film_layers is not None:
                h = self.film_layers[i](h, initial_conditions)
            
            # SIREN activation
            h = torch.sin(self.omega_0 * h)
            
            # Dropout (except last layer)
            if self.dropout is not None and i < len(self.layers) - 1:
                h = self.dropout(h)
        
        # Output
        return self.output_layer(h)


class FiLMDeepONet(nn.Module):
    """Deep Operator Network with FiLM conditioning - optimized."""
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        # Extract dimensions
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])

        # Architecture parameters
        branch_layers = config["model"]["branch_layers"]
        trunk_layers = config["model"]["trunk_layers"]
        self.basis_dim = config["model"]["basis_dim"]
        self.activation = self._get_activation(config["model"].get("activation", "gelu"))
        self.output_scale = config["model"].get("output_scale", 1.0)
        
        # Dropout
        dropout_rate = config["model"].get("dropout", 0.0)
        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None
        
        # FiLM configuration
        film_config = config.get("film", {})
        self.use_film = film_config.get("enabled", True)
        
        # Build networks
        self.branch_net = self._build_mlp_with_film(
            input_dim=self.num_species + self.num_globals,
            hidden_layers=branch_layers,
            output_dim=self.basis_dim * self.num_species,
            condition_dim=self.num_species + self.num_globals if self.use_film else None,
            film_config=film_config if self.use_film else None
        )
        
        self.trunk_net = self._build_mlp_with_film(
            input_dim=1,
            hidden_layers=trunk_layers,
            output_dim=self.basis_dim,
            condition_dim=self.num_species + self.num_globals if self.use_film else None,
            film_config=film_config if self.use_film else None,
            bias=True
        )
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(inplace=True),
            "tanh": nn.Tanh(),
            "silu": nn.SiLU(inplace=True),
            "leakyrelu": nn.LeakyReLU(0.2, inplace=True),
            "elu": nn.ELU(inplace=True)
        }
        return activations.get(name.lower(), nn.GELU())
    
    def _build_mlp_with_film(self, input_dim: int, hidden_layers: List[int],
                            output_dim: int, condition_dim: Optional[int] = None,
                            film_config: Optional[Dict] = None, bias: bool = True) -> nn.Module:
        """Build MLP with optional FiLM - optimized."""
        if self.use_film and condition_dim is not None and film_config is not None:
            # Build with FiLM
            layers = nn.ModuleList()
            film_layers = nn.ModuleList()

            # Get FiLM activations
            film_activations = film_config.get("activations", film_config.get("activation", "gelu"))
            if isinstance(film_activations, str):
                film_activations = [film_activations] * len(hidden_layers)

            prev_dim = input_dim
            for i, dim in enumerate(hidden_layers):
                layers.append(nn.Linear(prev_dim, dim, bias=bias))

                layer_activation = film_activations[i] if isinstance(film_activations, list) else film_activations
                film_layers.append(
                    FiLMLayer(
                        condition_dim=condition_dim,
                        feature_dim=dim,
                        hidden_dims=film_config.get("hidden_dims", [128, 128]),
                        activation=layer_activation,
                        use_beta=True
                    )
                )
                prev_dim = dim

            output_layer = nn.Linear(prev_dim, output_dim, bias=bias)

            class MLPWithFiLM(nn.Module):
                def __init__(self, layers, film_layers, output_layer, activation, dropout):
                    super().__init__()
                    self.layers = layers
                    self.film_layers = film_layers
                    self.output_layer = output_layer
                    self.activation = activation
                    self.dropout = dropout

                def forward(self, x, condition):
                    h = x
                    for i, (layer, film_layer) in enumerate(zip(self.layers, self.film_layers)):
                        h = layer(h)
                        h = film_layer(h, condition)
                        h = self.activation(h)
                        
                        # Dropout (except last layer)
                        if self.dropout is not None and i < len(self.layers) - 1:
                            h = self.dropout(h)
                            
                    return self.output_layer(h)

            return MLPWithFiLM(layers, film_layers, output_layer, self.activation, self.dropout)

        else:
            # Build standard MLP
            layers = []
            prev_dim = input_dim

            for i, dim in enumerate(hidden_layers):
                layers.append(nn.Linear(prev_dim, dim, bias=bias))
                layers.append(self.activation)
                
                # Dropout (except last layer)
                if self.dropout is not None and i < len(hidden_layers) - 1:
                    layers.append(self.dropout)
                    
                prev_dim = dim

            layers.append(nn.Linear(prev_dim, output_dim, bias=bias))
            return nn.Sequential(*layers)
            
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """Forward pass - optimized."""
        batch_size = inputs.shape[0]

        # Split inputs
        branch_input = inputs[:, :self.num_species + self.num_globals]
        trunk_input = inputs[:, -1:]  # Time

        # Process through networks
        if self.use_film:
            branch_out = self.branch_net(branch_input, branch_input)
            trunk_out = self.trunk_net(trunk_input, branch_input)
        else:
            branch_out = self.branch_net(branch_input)
            trunk_out = self.trunk_net(trunk_input)

        # Reshape and combine efficiently
        branch_out = branch_out.view(batch_size, self.num_species, self.basis_dim)
        
        # Efficient matrix multiplication
        output = torch.bmm(branch_out, trunk_out.unsqueeze(2)).squeeze(2)

        # Optional output scaling
        if self.output_scale != 1.0:
            output = output * self.output_scale

        return output


def create_model(config: Dict[str, Any], device: torch.device) -> nn.Module:
    """Create and compile model with validation."""
    model_type = config["model"]["type"].lower()
    prediction_mode = config.get("prediction", {}).get("mode", "absolute")
    
    # Validate model-mode compatibility
    if prediction_mode == "ratio" and model_type != "deeponet":
        raise ValueError(
            f"Prediction mode 'ratio' is only compatible with model type 'deeponet', "
            f"but '{model_type}' was specified. Either:\n"
            f"  1. Change model.type to 'deeponet' in your config, or\n"
            f"  2. Change prediction.mode to 'absolute'"
        )
    
    if model_type == "siren":
        model = FiLMSIREN(config)
    elif model_type == "deeponet":
        model = FiLMDeepONet(config)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    model = model.to(device)
    
    logger = logging.getLogger(__name__)
    logger.info(f"Created {model_type} model for {prediction_mode} mode")
    
    # Compile model for performance
    if config["system"].get("use_torch_compile", False) and hasattr(torch, 'compile'):
        compile_mode = config["system"].get("compile_mode", "default")
        logger.info(f"Compiling model with mode='{compile_mode}'...")
        
        try:
            # A100-optimized compilation
            compile_options = {
                "mode": compile_mode,
                "fullgraph": False,
                "dynamic": False,
            }
            
            if compile_mode == "max-autotune":
                # Additional options for maximum performance
                compile_options["options"] = {
                    "triton.cudagraphs": True,
                    "triton.max_autotune": True,
                }
            
            model = torch.compile(model, **compile_options)
            logger.info("Model compilation successful")
            
        except Exception as e:
            logger.warning(f"Model compilation failed: {e}. Running in eager mode.")
    
    return model


def export_model(model: nn.Module, example_input: torch.Tensor, save_path: Path):
    """Export model with robust unwrapping and dynamic batch support."""
    logger = logging.getLogger(__name__)
    
    model.eval()
    
    # Safely handle compiled models with multiple fallback attempts
    original_model = model
    if hasattr(model, '_orig_mod'):
        logger.info("Extracting original model from compiled wrapper (_orig_mod)")
        model = model._orig_mod
    elif hasattr(model, '_module'):
        logger.info("Extracting original model from compiled wrapper (_module)")
        model = model._module
    elif hasattr(model, 'module'):
        logger.info("Extracting original model from DataParallel/DistributedDataParallel wrapper")
        model = model.module
    else:
        # Try to detect if it's a compiled model by checking for graph attributes
        if hasattr(model, '_graph') or hasattr(model, '_code'):
            logger.warning(
                "Model appears to be compiled but cannot find unwrapping attribute. "
                "Export may fail or produce suboptimal results."
            )
    
    with torch.no_grad():
        try:
            if hasattr(torch, 'export') and hasattr(torch.export, 'export'):
                # Dynamic batch dimension
                batch_dim = Dim("batch", min=1, max=131072)
                
                # Safely detect parameter name
                import inspect
                try:
                    sig = inspect.signature(model.forward)
                    param_names = [p for p in sig.parameters.keys() if p != 'self']
                    param_name = param_names[0] if param_names else 'x'
                except Exception:
                    # Fallback if signature inspection fails
                    param_name = 'x' if hasattr(model, 'forward') else 'input'
                    logger.warning(f"Could not inspect forward signature, using '{param_name}' as parameter name")
                
                logger.info(f"Detected forward method parameter name: '{param_name}'")
                
                dynamic_shapes = {param_name: {0: batch_dim}}
                
                # Export with error handling
                try:
                    exported_program = torch.export.export(
                        model, 
                        (example_input,),
                        dynamic_shapes=dynamic_shapes
                    )
                    torch.export.save(exported_program, str(save_path))
                    logger.info(f"Model exported with torch.export to {save_path}")
                except Exception as e:
                    logger.warning(f"torch.export failed: {e}. Falling back to torch.jit")
                    raise  # Re-raise to trigger JIT fallback
            else:
                # Direct to JIT if torch.export not available
                raise AttributeError("torch.export not available")
                
        except Exception:
            # Fallback to JIT tracing
            try:
                # Try with the original model if unwrapping failed
                model_to_trace = model
                traced_model = torch.jit.trace(model_to_trace, example_input)
                torch.jit.save(traced_model, str(save_path))
                logger.info(f"Model exported with torch.jit to {save_path}")
                logger.warning("JIT export may not support dynamic batch sizes as well as torch.export")
            except Exception as jit_error:
                # Last resort: try with the original compiled model
                if model is not original_model:
                    logger.warning("Trying JIT export with original (possibly compiled) model")
                    try:
                        traced_model = torch.jit.trace(original_model, example_input)
                        torch.jit.save(traced_model, str(save_path))
                        logger.info(f"Model exported with torch.jit (compiled version) to {save_path}")
                    except Exception as final_error:
                        logger.error(f"All export methods failed. Last error: {final_error}")
                        raise
                else:
                    logger.error(f"JIT export failed: {jit_error}")
                    raise

===== /Users/imalsky/Desktop/Chemulator/src/data/preprocessor.py =====
#!/usr/bin/env python3
"""
Chemical kinetics data preprocessor.
This version uses a highly efficient, parallelized, two-pass process with an
architecture that minimizes inter-process communication (IPC) and memory overhead.
"""

import hashlib
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

import h5py
import numpy as np
import torch
import os

from .normalizer import DataNormalizer, NormalizationHelper
from utils.utils import save_json, load_json

DEFAULT_EPSILON_MIN = 1e-38
DEFAULT_EPSILON_MAX = 1e38

class CorePreprocessor:
    """A lightweight helper class containing only the logic needed within a worker."""
    def __init__(self, config: Dict[str, Any], norm_stats: Optional[Dict[str, Any]] = None):
        self.data_cfg = config["data"]
        self.norm_cfg = config["normalization"]
        self.train_cfg = config["training"]
        self.pred_cfg = config.get("prediction", {})
        self.proc_cfg = config["preprocessing"]

        self.species_vars = self.data_cfg["species_variables"]
        self.global_vars = self.data_cfg["global_variables"]
        self.time_var = self.data_cfg["time_variable"]
        self.var_order = self.species_vars + self.global_vars + [self.time_var]
        
        self.n_species = len(self.species_vars)
        self.n_globals = len(self.global_vars)
        self.n_vars = self.n_species + self.n_globals + 1
        
        self.min_value_threshold = self.proc_cfg.get("min_value_threshold", 1e-30)
        
        self.prediction_mode = self.pred_cfg.get("mode", "absolute")
        self.normalizer = DataNormalizer(config)
        self.norm_stats = norm_stats or {}
        
        # Create index mappings for robust variable ordering
        self._create_index_mappings()
        
        if norm_stats:
            self.norm_helper = NormalizationHelper(
                norm_stats, torch.device("cpu"), self.species_vars,
                self.global_vars, self.time_var, config
            )
    
    def _create_index_mappings(self):
        """Create index mappings for robust variable access"""
        self.var_to_idx = {var: i for i, var in enumerate(self.var_order)}
        self.species_indices = [self.var_to_idx[var] for var in self.species_vars]
        self.global_indices = [self.var_to_idx[var] for var in self.global_vars]
        self.time_idx = self.var_to_idx[self.time_var]

    def _is_profile_valid(self, group: h5py.Group) -> Tuple[bool, str]:
        """
        Checks if a profile is valid according to strict criteria.
        Returns (is_valid, reason_for_failure_or_success).
        """
        # Validate every variable (species + globals + time)
        required_keys = self.species_vars + [self.time_var]
        if not set(required_keys).issubset(group.keys()):
            return False, "missing_keys"

        # Check each dataset for NaNs, Infs, and value thresholds
        for var in required_keys:
            try:
                data = group[var][:]
            except Exception:
                return False, "read_error"

            if not np.all(np.isfinite(data)):
                return False, "non_finite"

            # Drop profile if any value is less than or equal to threshold
            if np.any(data <= self.min_value_threshold):
                return False, "below_threshold"

        return True, "valid"
    
    def process_file_for_stats(
        self, file_path: Path
    ) -> Tuple[Dict[str, Dict], Dict[str, Dict], int, Dict]:
        accumulators = self.normalizer._initialize_accumulators()

        ratio_accumulators: Dict[str, Dict[str, Any]] = {}
        if self.prediction_mode == "ratio":
            for v in self.species_vars:
                raw_method = self.normalizer._get_method(v)
                ratio_method = raw_method[4:] if raw_method.startswith("log-") else raw_method
                ratio_accumulators[v] = {
                    "method": ratio_method,
                    "count": 0,
                    "mean": 0.0,
                    "m2":   0.0,
                    "min":  float("inf"),
                    "max":  float("-inf"),
                }

        valid_sample_count = 0
        report = {
            "total_profiles":   0,
            "profiles_kept":    0,
            "dropped_reasons":  defaultdict(int),
        }

        with h5py.File(file_path, "r") as f:
            for gname in sorted(f.keys()):
                report["total_profiles"] += 1
                grp = f[gname]

                # dataset‑level validation
                is_ok, reason = self._is_profile_valid(grp)
                if not is_ok:
                    report["dropped_reasons"][reason] += 1
                    continue

                # deterministic down‑sampling
                if self.train_cfg["use_fraction"] < 1.0:
                    h = int(hashlib.sha256(gname.encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
                    if h >= self.train_cfg["use_fraction"]:
                        continue

                n_t = grp[self.time_var].shape[0]
                if n_t <= 1:
                    report["dropped_reasons"]["too_few_timesteps"] += 1
                    continue

                # assemble full profile (species + globals + time)
                profile = self._extract_profile(grp, gname, n_t)
                if profile is None:
                    report["dropped_reasons"]["extract_profile_failed"] += 1
                    continue

                # final profile‑level check
                if (np.any(~np.isfinite(profile)) or
                    np.any(profile <= self.min_value_threshold)):
                    report["dropped_reasons"]["below_threshold"] += 1
                    continue

                # update statistics
                report["profiles_kept"] += 1
                valid_sample_count += (n_t - 1)
                self._update_stats_for_profile(
                    profile, n_t,
                    accumulators,
                    ratio_accumulators,
                )

        return accumulators, ratio_accumulators, valid_sample_count, report

    def process_file_for_shards(self, file_path: Path, output_dir: Path) -> Dict[str, Any]:
        """Process file and write to split-specific shard directories."""
        # Create separate shard writers for each split
        shard_writers = {
            "train": ShardWriter(
                output_dir / "train", 
                self.proc_cfg["shard_size"], 
                file_path.stem
            ),
            "validation": ShardWriter(
                output_dir / "validation",
                self.proc_cfg["shard_size"],
                file_path.stem
            ),
            "test": ShardWriter(
                output_dir / "test",
                self.proc_cfg["shard_size"], 
                file_path.stem
            )
        }
        
        # Track samples per split
        split_counts = {"train": 0, "validation": 0, "test": 0}
        
        # Process file
        with h5py.File(file_path, "r") as f:
            for gname in sorted(f.keys()):
                # Validation checks
                is_valid, _ = self._is_profile_valid(f[gname])
                if not is_valid:
                    continue
                
                # Use fraction check
                use_fraction = self.train_cfg["use_fraction"]
                if use_fraction < 1.0:
                    hash_val = int(hashlib.sha256(gname.encode('utf-8')).hexdigest()[:8], 16) / 0xFFFFFFFF
                    if hash_val >= use_fraction:
                        continue
                
                # Process profile
                grp = f[gname]
                n_t = grp[self.time_var].shape[0]
                if n_t <= 1:
                    continue
                    
                profile = self._extract_profile(grp, gname, n_t)
                if profile is None:
                    continue

                if (np.any(~np.isfinite(profile)) or
                    np.any(profile <= self.min_value_threshold)):
                    continue

                # Determine split
                p = int(hashlib.sha256((gname + "_split").encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
                test_frac = self.train_cfg["test_fraction"]
                val_frac = self.train_cfg["val_fraction"]
                
                if p < test_frac:
                    split_key = "test"
                elif p < test_frac + val_frac:
                    split_key = "validation"
                else:
                    split_key = "train"
                
                # Convert profile to samples
                if self.prediction_mode == "ratio":
                    samples = self._profile_to_samples_ratio(
                        profile, n_t, self.norm_stats.get("ratio_stats", {})
                    )
                else:
                    norm_prof = self.norm_helper.normalize_profile(
                        torch.from_numpy(profile)
                    ).numpy()
                    samples = self._profile_to_samples(norm_prof, n_t)
                
                if samples is not None:
                    # Add to appropriate writer - no need to track indices
                    shard_writers[split_key].add_samples(samples)
                    split_counts[split_key] += samples.shape[0]
        
        # Flush all writers
        for writer in shard_writers.values():
            writer.flush()
        
        # Return metadata
        return {
            "splits": {
                "train": {
                    "shards": shard_writers["train"].get_shard_metadata(),
                    "samples_written": split_counts["train"]
                },
                "validation": {
                    "shards": shard_writers["validation"].get_shard_metadata(),
                    "samples_written": split_counts["validation"]
                },
                "test": {
                    "shards": shard_writers["test"].get_shard_metadata(),
                    "samples_written": split_counts["test"]
                }
            }
        }

    def _update_stats_for_profile(self, profile, n_t, accumulators, ratio_accumulators):
        """Consistent normalization in both modes."""
        import logging
        logger = logging.getLogger(__name__)
        
        if self.prediction_mode == "ratio":
            # Use full profiles for all variables in ratio mode
            for var, acc in accumulators.items():
                idx = acc["index"]
                method = acc["method"]
                
                # Use full profile data for all variables (not just initial timestep)
                if var == self.time_var and n_t > 1:
                    vec = profile[1:, idx]
                else:
                    vec = profile[:, idx]
                
                if vec.size > 0:
                    self.normalizer._update_single_accumulator(acc, vec, var)
            
            # Compute ratio statistics correctly with proper indices
            initial = profile[0, self.species_indices]
            future = profile[1:, self.species_indices]
            
            ratios = future / np.maximum(initial[None, :], self.normalizer.epsilon)
            ratios = np.clip(ratios, -DEFAULT_EPSILON_MAX, DEFAULT_EPSILON_MAX)
            log_ratios = np.sign(ratios) * np.log10( np.clip(np.abs(ratios), DEFAULT_EPSILON_MIN, DEFAULT_EPSILON_MAX))


            for i, var_name in enumerate(self.species_vars):
                self.normalizer._update_single_accumulator(ratio_accumulators[var_name], log_ratios[:, i], var_name)
        else:
            # Absolute mode
            for var, acc in accumulators.items():
                idx = acc["index"]
                vec = profile[1:, idx] if (var == self.time_var and n_t > 1) else profile[:, idx]
                if vec.size > 0:
                    self.normalizer._update_single_accumulator(acc, vec, var)

    def _process_single_group(self, grp, gname, ratio_stats) -> Optional[Tuple[np.ndarray, str]]:
        # The stricter validation is now done before this function is called.
        n_t = grp[self.time_var].shape[0]
        if n_t <= 1: return None
        profile = self._extract_profile(grp, gname, n_t)
        if profile is None: return None
        
        if self.prediction_mode == "ratio": samples = self._profile_to_samples_ratio(profile, n_t, ratio_stats)
        else: samples = self._profile_to_samples(self.norm_helper.normalize_profile(torch.from_numpy(profile)).numpy(), n_t)
        if samples is None: return None
        
        p = int(hashlib.sha256((gname + "_split").encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
        split_key = "test" if p < self.train_cfg["test_fraction"] else "validation" if p < self.train_cfg["test_fraction"] + self.train_cfg["val_fraction"] else "train"
        return samples, split_key

    
    def _extract_profile(self, group: h5py.Group, gname: str, n_t: int) -> Optional[np.ndarray]:
        import re
        globals_dict = {f"{lbl}_init": float(val) for lbl, val in re.findall(r"_([A-Z])_([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", gname) if f"{lbl}_init" in self.global_vars}
        if len(globals_dict) != len(self.global_vars): return None
        profile = np.empty((n_t, self.n_vars), dtype=np.float32)
        try:
            for i, var in enumerate(self.var_order):
                profile[:, i] = group[var][:] if var in group else globals_dict[var]
        except Exception: return None
        return profile

    def _profile_to_samples(self, norm_prof, n_t):
        """Get samples"""
        if n_t <= 1:
            return None
        
        n_inputs = self.n_species + self.n_globals + 1
        samples = np.empty((n_t - 1, n_inputs + self.n_species), dtype=np.float32)
        
        # Initial species
        samples[:, :self.n_species] = norm_prof[0, self.species_indices] 

        # Globals
        samples[:, self.n_species:self.n_species + self.n_globals] = norm_prof[0, self.global_indices]

        # Time
        samples[:, n_inputs - 1] = norm_prof[1:, self.time_idx]

        # Target species
        samples[:, n_inputs:] = norm_prof[1:, self.species_indices]
        
        return samples

    def _profile_to_samples_ratio(
        self, raw_prof: np.ndarray, n_t: int, ratio_stats: Dict[str, Dict]
    ) -> Optional[np.ndarray]:
        """
        Build (n_t‑1) samples for ratio‑prediction mode.
        raw_prof is **unnormalised** profile array  shape = (n_t, n_vars)
        ratio_stats contains mean/std/min/max for log‑ratios (already computed)
        """
        import logging
        logger = logging.getLogger(__name__)

        if n_t <= 1:
            return None
        
        n_inputs  = self.n_species + self.n_globals + 1
        samples   = np.empty((n_t - 1, n_inputs + self.n_species), dtype=np.float32)

        norm_prof = self.norm_helper.normalize_profile(torch.from_numpy(raw_prof)).numpy()
        samples[:, :self.n_species]                   = norm_prof[0, self.species_indices]
        samples[:, self.n_species:self.n_species+self.n_globals] = norm_prof[0, self.global_indices]
        samples[:, n_inputs - 1]                      = norm_prof[1:, self.time_idx]

        initial = raw_prof[0, self.species_indices]
        future  = raw_prof[1:, self.species_indices]

        ratios = future / np.maximum(initial[None, :], self.norm_cfg["epsilon"])
        ratios = np.clip(ratios, -DEFAULT_EPSILON_MAX, DEFAULT_EPSILON_MAX)
        log_ratios = np.sign(ratios) * np.log10(np.clip(np.abs(ratios), DEFAULT_EPSILON_MIN, DEFAULT_EPSILON_MAX))

        methods_cfg = self.norm_cfg.get("methods", {})
        default_m   = self.norm_cfg.get("default_method", "standard")
        clamp_val   = self.norm_cfg.get("clamp_value", 50.0)
        min_std     = self.norm_cfg.get("min_std", 1e-10)

        normd = np.empty_like(log_ratios, dtype=np.float32)

        for i, var in enumerate(self.species_vars):
            method = methods_cfg.get(var, default_m)
            if method.startswith("log-"):
                method = method[4:]

            stats = ratio_stats[var]

            if method == "min-max":
                rng = max(stats["max"] - stats["min"], self.norm_cfg["epsilon"])
                normd[:, i] = (log_ratios[:, i] - stats["min"]) / rng
            else:
                std = max(stats["std"], min_std)
                normd[:, i] = (log_ratios[:, i] - stats["mean"]) / std

        if np.any(np.abs(normd) > clamp_val):
            n_clamped = np.sum(np.abs(normd) > clamp_val)
            logger.warning(f"Clamping {n_clamped} normalised log‑ratio values to ±{clamp_val}")

        samples[:, n_inputs:] = np.clip(normd, -clamp_val, clamp_val)
        return samples

def stats_worker(file_path, config):
    return CorePreprocessor(config).process_file_for_stats(Path(file_path))

def shard_worker(file_path, config, norm_stats, output_dir):
    """Legacy helper kept for completeness; matches shard_worker_split_aware signature."""
    processor = CorePreprocessor(config, norm_stats)
    return processor.process_file_for_shards(Path(file_path), Path(output_dir))

class DataPreprocessor:
    """Main parent class to orchestrate parallel data preprocessing."""
    def __init__(self, raw_files: List[Path], output_dir: Path, config: Dict[str, Any]):
        self.raw_files = sorted(raw_files)
        self.output_dir = output_dir

        self.processed_dir = self.output_dir
        self.processed_dir.mkdir(parents=True, exist_ok=True)

        self.config = config
        self.logger = logging.getLogger(__name__)
        self.normalizer = DataNormalizer(config)
        self.num_workers = config["preprocessing"].get("num_workers", 1)
        self.parallel = self.num_workers > 1 and len(self.raw_files) > 1

    def process_to_npy_shards(self) -> None:
        """Main entry point - creates split-specific shard directories."""
        start_time = time.time()
        self.logger.info(f"Starting core data preprocessing with {len(self.raw_files)} files...")
        
        # Collect statistics
        norm_stats, file_sample_counts, summary_report = self._collect_stats_and_counts()
        save_json(norm_stats, self.output_dir / "normalization.json")

        # Write split-specific shards
        split_metadata = self._write_normalized_shards(norm_stats, file_sample_counts)
        
        # Save split-aware shard index
        shard_index = {
            "n_species": len(self.config["data"]["species_variables"]),
            "n_globals": len(self.config["data"]["global_variables"]),
            "samples_per_shard": self.config["preprocessing"]["shard_size"],
            "compression": self.config["preprocessing"].get("compression"),
            "prediction_mode": self.config.get("prediction", {}).get("mode", "absolute"),
            "splits": split_metadata,
            "total_samples": sum(meta["total_samples"] for meta in split_metadata.values())
        }
        save_json(shard_index, self.output_dir / "shard_index.json")

        self._write_summary_log(summary_report, shard_index["total_samples"])
        self.logger.info(f"Core data preprocessing completed in {time.time() - start_time:.1f}s")
        
    def generate_split_indices(self) -> None:
        """Generates train/val/test split indices from an existing shard_index.json. This is a very fast operation."""
        self.logger.info("Generating new train/val/test split indices...")
        shard_index_path = self.output_dir / "shard_index.json"
        if not shard_index_path.exists():
            raise FileNotFoundError(f"Cannot generate splits: shard_index.json not found in {self.output_dir}")
        
        shard_index = load_json(shard_index_path)
        total_samples = shard_index["total_samples"]
        indices = np.arange(total_samples)
        
        seed = self.config.get("system", {}).get("seed", 42)
        np.random.seed(seed)
        np.random.shuffle(indices)
        
        use_fraction = self.config["training"].get("use_fraction", 1.0)
        if use_fraction < 1.0:
            indices = indices[:int(total_samples * use_fraction)]
        
        n = len(indices)
        test_frac = self.config["training"]["test_fraction"]
        val_frac = self.config["training"]["val_fraction"]
        
        test_split_idx = int(n * test_frac)
        val_split_idx = test_split_idx + int(n * val_frac)
        
        split_data = {
            "test": np.sort(indices[:test_split_idx]).astype(np.int64),
            "validation": np.sort(indices[test_split_idx:val_split_idx]).astype(np.int64),
            "train": np.sort(indices[val_split_idx:]).astype(np.int64)
        }
        
        for name, idx_array in split_data.items():
            path = self.output_dir / shard_index["split_files"][name]
            np.save(path, idx_array)
            self.logger.info(f"Saved {name} indices to {path} ({len(idx_array)} samples)")

    def _write_normalized_shards(self, norm_stats, file_sample_counts) -> Dict[str, List[Dict]]:
        """Second pass: write split-specific shards to separate directories."""
        self.logger.info("Writing split-specific shards...")
        
        # Create split directories
        split_dirs = {
            "train": self.processed_dir / "train",
            "validation": self.processed_dir / "validation", 
            "test": self.processed_dir / "test"
        }
        for split_name, dir_path in split_dirs.items():
            dir_path.mkdir(exist_ok=True)
            self.logger.info(f"Created {split_name} directory: {dir_path}")
        
        # Initialize split metadata
        split_metadata = {
            "train": {"shards": [], "total_samples": 0},
            "validation": {"shards": [], "total_samples": 0},
            "test": {"shards": [], "total_samples": 0}
        }
        
        # Process each file
        with ProcessPoolExecutor(max_workers=self.num_workers if self.parallel else 1) as exe:
            futures = []
            
            for file_path in self.raw_files:
                future = exe.submit(
                    shard_worker_split_aware,
                    file_path,
                    self.config,
                    norm_stats,
                    self.processed_dir
                )
                futures.append((future, file_path))
            
            # Collect results
            for future, file_path in futures:
                result = future.result()
                
                # Aggregate metadata by split
                for split_name in ["train", "validation", "test"]:
                    split_meta = result["splits"][split_name]
                    split_metadata[split_name]["shards"].extend(split_meta["shards"])
                    split_metadata[split_name]["total_samples"] += split_meta["samples_written"]
        
        # Sort shards within each split and assign global indices
        for split_name, meta in split_metadata.items():
            # Sort shards by filename
            meta["shards"].sort(key=lambda x: x["filename"])
            
            # Assign sequential start/end indices
            current_idx = 0
            for shard in meta["shards"]:
                shard["start_idx"] = current_idx
                shard["end_idx"] = current_idx + shard["n_samples"]
                current_idx = shard["end_idx"]
            
            self.logger.info(f"{split_name}: {len(meta['shards'])} shards, {meta['total_samples']:,} samples")
        
        return split_metadata
     
    def _collect_stats_and_counts(self) -> Tuple[Dict, Dict, Dict]:
        self.logger.info("Pass 1: Collecting statistics and sample counts...")
        prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        final_accs = self.normalizer._initialize_accumulators()
        final_ratio_accs = {var: {"count": 0,"mean": 0.0,"m2": 0.0,"min": float('inf'),"max": float('-inf')} for var in self.config["data"]["species_variables"]} if prediction_mode == "ratio" else {}
        file_counts = {}
        
        total_report = {
            "total_profiles": 0,
            "profiles_kept": 0,
            "dropped_reasons": defaultdict(int)
        }

        with ProcessPoolExecutor(max_workers=self.num_workers if self.parallel else 1) as executor:
            futures = {executor.submit(stats_worker, fp, self.config): fp for fp in self.raw_files}
            for fut in as_completed(futures):
                accs, ratio_accs, count, worker_report = fut.result()
                
                # Aggregate results
                self.normalizer._merge_accumulators(final_accs, accs)
                if ratio_accs: self.normalizer._merge_accumulators(final_ratio_accs, ratio_accs)
                file_counts[futures[fut].name] = count
                
                # --- NEW: Aggregate the reports ---
                total_report["total_profiles"] += worker_report["total_profiles"]
                total_report["profiles_kept"] += worker_report["profiles_kept"]
                for reason, num in worker_report["dropped_reasons"].items():
                    total_report["dropped_reasons"][reason] += num

        norm_stats = self.normalizer._finalize_statistics(final_accs)
        if final_ratio_accs:
            norm_stats["ratio_stats"] = self.normalizer._finalize_statistics(final_ratio_accs, is_ratio=True)

        file_counts = {Path(k).stem: v for k, v in file_counts.items()}

        return norm_stats, file_counts, total_report

    def _write_summary_log(self, report: Dict, total_samples: int):
        """Writes a human-readable summary of the preprocessing results."""
        log_dir = Path(self.config["paths"]["log_dir"])
        log_dir.mkdir(exist_ok=True)
        summary_path = log_dir / f"preprocessing_summary_{time.strftime('%Y%m%d_%H%M%S')}.txt"
        
        dropped_count = report["total_profiles"] - report["profiles_kept"]
        
        reason_map = {
            "missing_keys": "Required dataset keys were missing",
            "non_finite": "Contained NaN or Infinity values",
            "below_threshold": f"A species value was below the threshold ({self.config['preprocessing']['min_value_threshold']:.1e})",
            "too_few_timesteps": "Contained 1 or fewer time steps",
            "extract_profile_failed": "Failed to extract global variables from name",
            "read_error": "Could not read a dataset from the HDF5 group"
        }

        with open(summary_path, 'w') as f:
            f.write("="*60 + "\n")
            f.write("      Data Preprocessing Summary\n")
            f.write("="*60 + "\n\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Raw Files Processed: {len(self.raw_files)}\n\n")

            f.write("--- Profile Filtering --- \n")
            f.write(f"Total Profiles Found:     {report['total_profiles']:,}\n")
            f.write(f"Profiles Kept:            {report['profiles_kept']:,}\n")
            f.write(f"Profiles Dropped:         {dropped_count:,}\n\n")
            
            if dropped_count > 0:
                f.write("--- Reasons for Dropped Profiles ---\n")
                for reason, count in sorted(report["dropped_reasons"].items()):
                    f.write(f"  - {count:>10,} : {reason_map.get(reason, reason)}\n")
                f.write("\n")

            f.write("--- Final Sample Count ---\n")
            f.write(f"Total Usable Samples:     {total_samples:,}\n")
            f.write("(Train/Val/Test splits generated separately)\n")

        self.logger.info(f"Preprocessing summary saved to: {summary_path}")


class ShardWriter:
    """Writes numpy arrays to shard files, handling buffering and file naming."""
    def __init__(self, output_dir: Path, shard_size: int, shard_idx_base: str):
        self.output_dir = output_dir
        self.shard_size = shard_size
        self.shard_idx_base = shard_idx_base
        self.buffer: List[np.ndarray] = []
        self.buffer_size = 0
        self.local_shard_id = 0
        self.shard_metadata: List[Dict] = []
        
        # Ensure the output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def add_samples(self, samples: np.ndarray):
        """Simplified add_samples without tracking global indices."""
        if samples.dtype != np.float32:
            samples = samples.astype(np.float32)
        
        self.buffer.append(samples)
        self.buffer_size += samples.shape[0]
        
        while self.buffer_size >= self.shard_size:
            self._write_shard()

    def _write_shard(self) -> None:
        """Write one shard of exactly shard_size samples (or less if flushing)."""
        if not self.buffer:
            return

        rows_to_write = []
        size_so_far = 0

        # Collect arrays until we have enough for a shard
        while self.buffer and size_so_far < self.shard_size:
            arr = self.buffer.pop(0)
            needed = self.shard_size - size_so_far
            
            if arr.shape[0] <= needed:
                rows_to_write.append(arr)
                size_so_far += arr.shape[0]
            else:
                # Split the array
                rows_to_write.append(arr[:needed])
                self.buffer.insert(0, arr[needed:])
                size_so_far += needed
        
        # Update buffer size
        self.buffer_size = sum(arr.shape[0] for arr in self.buffer)

        # Write shard
        data = np.concatenate(rows_to_write).astype(np.float32, copy=False)
        
        final_path = self.output_dir / f"shard_{self.shard_idx_base}_{self.local_shard_id:04d}.npy"
        tmp_path = final_path.with_suffix(".tmp.npy")

        np.save(tmp_path, data, allow_pickle=False)
        os.replace(tmp_path, final_path)

        self.shard_metadata.append({
            "filename": final_path.name,
            "n_samples": data.shape[0],
        })
        
        self.local_shard_id += 1

    def flush(self) -> None:
        """Write out any rows left in the buffer (< shard_size)."""
        if self.buffer_size == 0:
            return
        
        # write whatever is left as a final shard
        rows_to_write = self.buffer
        self.buffer = []
        self.buffer_size = 0

        data = np.concatenate(rows_to_write).astype(np.float32, copy=False)
        final_path = self.output_dir / f"shard_{self.shard_idx_base}_{self.local_shard_id:04d}.npy"
        tmp_path   = final_path.with_suffix(".tmp.npy")
        np.save(tmp_path, data, allow_pickle=False)
        os.replace(tmp_path, final_path)

        self.shard_metadata.append({
            "filename": final_path.name,
            "n_samples": data.shape[0],
        })
        self.local_shard_id += 1

    def get_shard_metadata(self) -> List[Dict]:
        """Return metadata collected for all shards."""
        return list(self.shard_metadata)
    
def shard_worker_split_aware(file_path, config, norm_stats, output_dir):
    """Worker function that creates split-specific shards."""
    processor = CorePreprocessor(config, norm_stats)
    return processor.process_file_for_shards(Path(file_path), Path(output_dir))   

===== /Users/imalsky/Desktop/Chemulator/src/data/dataset.py =====
#!/usr/bin/env python3
"""
High-performance dataset implementation for chemical kinetics training.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple, Optional, List
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import math
from functools import lru_cache


class NPYDataset(Dataset):
    """
    PyTorch Dataset with GPU caching and CPU fallback.
    """
    def __init__(self, shard_dir: Path, split_name: str, config: Dict[str, Any], 
                 device: torch.device):
        """Initialize dataset for a specific split."""
        super().__init__()
        self.shard_dir = Path(shard_dir)
        self.split_dir = self.shard_dir / split_name
        self.split_name = split_name
        self.config = config
        self.device = device
        self.logger = logging.getLogger(__name__)

        # Verify split directory exists
        if not self.split_dir.exists():
            raise FileNotFoundError(f"Split directory not found: {self.split_dir}")

        # Load shard index metadata
        shard_index_path = self.shard_dir / "shard_index.json"
        with open(shard_index_path) as f:
            full_index = json.load(f)
        
        self.shard_index = full_index
        self.split_info = full_index["splits"][split_name]
        
        # Extract dimensions
        self.n_species = self.shard_index["n_species"]
        self.n_globals = self.shard_index["n_globals"]
        self.samples_per_shard = self.shard_index["samples_per_shard"]
        self.prediction_mode = self.shard_index.get("prediction_mode", "absolute")
        self.n_features = self.n_species * 2 + self.n_globals + 1
        self.n_inputs = self.n_species + self.n_globals + 1
        
        # Get shard info for this split
        self.shards = self.split_info["shards"]
        self.n_shards = len(self.shards)
        self.n_total_samples = self.split_info["total_samples"]
        
        # Build lookup arrays
        self._shard_starts = np.array([s["start_idx"] for s in self.shards])
        self._shard_ends = np.array([s["end_idx"] for s in self.shards])
        
        # Memory info
        self.bytes_per_sample = self.n_features * 4  # float32
        self.total_bytes = self.n_total_samples * self.bytes_per_sample
        
        # Initialize caching
        self.gpu_cache = None
        self.cpu_fallback = False
        self._try_gpu_cache()
        
    def _try_gpu_cache(self):
        """Load the entire split to GPU memory; fall back to CPU if needed."""
        gpu_cache_setting = self.config.get("training", {}).get("gpu_cache_dataset", "auto")
        if gpu_cache_setting is False:
            self.logger.info(f"GPU caching disabled for {self.split_name}")
            self.cpu_fallback = True
            return

        if self.device.type != "cuda":
            self.logger.info(f"GPU caching not available on {self.device.type}")
            self.cpu_fallback = True
            return

        free_mem, _ = torch.cuda.mem_get_info(self.device.index)
        needed_gb = self.total_bytes / 1024 ** 3
        free_gb   = free_mem       / 1024 ** 3
        if needed_gb > free_gb * 0.85:     
            self.logger.warning(
                f"Insufficient GPU memory for {self.split_name}: "
                f"need {needed_gb:.1f} GB, have {free_gb:.1f} GB.  Falling back to CPU."
            )
            self.cpu_fallback = True
            return

        self.logger.info(f"Loading {self.split_name} dataset to GPU ({needed_gb:.1f} GB)…")
        start = time.time()

        self.gpu_cache = torch.empty(
            (self.n_total_samples, self.n_features),
            dtype=torch.float32,
            device=self.device,
        )

        cur = 0
        for shard in self.shards:
            shard_path = self.split_dir / shard["filename"]
            if self.shard_index.get("compression") == "npz":
                with np.load(shard_path) as zf:
                    data = zf["data"].astype(np.float32, copy=False)
            else:
                data = np.load(shard_path).astype(np.float32, copy=False)

            n = data.shape[0]
            self.gpu_cache[cur:cur + n] = torch.from_numpy(data).pin_memory().to(
                self.device, non_blocking=True
            )
            cur += n
            del data                  

        torch.cuda.synchronize()
        t = time.time() - start
        self.logger.info(f"GPU cache loaded in {t:.1f}s ({needed_gb/t:.1f} GB/s)")

    def _load_shard_cpu(self, shard_idx: int) -> np.ndarray:
        """Load a shard from disk (CPU fallback)."""
        # ====================================================================
        # FIXED: Added simple per-worker LRU cache to avoid re-reading files
        # ====================================================================
        # Check if the cache exists on this worker process
        if not hasattr(self, '_shard_cache'):
            # functools.lru_cache is a simpler way to do this
            from functools import lru_cache
            # Cache up to 2 shards per worker, a common scenario
            self._shard_cache = lru_cache(maxsize=2)(self._load_shard_from_disk)
        
        return self._shard_cache(shard_idx)

    def _load_shard_from_disk(self, shard_idx: int) -> np.ndarray:
        """Helper for the LRU cache that actually hits the disk."""
        shard_info = self.shards[shard_idx]
        shard_path = self.split_dir / shard_info["filename"]
        
        if self.shard_index.get("compression") == "npz":
            with np.load(shard_path) as zf:
                return zf["data"].astype(np.float32, copy=False)
        else:
            return np.load(shard_path).astype(np.float32, copy=False)

    def __len__(self) -> int:
        """Return the total number of samples."""
        return self.n_total_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get a single sample by index."""
        if self.gpu_cache is not None:
            # Fast path: Direct GPU slicing, no change needed here.
            row = self.gpu_cache[idx]
            return row[:self.n_inputs], row[self.n_inputs:]
        else:
            # CPU fallback path
            if not self.cpu_fallback:
                raise RuntimeError("Dataset not properly initialized")
            
            # Find which shard contains this index
            shard_idx = np.searchsorted(self._shard_starts, idx, side='right') - 1
            local_idx = idx - self._shard_starts[shard_idx]
            
            # Load shard and get sample
            shard_data = self._load_shard_cpu(shard_idx)
            row = shard_data[local_idx]
            
            # Return CPU tensors. The batch will be moved to the GPU
            # all at once by the dataloader / training loop.
            input_tensor = torch.from_numpy(row[:self.n_inputs].copy())
            target_tensor = torch.from_numpy(row[self.n_inputs:].copy())
            
            return input_tensor, target_tensor

    def get_cache_info(self) -> Dict[str, Any]:
        """Get cache statistics with consistent structure."""
        if self.gpu_cache is not None:
            return {
                "type": "gpu",
                "size_gb": self.total_bytes / 1024**3,
                "device": str(self.device),
                "status": "active"
            }
        elif self.cpu_fallback:
            return {
                "type": "cpu",
                "size_gb": 0,
                "device": str(self.device),
                "status": "fallback",
                "message": "Using CPU loading due to insufficient GPU memory"
            }
        else:
            return {
                "type": "none",
                "size_gb": 0,
                "device": str(self.device),
                "status": "error",
                "message": "Dataset caching failed"
            }

def create_dataloader(dataset: Dataset,
                      config: Dict[str, Any],
                      shuffle: bool = True,
                      device: Optional[torch.device] = None,
                      drop_last: bool = True,
                      **_) -> DataLoader:
    """
    Build a DataLoader that never loops Python‑side over 32 768 items.
    If the dataset lives on the GPU, we wrap it in GPUBatchDataset so each
    __getitem__ delivers a full batch slice in one shot.
    """
    if dataset is None or len(dataset) == 0:
        logging.getLogger(__name__).warning("Cannot create DataLoader for empty dataset")
        return None

    log  = logging.getLogger(__name__)
    tcfg = config["training"]
    bs   = tcfg["batch_size"]

    is_gpu_cached  = getattr(dataset, "gpu_cache", None) is not None
    is_cpu_fallback = getattr(dataset, "cpu_fallback", False)

    # ---------- GPU‑cached fast path ----------
    if is_gpu_cached:
        log.info(f"DataLoader[{dataset.split_name}] GPU‑batch mode: bs={bs}")

        class GPUBatchDataset(torch.utils.data.Dataset):
            def __init__(self, gpu_tensor: torch.Tensor, n_inputs: int,
                         batch_size: int, shuffle_batches: bool):
                self.gpu_tensor  = gpu_tensor
                self.n_inputs    = n_inputs
                self.batch_size  = batch_size
                self.shuffle     = shuffle_batches
                self.total       = gpu_tensor.size(0)
                self.n_batches   = math.ceil(self.total / batch_size)
                self.permutation = None

            def __len__(self):
                return self.n_batches

            def __getitem__(self, batch_idx: int):
                if self.shuffle:
                    if self.permutation is None or batch_idx == 0:
                        self.permutation = torch.randperm(
                            self.total, device=self.gpu_tensor.device
                        )
                    idx = self.permutation
                else:
                    idx = None  # straight slice

                start = batch_idx * self.batch_size
                end   = min(start + self.batch_size, self.total)

                if idx is None:
                    batch = self.gpu_tensor[start:end]
                else:
                    batch = self.gpu_tensor.index_select(0, idx[start:end])

                return batch[:, :self.n_inputs], batch[:, self.n_inputs:]

        gpu_ds = GPUBatchDataset(
            dataset.gpu_cache, dataset.n_inputs, bs, shuffle
        )
        return DataLoader(
            gpu_ds,
            batch_size=None,          # dataset already returns a full batch
            shuffle=False,            # handled inside GPUBatchDataset
            num_workers=0,
            pin_memory=False,
            drop_last=False,
        )

    # ---------- CPU fallback ----------
    log.warning(f"DataLoader[{dataset.split_name}] CPU fallback mode: bs={bs}")
    workers = tcfg.get("num_workers") or min(16, (os.cpu_count() or 1))
    return DataLoader(
        dataset,
        batch_size=bs,
        shuffle=shuffle,
        num_workers=workers,
        pin_memory=(device and device.type == "cuda"),
        drop_last=drop_last,
        persistent_workers=(workers > 0),
        prefetch_factor=2 if workers > 0 else None,
    )

===== /Users/imalsky/Desktop/Chemulator/src/data/normalizer.py =====
#!/usr/bin/env python3
"""
Data normalization module for chemical kinetics datasets.
This version preserves all data validation and logging and supports the
parallel preprocessing architecture with _merge_accumulators.
"""

import logging
import math
from typing import Dict, List, Any, Optional

import numpy as np
import torch

DEFAULT_EPSILON = 1e-30
DEFAULT_MIN_STD = 1e-10
DEFAULT_CLAMP = 50.0
# Safe epsilon for float32
SAFE_EPSILON = 1e-38
MIN_RANGE = 1e-10

class DataNormalizer:
    """Calculates normalization statistics with robust data validation."""
    
    def __init__(self, config: Dict[str, Any]) -> None:
        self.config = config
        self.data_config = config["data"]
        self.norm_config = config["normalization"]

        self.species_vars = self.data_config["species_variables"]
        self.global_vars = self.data_config["global_variables"]
        self.time_var = self.data_config["time_variable"]
        self.all_vars = self.species_vars + self.global_vars + [self.time_var]

        self.epsilon = self.norm_config.get("epsilon", DEFAULT_EPSILON)
        self.min_std = self.norm_config.get("min_std", DEFAULT_MIN_STD)
        self.logger = logging.getLogger(__name__)
        
    def _initialize_accumulators(self) -> Dict[str, Dict[str, Any]]:
        """Initialize per-variable statistics accumulators."""
        accumulators = {}
        for i, var in enumerate(self.all_vars):
            method = self._get_method(var)
            if method == "none":
                continue
            acc = {
                "method": method, "index": i, "count": 0, "mean": 0.0, "m2": 0.0,
                "min": float("inf"), "max": float("-inf"),
            }
            accumulators[var] = acc
        return accumulators
    
    def _get_method(self, var: str) -> str:
        """Get normalization method for a variable."""
        methods = self.norm_config.get("methods", {})
        return methods.get(var, self.norm_config["default_method"])
    
    def _update_single_accumulator(self, acc: Dict[str, Any], vec: np.ndarray, var_name: str):
        """
        Vectorized update for one accumulator with safe epsilon handling.
        """
        if vec.size == 0:
            return

        # 1. Filter non-finite values and warn if there are many
        finite_mask = np.isfinite(vec)
        if not np.all(finite_mask):
            n_non_finite = (~finite_mask).sum()
            if n_non_finite / vec.size > 0.01:
                self.logger.warning(f"Variable '{var_name}' has {n_non_finite}/{vec.size} non-finite values.")
            vec = vec[finite_mask]
            if vec.size == 0:
                self.logger.warning(f"Variable '{var_name}' has no finite values after filtering, skipping.")
                return

        # 2. Handle log-transformations with safe epsilon
        if acc["method"].startswith("log-"):
            below_epsilon = vec < SAFE_EPSILON
            if np.any(below_epsilon):
                self.logger.warning(
                    f"Variable '{var_name}' has {below_epsilon.sum()} values below epsilon {SAFE_EPSILON} "
                    f"Min value: {vec.min():.2e}"
                )
            vec = np.log10(np.maximum(vec, SAFE_EPSILON))
            
            # Check for extreme values after log transform
            if vec.min() < -25 or vec.max() > 25:
                self.logger.warning(
                    f"Variable '{var_name}' has extreme log values: [{vec.min():.1f}, {vec.max():.1f}]"
                )

        # 3. Perform Chan's parallel update for mean and variance
        n_b = vec.size
        mean_b = float(vec.mean())
        m2_b = float(((vec - mean_b) ** 2).sum()) if n_b > 1 else 0.0

        n_a = acc["count"]
        delta = mean_b - acc["mean"]
        n_ab = n_a + n_b

        if n_ab > 0:
            acc["mean"] = (n_a * acc["mean"] + n_b * mean_b) / n_ab
            acc["m2"] += m2_b + delta**2 * n_a * n_b / n_ab
        
        acc["count"] = n_ab
        acc["min"] = min(acc["min"], float(vec.min()))
        acc["max"] = max(acc["max"], float(vec.max()))

    def _merge_accumulators(
        self,
        main_accs: Dict[str, Dict[str, Any]],
        other_accs: Dict[str, Dict[str, Any]],
    ) -> None:
        """Merge statistics from another set of accumulators into the main one."""
        for var, other_acc in other_accs.items():
            if not other_acc: continue
            if var not in main_accs:
                main_accs[var] = other_acc
                continue

            main_acc = main_accs[var]
            
            n_a, mean_a, m2_a = main_acc["count"], main_acc["mean"], main_acc["m2"]
            n_b, mean_b, m2_b = other_acc["count"], other_acc["mean"], other_acc["m2"]
            
            n_ab = n_a + n_b
            if n_ab == 0: continue
                
            delta = mean_b - mean_a
            
            main_acc["mean"] = (n_a * mean_a + n_b * mean_b) / n_ab
            main_acc["m2"] = m2_a + m2_b + (delta**2 * n_a * n_b) / n_ab
            main_acc["count"] = n_ab
            main_acc["min"] = min(main_acc["min"], other_acc["min"])
            main_acc["max"] = max(main_acc["max"], other_acc["max"])
        
    def _finalize_statistics(self, accumulators: Dict[str, Dict[str, Any]], is_ratio: bool = False) -> Dict[str, Any]:
        """Finalize statistics from accumulators with improved numerical stability."""
        stats = { "per_key_stats": {} }
        if not is_ratio:
            stats["normalization_methods"] = {}
        
        for var, acc in accumulators.items():
            method = acc.get("method", "standard")
            if not is_ratio:
                stats["normalization_methods"][var] = method
            
            if method == "none":
                continue
            
            # Check if we have any valid samples
            if acc["count"] == 0:
                self.logger.warning(f"Variable '{var}' has no valid samples. Setting normalization to 'none'.")
                if not is_ratio:
                    stats["normalization_methods"][var] = "none"
                continue
            
            var_stats = {"method": method}
            
            # Calculate std with safeguards
            if acc["count"] > 1:
                variance = acc["m2"] / (acc["count"] - 1)
                std = max(math.sqrt(variance), self.min_std)
            else:
                std = 1.0
            
            if "standard" in method:
                mean_key, std_key = ("log_mean", "log_std") if "log" in method else ("mean", "std")
                var_stats[mean_key], var_stats[std_key] = acc["mean"], std
                
                # Warn if std is very small (constant variable)
                if std < MIN_RANGE:
                    self.logger.warning(f"Variable '{var}' appears constant (std={std:.2e}). Consider using 'none' normalization.")
                    
            elif "min-max" in method:
                var_stats["min"], var_stats["max"] = acc["min"], acc["max"]
                
                # Handle constant or near-constant variables
                range_val = acc["max"] - acc["min"]
                if range_val < MIN_RANGE:
                    self.logger.warning(f"Variable '{var}' has very small range ({range_val:.2e}). Adjusting to prevent instability.")
                    # Expand the range artificially to prevent division by tiny numbers
                    center = (acc["max"] + acc["min"]) / 2
                    var_stats["min"] = center - MIN_RANGE / 2
                    var_stats["max"] = center + MIN_RANGE / 2
            
            if is_ratio:
                stats[var] = {"mean": acc["mean"], "std": std, "min": acc["min"], "max": acc["max"], "count": acc["count"]}
            else:
                stats["per_key_stats"][var] = var_stats

        if not is_ratio:
            # Set any missing variables to "none" normalization
            for var in self.all_vars:
                if var not in stats["normalization_methods"]:
                    self.logger.warning(f"Variable '{var}' not found in statistics. Setting normalization to 'none'.")
                    stats["normalization_methods"][var] = "none"
            
            stats["epsilon"] = SAFE_EPSILON  # Use safe epsilon
            stats["clamp_value"] = self.norm_config.get("clamp_value", DEFAULT_CLAMP)
        
        return stats


class NormalizationHelper:
    """Applies pre-computed normalization statistics to data tensors."""
    
    def __init__(self, stats: Dict[str, Any], device: torch.device, 
                 species_vars: List[str], global_vars: List[str], 
                 time_var: str, config: Optional[Dict[str, Any]] = None):
        self.stats = stats
        self.device = device
        self.species_vars = species_vars
        self.global_vars = global_vars
        self.time_var = time_var

        self.n_species = len(species_vars)
        self.n_globals = len(global_vars)
        self.methods = stats["normalization_methods"]
        self.per_key_stats = stats["per_key_stats"]

        self.epsilon = stats.get("epsilon", DEFAULT_EPSILON)
        self.clamp_value = stats.get("clamp_value", DEFAULT_CLAMP)
        
        self.ratio_stats = stats.get("ratio_stats", None)

        self.logger = logging.getLogger(__name__)
        self._precompute_parameters()

    def _precompute_parameters(self):
        """Pre-compute normalization parameters on the target device for efficiency."""
        self.norm_params = {}
        self.method_groups = { "standard": [], "log-standard": [], "min-max": [], "log-min-max": [], "none": [] }
        var_to_col = {var: i for i, var in enumerate(self.species_vars + self.global_vars + [self.time_var])}

        for var, method in self.methods.items():
            if method == "none" or var not in self.per_key_stats:
                self.method_groups["none"].append(var)
                continue

            var_stats = self.per_key_stats[var]
            params = {"method": method}

            if "standard" in method:
                mean_key, std_key = ("log_mean", "log_std") if "log" in method else ("mean", "std")
                params["mean"] = torch.tensor(var_stats[mean_key], dtype=torch.float32, device=self.device)
                params["std"] = torch.tensor(var_stats[std_key], dtype=torch.float32, device=self.device)
            elif "min-max" in method:
                params["min"] = torch.tensor(var_stats["min"], dtype=torch.float32, device=self.device)
                params["max"] = torch.tensor(var_stats["max"], dtype=torch.float32, device=self.device)

            self.norm_params[var] = params
            self.method_groups[method].append(var)

        self.col_indices = {method: [var_to_col[var] for var in v_list] for method, v_list in self.method_groups.items() if v_list}

    def normalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """Normalize a complete profile tensor using vectorized operations."""
        if profile.device != self.device:
            profile = profile.to(self.device)
        normalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none": continue
            cols = normalized[:, col_idxs]
            if "standard" in method:
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                data = torch.log10(torch.clamp(cols, min=self.epsilon)) if "log" in method else cols
                normalized[:, col_idxs] = torch.clamp((data - means) / stds, -self.clamp_value, self.clamp_value)
            elif "min-max" in method:
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = torch.clamp(maxs - mins, min=self.epsilon)
                data = torch.log10(torch.clamp(cols, min=self.epsilon)) if "log" in method else cols
                normalized[:, col_idxs] = torch.clamp((data - mins) / ranges, 0.0, 1.0)
        return normalized

    def denormalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """Denormalize a complete profile tensor using vectorized operations."""
        if profile.device != self.device:
            profile = profile.to(self.device)
        denormalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none": continue
            cols = denormalized[:, col_idxs]
            if "standard" in method:
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                raw_vals = cols * stds + means
                if "log" in method:
                    raw_vals = torch.pow(10.0, torch.clamp(raw_vals, min=-38.0, max=38.0))
                denormalized[:, col_idxs] = torch.clamp(raw_vals, min=-3.4e38, max=3.4e38)
            elif "min-max" in method:
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = torch.clamp(maxs - mins, min=self.epsilon)
                raw_vals = cols * ranges + mins
                if "log" in method:
                    raw_vals = torch.pow(10.0, torch.clamp(raw_vals, min=-38.0, max=38.0))
                denormalized[:, col_idxs] = raw_vals
        return denormalized
        
    def denormalize_ratio_predictions(self, standardized_log_ratios: torch.Tensor,
                                    initial_species: torch.Tensor) -> torch.Tensor:
        """Convert standardized log-ratio predictions back to absolute species values."""
        if self.ratio_stats is None:
            raise ValueError("Ratio statistics not available for denormalization.")

        device = standardized_log_ratios.device
        initial_species = initial_species.to(device)

        ratio_means = torch.tensor([self.ratio_stats[var]["mean"] for var in self.species_vars], device=device, dtype=torch.float32)
        ratio_stds = torch.tensor([self.ratio_stats[var]["std"] for var in self.species_vars], device=device, dtype=torch.float32)
        
        log_ratios = (standardized_log_ratios * ratio_stds) + ratio_means
        log_ratios = torch.clamp(log_ratios, min=-38.0, max=38.0)
        
        ratios = torch.pow(10.0, log_ratios)
        predicted_species = initial_species * ratios
        return predicted_species

