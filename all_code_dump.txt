===== /Users/imalsky/Desktop/Chemulator/dump.py =====
#!/usr/bin/env python3
"""
dump_code_and_jsonc.py — Recursively collects all .py and .jsonc files in a directory tree
and writes their paths and contents to a single dump file.

"""

import os
import sys
from pathlib import Path

# Configuration
ROOT_DIR = "."                   # Directory to start searching from
OUTPUT_FILE = "all_code_dump.txt"  # File to write the dump into
EXTENSIONS = (".py", ".jsonc")   # File extensions to include

def dump_files(root_dir: str, output_path: str, exts: tuple) -> None:
    """
    Recursively write every file with a matching extension into *output_path*.

    Symlinks, the output file itself, and unreadable paths are skipped
    to prevent infinite recursion and permission errors.
    """
    import os, sys
    output_abs  = os.path.abspath(output_path)
    seen_dirs   = set()

    with open(output_path, "w", encoding="utf-8") as out:
        for dirpath, _, filenames in os.walk(root_dir, followlinks=False):
            dir_abs = os.path.abspath(dirpath)
            if dir_abs in seen_dirs:
                continue
            seen_dirs.add(dir_abs)

            for name in filenames:
                if not any(name.lower().endswith(ext.lower()) for ext in exts):
                    continue
                file_abs = os.path.abspath(os.path.join(dirpath, name))
                if file_abs == output_abs or os.path.islink(file_abs):
                    continue

                out.write(f"===== {file_abs} =====\n")
                try:
                    with open(file_abs, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as exc:
                    out.write(f"# Could not read file: {type(exc).__name__}: {exc}\n")
                out.write("\n\n")


if __name__ == "__main__":
    # Allow command line arguments for flexibility
    if len(sys.argv) > 1:
        root_dir = sys.argv[1]
    else:
        root_dir = ROOT_DIR
    
    if len(sys.argv) > 2:
        output_file = sys.argv[2]
    else:
        output_file = OUTPUT_FILE
    
    if len(sys.argv) > 3:
        extensions = tuple(sys.argv[3:])
    else:
        extensions = EXTENSIONS
    
    dump_files(root_dir, output_file, extensions)
    print(f"Dumped all {', '.join(extensions)} files under '{root_dir}' into '{output_file}'")

===== /Users/imalsky/Desktop/Chemulator/config/config.jsonc =====
{
  // ════════════════════════════ 1. PATHS ════════════════════════════
  // Directories & file lists used throughout the pipeline
  "paths": {
    // List of raw HDF5 files (absolute or relative). Order is irrelevant.
    "raw_data_files": [
      "data/raw/run8001-result.h5",
      "data/raw/run8002-result.h5",
      "data/raw/run8003-result.h5",
      "data/raw/run8004-result.h5",
      "data/raw/run8005-result.h5",
      "data/raw/run8006-result.h5",
      "data/raw/run8007-result.h5",
      "data/raw/run8008-result.h5",
      "data/raw/run8009-result.h5"
    ],

    // Where pre‑processed HDF5 file goes
    "processed_data_dir": "data/processed",
    
    // Preprocessed HDF5 filename
    "processed_hdf5_file": "preprocessed_data.h5",

    // Parent dir for run‑specific model folders
    "model_save_dir": "data/models",

    // Central log directory (one file per run inside)
    "log_dir": "logs"
  },

  // ════════════════════════════ 2. DATA ════════════════════════════
  // Dataset layout & processing settings
  "data": {
    "species_variables": [
      "C2H2_evolution",
      "CH4_evolution",
      "CO2_evolution",
      "CO_evolution",
      "H2O_evolution",
      "H2_evolution",
      "HCN_evolution",
      "H_evolution",
      "N2_evolution",
      "NH3_evolution",
      "OH_evolution",
      "O_evolution"
    ],
    "global_variables": ["P_init","T_init"],

    // Name of the time column
    "time_variable": "t_time",

    // HDF5 read/write chunk (rows) – aligned with typical batch sizes
    "chunk_size": 4096,
    
    // HDF5 compression settings
    "compression": "gzip",
    "compression_level": 9
  },

  // ═══════════════════════ 3. NORMALISATION ════════════════════════
  // Preprocessing‑only; **never** used at run‑time inference
  "normalization": {
    // Fallback for any var not listed in "methods"
    // Allowed: "standard" | "log-standard" | "log-min-max" | "symlog" | "none"
    "default_method": "log-min-max",

    // Per‑variable overrides (omit to inherit default_method)
    "methods": {
      "T_init": "standard",
      "P_init": "log-min-max",
      "t_time": "log-min-max"
    },

    // Numerical safety knobs – see DataNormalizer
    "epsilon": 1e-37,          // floor added before log ops
    "min_std": 1e-10,          // lower‑bound on σ to avoid divide‑by‑zero
    "symlog_percentile": 0.1,  // threshold as |x|‑percentile for symlog
    "clamp_value": 50.0        // post‑norm clipping limit
  },

  // ═══════════════════════════ 4. MODEL ════════════════════════════
  // Architecture‑specific knobs.  **Unused keys are ignored**.
  "model": {
    // "siren" | "resnet" | "deeponet"
    "type": "deeponet",

    // ── shared / optional ───────────────────────────────────────────
    "activation": "gelu",          // "gelu" | "relu" | "tanh"
    "dropout": 0.0,                // 0.0‑1.0 (only if you add dropout layers)
    "use_time_embedding": false,   // (future feature)
    "time_embedding_dim": 256,
    "output_scale": 1.0,           // scalar multiplier on final layer
    "use_residual": true,          // add Δy onto y₀ (DeepONet only)

    // ── SIREN / ResNet common ───────────────────────────────────────
    "hidden_dims": [256, 256, 256],

    // ── DeepONet specific ───────────────────────────────────────────
    "branch_layers": [256, 256, 256],
    "trunk_layers":  [64, 64, 64],
    "basis_dim": 64
  },

  // ═════════════════════════ 5. TRAINING ═══════════════════════════
  // Everything the Trainer consumes
  "training": {
    // ── dataset split ───────────────────────────────────────────────
    "val_fraction":  0.15,   // 0‑1 – must leave ≥0 for train
    "test_fraction": 0.15,   // 0‑1
    "use_fraction":  1.0,    // subsample for quick tests

    // ── global schedule ────────────────────────────────────────────
    "epochs": 100,
    "batch_size": 4096,
    "gradient_accumulation_steps": 4,

    // ── DataLoader knobs (overrides hardware auto‑tuning) ──────────
    "num_workers": 16,
    "pin_memory": true,
    "persistent_workers": true,
    "prefetch_factor": 4,   // set null to let code decide
    "drop_last": true,

    // ── optimisation ───────────────────────────────────────────────
    "learning_rate": 1e-4,
    "weight_decay": 1e-5,
    "gradient_clip": 1.0,

    // ── LR scheduler ───────────────────────────────────────────────
    // "plateau" | "cosine"
    "scheduler": "cosine",

    //   • For "plateau": factor, patience, min_lr
    //   • For "cosine" : T_0 (epochs), T_mult, eta_min
    "scheduler_params": {
      // Plateau defaults
      //factor": 0.5,
      //"patience": 5,
      //"min_lr": 1e-10

      //Cosine example:
      "T_0": 20,
      "T_mult": 2,
      "eta_min": 1e-8
    },

    // ── loss ───────────────────────────────────────────────────────
    // "mse" | "huber"
    "loss": "mse",
    "huber_delta": 0.25,

    // ── mixed precision ────────────────────────────────────────────
    "use_amp": true,
    // "float16" (needs GradScaler on CUDA) | "bfloat16"
    "amp_dtype": "bfloat16",

    // ── early stopping & logging ───────────────────────────────────
    "early_stopping_patience": 30,
    "min_delta": 1e-10,        // improvement threshold
    "log_interval": 10,        // batches
    "save_interval": 20       // epochs
  },

  // ════════════════════════ 6. OPTUNA ═════════════════════════════
  // Hyperparameter search configuration
  "optuna": {
    "enabled": true,              // Set to true to enable hyperparameter search
    "n_trials": 50,               // Number of trials to run
    "n_jobs": 4,                   // Number of parallel jobs
    "study_name": "chemulator_optimization",
    "direction": "minimize",       // minimize validation loss
    "sampler": "TPE",             // TPE, Random, or Grid
    "pruner": "HyperbandPruner",     // MedianPruner, HyperbandPruner, or None
    
    // Search spaces - each parameter can be:
    // - {"type": "float", "low": min, "high": max, "log": true/false}
    // - {"type": "int", "low": min, "high": max}
    // - {"type": "categorical", "choices": [...]}
    // - {"type": "fixed", "value": ...} (not searched)
    "search_space": {
      // Model architecture search
      "model.basis_dim": {
        "type": "categorical",
        "choices": [32, 64, 128, 256]
      },
      "model.branch_layers": {
        "type": "categorical", 
        "choices": [
          [128, 128, 128],
          [256, 256, 256], 
          [512, 256, 128],
          [256, 256, 256, 256]
        ]
      },
      "model.trunk_layers": {
        "type": "categorical",
        "choices": [
          [32, 32, 32],
          [64, 64, 64],
          [128, 64, 32]
        ]
      },
      "model.activation": {
        "type": "categorical",
        "choices": ["gelu", "tanh"]
      },
      
      // Training hyperparameters
      "training.learning_rate": {
        "type": "float",
        "low": 1e-5,
        "high": 5e-4,
        "log": true
      },
      "training.weight_decay": {
        "type": "float", 
        "low": 1e-7,
        "high": 1e-3,
        "log": true
      },
      "training.batch_size": {
        "type": "categorical",
        "choices": [1024, 2048, 4096, 8192]
      },
      "training.gradient_accumulation_steps": {
        "type": "categorical",
        "choices": [1, 2, 4, 8]
      },
      
      // Keep some parameters fixed
      "training.epochs": {
        "type": "fixed",
        "value": 50  // Shorter for hyperparameter search
      }
    }
  },

  // ═════════════════════════ 7. SYSTEM ════════════════════════════
  "system": {
    "seed": 42,                       // global RNG seed

    // torch.compile & friends
    "use_torch_compile": true,
    "compile_mode": "default",        // "default" | "max-autotune"
    "compile_fullgraph": false,
    "compile_dynamic_shapes": false,

    // Memory‑saving / correctness helpers
    "gradient_checkpointing": true,
    "detect_anomaly": false,

    // Model export helpers
    "use_torch_export": true,        

    // CUDA / cuDNN micro‑tuning
    "cudnn_benchmark": true,
    "tf32": true,                     // enable TF32 matmuls on Ampere+

    // Cache management inside training loop
    "empty_cache_interval": 10000        // batches between torch.cuda.empty_cache()
  }
}

===== /Users/imalsky/Desktop/Chemulator/src/optuna_optimization.py =====

#!/usr/bin/env python3
"""
Hyperparameter optimization using Optuna for chemical kinetics models.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional
import copy

import optuna
from optuna.pruners import MedianPruner, HyperbandPruner
from optuna.samplers import TPESampler, RandomSampler, GridSampler
import torch
import numpy as np

from models.model import create_model
from training.trainer import Trainer
from data.dataset import HDF5Dataset, create_dataloader
from utils.hardware import setup_device, optimize_hardware
from utils.utils import seed_everything, save_json, load_json


class OptunaOptimizer:
    """
    Hyperparameter optimizer using Optuna for chemical kinetics models.
    """
    
    def __init__(
        self,
        base_config: Dict[str, Any],
        preprocessed_hdf5_path: Path,
        save_dir: Path,
        device: torch.device
    ):
        self.base_config = base_config
        self.hdf5_path = preprocessed_hdf5_path
        self.save_dir = save_dir
        self.device = device
        
        self.logger = logging.getLogger(__name__)
        
        # Get optuna configuration
        self.optuna_config = base_config["optuna"]
        
        # Create study directory
        self.study_dir = save_dir / "optuna_study"
        self.study_dir.mkdir(parents=True, exist_ok=True)
        
        # Load normalization stats for dataset creation
        norm_path = preprocessed_hdf5_path.parent / "normalization.json"
        if norm_path.exists():
            self.norm_stats = load_json(norm_path)
        else:
            self.logger.warning("Normalization stats not found")
            self.norm_stats = None
    
    def _parse_search_space(self, trial: optuna.Trial, search_space: Dict[str, Any]) -> Dict[str, Any]:
        """Parse search space configuration and suggest values."""
        suggested_params = {}
        
        for param_path, param_config in search_space.items():
            param_type = param_config["type"]
            
            if param_type == "fixed":
                value = param_config["value"]
            elif param_type == "float":
                value = trial.suggest_float(
                    param_path,
                    param_config["low"],
                    param_config["high"],
                    log=param_config.get("log", False)
                )
            elif param_type == "int":
                value = trial.suggest_int(
                    param_path,
                    param_config["low"],
                    param_config["high"]
                )
            elif param_type == "categorical":
                value = trial.suggest_categorical(
                    param_path,
                    param_config["choices"]
                )
            else:
                raise ValueError(f"Unknown parameter type: {param_type}")
            
            suggested_params[param_path] = value
        
        return suggested_params
    
    def _update_config_with_params(self, config: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """Update configuration dictionary with suggested parameters."""
        config = copy.deepcopy(config)
        
        for param_path, value in params.items():
            # Split path (e.g., "model.learning_rate" -> ["model", "learning_rate"])
            keys = param_path.split(".")
            
            # Navigate to the correct location in config
            current = config
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            
            # Set the value
            current[keys[-1]] = value
        
        return config
    
    def _create_datasets(self, config: Dict[str, Any]) -> tuple:
        """Create datasets for training."""
        train_dataset = HDF5Dataset(
            hdf5_path=self.hdf5_path,
            split_name="train",
            config=config,
            device=self.device
        )
        
        val_dataset = HDF5Dataset(
            hdf5_path=self.hdf5_path,
            split_name="validation",
            config=config,
            device=self.device
        )
        
        # For hyperparameter search, we typically don't use test set
        # to avoid overfitting hyperparameters to test performance
        return train_dataset, val_dataset
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Objective function for Optuna optimization.
        
        Args:
            trial: Optuna trial object
            
        Returns:
            Validation loss to minimize
        """
        # Log trial start
        self.logger.info(f"Starting trial {trial.number}")
        trial_start = time.time()
        
        # Suggest hyperparameters
        suggested_params = self._parse_search_space(
            trial, 
            self.optuna_config["search_space"]
        )
        
        # Update configuration
        trial_config = self._update_config_with_params(
            self.base_config,
            suggested_params
        )
        
        # Set seed for reproducibility
        seed = self.base_config["system"]["seed"] + trial.number
        seed_everything(seed)
        
        # Create model
        model = create_model(trial_config, self.device)
        
        # Create datasets
        train_dataset, val_dataset = self._create_datasets(trial_config)
        
        # Create trial-specific save directory
        trial_save_dir = self.study_dir / f"trial_{trial.number:04d}"
        trial_save_dir.mkdir(exist_ok=True)
        
        # Save trial configuration
        save_json(trial_config, trial_save_dir / "config.json")
        save_json(suggested_params, trial_save_dir / "suggested_params.json")
        
        # Initialize trainer with pruning callback
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=None,  # Don't use test set during hyperparameter search
            config=trial_config,
            save_dir=trial_save_dir,
            device=self.device
        )
        
        # Training with pruning
        best_val_loss = float('inf')
        pruned = False
        
        try:
            # Modified training loop to support pruning
            for epoch in range(1, trial_config["training"]["epochs"] + 1):
                trainer.current_epoch = epoch
                
                # Train one epoch
                train_loss, train_metrics = trainer._train_epoch()
                
                # Validate
                val_loss, val_metrics = trainer._validate()
                
                # Update scheduler
                if not trainer.scheduler_step_on_batch:
                    trainer.scheduler.step(val_loss)
                
                # Track best loss
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                
                # Report to Optuna and check for pruning
                trial.report(val_loss, epoch)
                
                if trial.should_prune():
                    self.logger.info(f"Trial {trial.number} pruned at epoch {epoch}")
                    pruned = True
                    raise optuna.TrialPruned()
                
                # Early stopping check
                if hasattr(trainer, 'patience_counter'):
                    if trainer.patience_counter >= trainer.early_stopping_patience:
                        self.logger.info(f"Trial {trial.number} early stopped at epoch {epoch}")
                        break
        
        except optuna.TrialPruned:
            pruned = True
            raise
        
        except Exception as e:
            self.logger.error(f"Trial {trial.number} failed: {e}")
            raise
        
        finally:
            # Save trial results
            trial_time = time.time() - trial_start
            results = {
                "trial_number": trial.number,
                "best_val_loss": best_val_loss,
                "pruned": pruned,
                "suggested_params": suggested_params,
                "trial_time": trial_time
            }
            save_json(results, trial_save_dir / "results.json")
            
            # Clean up GPU memory
            del model
            del trainer
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        return best_val_loss
    
    def optimize(self) -> optuna.Study:
        """
        Run hyperparameter optimization.
        
        Returns:
            Optuna study object
        """
        self.logger.info("="*80)
        self.logger.info("Starting Optuna hyperparameter optimization")
        self.logger.info(f"Study name: {self.optuna_config['study_name']}")
        self.logger.info(f"N trials: {self.optuna_config['n_trials']}")
        self.logger.info(f"Direction: {self.optuna_config['direction']}")
        
        # Create sampler
        sampler_type = self.optuna_config.get("sampler", "TPE")
        if sampler_type == "TPE":
            sampler = TPESampler(seed=self.base_config["system"]["seed"])
        elif sampler_type == "Random":
            sampler = RandomSampler(seed=self.base_config["system"]["seed"])
        elif sampler_type == "Grid":
            sampler = GridSampler(self._create_grid_search_space())
        else:
            raise ValueError(f"Unknown sampler: {sampler_type}")
        
        # Create pruner
        pruner_type = self.optuna_config.get("pruner", "MedianPruner")
        if pruner_type == "MedianPruner":
            pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=10)
        elif pruner_type == "HyperbandPruner":
            pruner = HyperbandPruner()
        elif pruner_type == "None" or pruner_type is None:
            pruner = None
        else:
            raise ValueError(f"Unknown pruner: {pruner_type}")
        
        # Create or load study
        study_path = self.study_dir / "study.db"
        storage = f"sqlite:///{study_path}"
        
        study = optuna.create_study(
            study_name=self.optuna_config["study_name"],
            direction=self.optuna_config["direction"],
            sampler=sampler,
            pruner=pruner,
            storage=storage,
            load_if_exists=True
        )
        
        # Run optimization
        optimization_start = time.time()
        
        study.optimize(
            self.objective,
            n_trials=self.optuna_config["n_trials"],
            n_jobs=self.optuna_config.get("n_jobs", 1),
            gc_after_trial=True
        )
        
        optimization_time = time.time() - optimization_start
        
        # Log results
        self.logger.info("="*80)
        self.logger.info("Optimization completed!")
        self.logger.info(f"Total time: {optimization_time/3600:.2f} hours")
        self.logger.info(f"Best trial: {study.best_trial.number}")
        self.logger.info(f"Best value: {study.best_value:.6f}")
        self.logger.info("Best parameters:")
        for param, value in study.best_params.items():
            self.logger.info(f"  {param}: {value}")
        
        # Save study results
        study_results = {
            "best_trial": study.best_trial.number,
            "best_value": study.best_value,
            "best_params": study.best_params,
            "n_trials": len(study.trials),
            "optimization_time": optimization_time
        }
        save_json(study_results, self.study_dir / "study_results.json")
        
        # Save best configuration
        best_config = self._update_config_with_params(
            self.base_config,
            study.best_params
        )
        save_json(best_config, self.study_dir / "best_config.json")
        
        return study
    
    def _create_grid_search_space(self) -> Dict[str, Any]:
        """Create grid search space from configuration."""
        grid_space = {}
        
        for param_path, param_config in self.optuna_config["search_space"].items():
            if param_config["type"] == "categorical":
                grid_space[param_path] = param_config["choices"]
            elif param_config["type"] == "fixed":
                grid_space[param_path] = [param_config["value"]]
            else:
                self.logger.warning(
                    f"Grid search only supports categorical parameters, "
                    f"skipping {param_path}"
                )
        
        return grid_space

===== /Users/imalsky/Desktop/Chemulator/src/main.py =====
#!/usr/bin/env python3
"""
Main entry point for chemical kinetics neural network training.
Supports both standard training and hyperparameter optimization with Optuna.
Uses chunked HDF5 format for efficient data storage and loading.
"""

import json
import logging
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional
import os
import hashlib
import warnings
import platform

# Check for OpenMP library conflicts on macOS
if platform.system() == "Darwin" and "KMP_DUPLICATE_LIB_OK" not in os.environ:
    # Only set this on macOS where the issue commonly occurs
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    warnings.warn(
        "Setting KMP_DUPLICATE_LIB_OK=TRUE for macOS compatibility. "
        "This may mask underlying library conflicts."
    )

import torch
import numpy as np

# Local imports
from utils.hardware import setup_device, optimize_hardware
from utils.utils import (
    setup_logging, 
    seed_everything, 
    ensure_directories,
    load_json_config,
    save_json
)
from data.preprocessor import DataPreprocessor
from data.dataset import HDF5Dataset
from models.model import create_model
from training.trainer import Trainer

# Only suppress specific known warnings
warnings.filterwarnings("ignore", message=".*torch.compile.*", category=UserWarning)
warnings.filterwarnings("ignore", message=".*flash attention.*", category=UserWarning)


def compute_config_hash(config: Dict[str, Any]) -> str:
    """
    Compute a hash of the configuration for cache validation.
    """
    # Extract all parts that affect data preprocessing
    relevant_config = {
        "raw_data_files": sorted(config["paths"]["raw_data_files"]),  # Sort for consistency
        "data": config["data"],
        "normalization": config["normalization"],
        "training": {
            "val_fraction": config["training"]["val_fraction"],
            "test_fraction": config["training"]["test_fraction"],
            "use_fraction": config["training"]["use_fraction"]
        }
    }
    
    # Convert to JSON string for hashing
    config_str = json.dumps(relevant_config, sort_keys=True)
    
    # Compute SHA256 hash
    return hashlib.sha256(config_str.encode()).hexdigest()[:16]


class ChemicalKineticsPipeline:
    """
    Complete training pipeline for chemical kinetics prediction.
    Uses chunked HDF5 format for optimal performance.
    """
    
    def __init__(self, config_path: Path):
        """Initialize the pipeline with configuration."""
        # Load configuration
        self.config = load_json_config(config_path)
        
        # Setup paths
        self.setup_paths()
        
        # Setup logging
        log_file = self.log_dir / f"pipeline_{time.strftime('%Y%m%d_%H%M%S')}.log"
        setup_logging(log_file=log_file)
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("="*80)
        self.logger.info("Chemical Kinetics Neural Network Pipeline")
        self.logger.info(f"Configuration: {config_path}")
        
        # Set random seed for reproducibility
        seed = self.config["system"]["seed"]
        seed_everything(seed)
        
        # Setup hardware
        self.device = setup_device()
        optimize_hardware(self.config["system"], self.device)
        
        self.logger.info("Pipeline initialized with chunked HDF5 format for high-performance data loading")
        
    def setup_paths(self):
        """Create directory structure and setup paths."""
        paths = self.config["paths"]
        
        # Create run-specific directory
        model_type = self.config["model"]["type"]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.run_save_dir = Path(paths["model_save_dir"]) / f"trained_model_{model_type}_{timestamp}"
        
        # Convert to Path objects
        self.raw_data_files = [Path(f) for f in paths["raw_data_files"]]
        
        # Use environment variable if set, otherwise use config
        self.processed_dir = Path(os.getenv("PROCESSED_DATA_DIR", paths["processed_data_dir"]))
        self.log_dir = Path(paths["log_dir"])
        
        # Create directories
        ensure_directories(self.processed_dir, self.run_save_dir, self.log_dir)
        
        # Define processed data paths
        self.processed_hdf5_file = self.processed_dir / paths.get("processed_hdf5_file", "preprocessed_data.h5")
        self.normalization_file = self.processed_dir / "normalization.json"
        self.config_hash_file = self.processed_dir / "config_hash.txt"
        
    def preprocess_data(self) -> bool:
        """
        Pre-process raw files to normalized chunked HDF5.
        """
        cfg_hash = compute_config_hash(self.config)
        
        # Check if preprocessed data already exists
        cache_ok = (
            self.processed_hdf5_file.exists()
            and self.normalization_file.exists()
            and self.config_hash_file.exists()
            and self.config_hash_file.read_text().strip() == cfg_hash
        )
        
        if cache_ok:
            self.logger.info("Using cached HDF5 data and normalization.")
            return False
        
        # Check raw files exist
        missing = [p for p in self.raw_data_files if not p.exists()]
        if missing:
            raise FileNotFoundError(f"Missing raw data files: {missing}")
        
        # Process to HDF5 with integrated normalization
        self.logger.info("Starting HDF5 creation from raw files with normalization...")
        
        preprocessor = DataPreprocessor(
            raw_files=self.raw_data_files,
            output_dir=self.processed_dir,
            config=self.config
        )
        
        dataset_info = preprocessor.process_to_hdf5()
        
        # Check for errors
        if "error" in dataset_info:
            self.logger.error(f"Preprocessing failed: {dataset_info['error']}")
            sys.exit(1)
        
        # Check if any data was processed
        total_samples = dataset_info["total_samples"]
        if total_samples == 0:
            self.logger.error("No valid data processed from raw files. Exiting.")
            sys.exit(1)
        
        # Save config hash
        self.config_hash_file.write_text(cfg_hash)
        
        self.logger.info("Pre-processing complete.")
        return True
        
    def train_model(self):
        """Train the neural network model."""
        self.logger.info("Starting model training...")
        
        # Save the exact config used for this run
        save_json(self.config, self.run_save_dir / "run_config.json")

        # Load normalization stats (for potential denorm in inference)
        with open(self.normalization_file, 'r') as f:
            norm_stats = json.load(f)
        
        # Create model
        model = create_model(self.config, self.device)
        
        # Log model info
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"Model: {self.config['model']['type'].upper()}")
        self.logger.info(f"Parameters: {total_params:,}")
        
        # Create datasets
        self.logger.info("Creating HDF5 datasets for high-performance loading...")
        
        train_dataset = HDF5Dataset(
            hdf5_path=self.processed_hdf5_file,
            split_name="train",
            config=self.config,
            device=self.device
        )
        
        val_dataset = HDF5Dataset(
            hdf5_path=self.processed_hdf5_file,
            split_name="validation",
            config=self.config,
            device=self.device
        )
        
        test_dataset = HDF5Dataset(
            hdf5_path=self.processed_hdf5_file,
            split_name="test",
            config=self.config,
            device=self.device
        )
        
        # Log dataset info
        train_info = train_dataset.get_batch_info()
        self.logger.info(f"HDF5 Dataset info: {train_info}")
        
        # Initialize trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device
        )
        
        # Train model
        best_val_loss = trainer.train()
        
        # Evaluate on test set
        test_loss = trainer.evaluate_test()
        
        self.logger.info(f"Training complete! Best validation loss: {best_val_loss:.6f}")
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        # Save final results
        results = {
            "config_path": str(self.run_save_dir / "run_config.json"),
            "best_val_loss": best_val_loss,
            "test_loss": test_loss,
            "model_path": str(self.run_save_dir / "best_model.pt"),
            "training_time": trainer.total_training_time,
            "data_format": "hdf5_chunked"
        }
        
        save_json(results, self.run_save_dir / "results.json")
    
    def optimize_hyperparameters(self):
        """Run hyperparameter optimization using Optuna."""
        if not self.config["optuna"]["enabled"]:
            self.logger.info("Optuna optimization not enabled in config")
            return
        
        # Import here to avoid dependency if not using Optuna
        try:
            from optuna_optimization import OptunaOptimizer
        except ImportError:
            self.logger.error("Optuna not installed. Install with: pip install optuna")
            sys.exit(1)
        
        self.logger.info("Starting hyperparameter optimization with Optuna...")
        
        # Ensure data is preprocessed
        self.preprocess_data()
        
        # Create optimizer
        optimizer = OptunaOptimizer(
            base_config=self.config,
            preprocessed_hdf5_path=self.processed_hdf5_file,
            save_dir=self.run_save_dir,
            device=self.device
        )
        
        # Run optimization
        study = optimizer.optimize()
        
        # Train final model with best parameters
        self.logger.info("Training final model with best hyperparameters...")
        
        # Load best config
        best_config_path = self.run_save_dir / "optuna_study" / "best_config.json"
        self.config = load_json_config(best_config_path)
        
        # Train with best config
        self.train_model()
    
    def run(self, mode: str = "train"):
        """
        Execute the pipeline.
        
        Args:
            mode: "train" for standard training, "optimize" for hyperparameter optimization
        """
        try:
            # Step 1: Preprocess data (if needed)
            self.preprocess_data()
            
            # Step 2: Train or optimize
            if mode == "optimize":
                self.optimize_hyperparameters()
            else:
                self.train_model()
            
            self.logger.info("="*80)
            self.logger.info("Pipeline completed successfully!")
            self.logger.info(f"Results saved in: {self.run_save_dir}")
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            sys.exit(1)


def main():
    """Main entry point."""
    import argparse
    parser = argparse.ArgumentParser(
        description="Chemical Kinetics Neural Network Training Pipeline"
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/config.jsonc"),
        help="Path to configuration file (default: config/config.jsonc)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["train", "optimize"],
        default="train",
        help="Mode: train (standard training) or optimize (hyperparameter optimization)"
    )
    
    args = parser.parse_args()
    
    # Validate config path
    if not args.config.exists():
        print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
        sys.exit(1)
    
    # Check for json5 if using .jsonc file
    if args.config.suffix == ".jsonc":
        try:
            import json5
        except ImportError:
            print("Error: JSON with comments (.jsonc) requires json5 package", file=sys.stderr)
            print("Install it with: pip install json5", file=sys.stderr)
            sys.exit(1)
    
    # Run pipeline
    pipeline = ChemicalKineticsPipeline(args.config)
    pipeline.run(mode=args.mode)


if __name__ == "__main__":
    main()

===== /Users/imalsky/Desktop/Chemulator/src/training/trainer.py =====
#!/usr/bin/env python3
"""
Optimized training pipeline for chemical kinetics models.

Features:
- Mixed precision training with BFloat16 on A100
- Gradient accumulation for large effective batch sizes
- Advanced learning rate scheduling
- Memory-efficient training with periodic cache clearing
- Comprehensive logging
- Compatible with all device types (CUDA, MPS, CPU)
- Full CUDA graph support with proper step marking
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple
import sys

import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau
from torch.utils.data import DataLoader

from data.dataset import HDF5Dataset, create_dataloader
from models.model import export_model  # Updated to modern export
from data.device_prefetch import DevicePrefetchLoader


# Training constants
DEFAULT_BETAS = (0.9, 0.999)
DEFAULT_EPS = 1e-8
LOGGING_INTERVAL_SECONDS = 30.0
SCHEDULER_PLATEAU_FACTOR = 0.5
SCHEDULER_PLATEAU_PATIENCE = 10
SCHEDULER_PLATEAU_MIN_LR = 1e-7
DEFAULT_EMPTY_CACHE_INTERVAL = 200  # Increased to reduce overhead on A100


class Trainer:
    """
    Optimised trainer for chemical-kinetics networks.
    """
    def __init__(               
        self,
        model: nn.Module,
        train_dataset: HDF5Dataset,
        val_dataset: HDF5Dataset,
        test_dataset: HDF5Dataset,
        config: Dict[str, Any],
        save_dir: Path,
        device: torch.device,
    ):
        self.logger = logging.getLogger(__name__)

        self.model          = model
        self.config         = config
        self.save_dir       = save_dir
        self.device         = device
        self.train_config   = config["training"]
        self.system_config  = config["system"]

        # Data loaders
        base_train = create_dataloader(train_dataset, config, shuffle=True,  device=device)
        base_val   = create_dataloader(val_dataset,   config, shuffle=False, device=device)
        base_test  = create_dataloader(test_dataset,  config, shuffle=False, device=device)

        self.logger.info("Using pre-normalized data - direct GPU streaming enabled")
        
        try:
            from data.device_prefetch import DevicePrefetchLoader
            use_prefetch = device.type == "cuda"
        except ModuleNotFoundError:
            self.logger.warning("DevicePrefetchLoader not found - falling back to plain loaders")
            use_prefetch = False

        if use_prefetch:
            self.train_loader = DevicePrefetchLoader(base_train, device)
            self.val_loader = DevicePrefetchLoader(base_val, device)
            self.test_loader = DevicePrefetchLoader(base_test, device)
            self.logger.info("GPU pre-fetch enabled")
        else:
            self.train_loader = base_train
            self.val_loader = base_val
            self.test_loader = base_test

        # Misc setup
        self.log_interval            = self.train_config.get("log_interval", 10)
        self.current_epoch           = 0
        self.global_step             = 0
        self.best_val_loss           = float("inf")
        self.best_epoch              = -1
        self.total_training_time     = 0
        self.patience_counter        = 0
        self.early_stopping_patience = self.train_config["early_stopping_patience"]
        self.min_delta               = self.train_config["min_delta"]
        self.empty_cache_interval    = self.system_config.get("empty_cache_interval", DEFAULT_EMPTY_CACHE_INTERVAL)

        self.log_file         = self.save_dir / "training_log.json"
        self.training_history = {"config": config, "epochs": []}
        self.max_history_epochs     = 1_000
        self.history_save_interval  = 100

        self._setup_optimizer()
        self._setup_scheduler()
        self._setup_loss()
        self._setup_amp()
        self._cuda_graphs_enabled = False
        self._check_cuda_graphs_compatibility()
        
    def _check_cuda_graphs_compatibility(self):
        """Check if we should use CUDA graphs based on the configuration and device."""
        if (self.device.type == "cuda" and 
            self.system_config.get("use_torch_compile", False) and 
            self.train_config.get("use_amp", False) and
            self.system_config.get("compile_mode") == "max-autotune"):
            self._cuda_graphs_enabled = True
            self.logger.info("CUDA graphs compatibility mode enabled")
            
            # Import the cudagraph marking function with fallback
            try:
                from torch._inductor import cudagraph_mark_step_begin
                self.cudagraph_mark_step = cudagraph_mark_step_begin
            except ImportError:
                try:
                    from torch._dynamo import mark_step_begin
                    self.cudagraph_mark_step = mark_step_begin
                except ImportError:
                    self.logger.warning("CUDA graph step marking not available, disabling")
                    self._cuda_graphs_enabled = False
        else:
            self._cuda_graphs_enabled = False
    
    def _setup_optimizer(self):
        """Setup AdamW optimizer with weight decay."""
        # Separate parameters for weight decay
        decay_params = []
        no_decay_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            # Don't apply weight decay to biases, layer norm, and embeddings
            if param.dim() == 1 or "bias" in name or "norm" in name.lower() or "embed" in name.lower():
                no_decay_params.append(param)
            else:
                decay_params.append(param)
        
        param_groups = [
            {"params": decay_params, "weight_decay": self.train_config["weight_decay"]},
            {"params": no_decay_params, "weight_decay": 0.0}
        ]
        
        self.optimizer = AdamW(
            param_groups,
            lr=self.train_config["learning_rate"],
            betas=DEFAULT_BETAS,
            eps=DEFAULT_EPS
        )
        
        self.logger.info(
            f"Optimizer: AdamW with lr={self.train_config['learning_rate']:.2e}, "
            f"weight_decay={self.train_config['weight_decay']:.2e}"
        )
    
    def _setup_scheduler(self):
        """Setup learning rate scheduler."""
        scheduler_type = self.train_config["scheduler"]
        
        if scheduler_type == "cosine":
            params = self.train_config["scheduler_params"]
            
            # Calculate T_0 in steps - ensure at least 1
            steps_per_epoch = max(1, len(self.train_loader) // self.train_config["gradient_accumulation_steps"])
            T_0 = max(1, params["T_0"] * steps_per_epoch)
            
            self.scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=T_0,
                T_mult=params["T_mult"],
                eta_min=params["eta_min"]
            )
            self.scheduler_step_on_batch = True
            
        elif scheduler_type == "plateau":
            params = self.train_config.get("scheduler_params", {})
            
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                factor=params.get("factor", SCHEDULER_PLATEAU_FACTOR),
                patience=params.get("patience", SCHEDULER_PLATEAU_PATIENCE),
                min_lr=params.get("min_lr", SCHEDULER_PLATEAU_MIN_LR)
            )
            self.scheduler_step_on_batch = False
        
        else:
            raise ValueError(f"Unknown scheduler: {scheduler_type}")
        
        self.logger.info(f"Scheduler: {scheduler_type}")
    
    def _setup_loss(self):
        """Setup loss function."""
        loss_type = self.train_config["loss"]
        
        if loss_type == "mse":
            self.criterion = nn.MSELoss()
        elif loss_type == "huber":
            self.criterion = nn.HuberLoss(delta=self.train_config["huber_delta"])
        else:
            raise ValueError(f"Unknown loss: {loss_type}")
        
        self.logger.info(f"Loss function: {loss_type}")
    
    def _setup_amp(self):
        self.use_amp = self.train_config.get("use_amp", False) and self.device.type in ("cuda", "mps")
        self.scaler = None
        self.amp_dtype = None

        if not self.use_amp:
            self.logger.info("AMP disabled")
            return

        dtype_str = str(self.train_config.get("amp_dtype", "float16")).lower()
        if dtype_str not in ("float16", "bfloat16"):
            raise ValueError(f"Invalid amp_dtype '{dtype_str}' – choose 'float16' or 'bfloat16'.")

        self.amp_dtype = torch.bfloat16 if dtype_str == "bfloat16" else torch.float16

        # GradScaler only meaningful for fp16 on CUDA
        if self.amp_dtype is torch.float16 and self.device.type == "cuda":
            self.scaler = GradScaler()
        elif self.device.type == "mps" and self.amp_dtype is torch.float16:
            self.logger.warning("GradScaler not supported on MPS – continuing without it.")

        self.logger.info("AMP enabled (dtype=%s, device=%s)", dtype_str, self.device.type)
        
    def train(self) -> float:
        """
        Execute the training loop.
        
        Returns:
            Best validation loss achieved
        """
        self.logger.info("Starting training...")
        self.logger.info(f"Train batches: {len(self.train_loader)}")
        self.logger.info(f"Val batches: {len(self.val_loader)}")
        
        if len(self.train_loader) == 0:
            self.logger.error("Training dataset is empty! Check data splits.")
            sys.exit(1)
        
        try:
            for epoch in range(1, self.train_config["epochs"] + 1):
                self.current_epoch = epoch
                epoch_start = time.time()
                
                # Training phase
                train_loss, train_metrics = self._train_epoch()
                
                # Validation phase
                val_loss, val_metrics = self._validate()
                
                # Update scheduler
                if not self.scheduler_step_on_batch:
                    self.scheduler.step(val_loss)
                
                # Track time
                epoch_time = time.time() - epoch_start
                self.total_training_time += epoch_time
                
                # Log results
                self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)
                
                # Check for improvement
                if val_loss < self.best_val_loss - self.min_delta:
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1
                
                # Early stopping
                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            
            self.logger.info(
                f"Training completed. Best validation loss: {self.best_val_loss:.6f} "
                f"at epoch {self.best_epoch}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
        
        except Exception as e:
            self.logger.error(f"Training failed: {e}", exc_info=True)
            raise
        
        finally:
            # Save training history
            with open(self.log_file, 'w') as f:
                json.dump(self.training_history, f, indent=2)
        
        return self.best_val_loss
    
    def _train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """
        One training epoch with gradient accumulation.
        Ensures all batches contribute to gradient updates.
        """
        self.model.train()
        total_loss, total_samples = 0.0, 0
        grad_norm_ema = 0.0
        ema_decay = 0.98
        accumulation_steps = self.train_config["gradient_accumulation_steps"]
        last_log_time = time.time()
        accumulated_loss = 0.0
        accumulated_batches = 0

        if self._cuda_graphs_enabled and hasattr(self, "cudagraph_mark_step"):
            self.cudagraph_mark_step()

        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            with autocast(enabled=self.use_amp, device_type=self.device.type, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets) / accumulation_steps

            (self.scaler.scale(loss) if self.scaler else loss).backward()
            accumulated_loss += loss.item() * accumulation_steps
            accumulated_batches += 1

            # Perform optimizer step at accumulation boundary or last batch
            is_accumulation_boundary = (batch_idx + 1) % accumulation_steps == 0
            is_last_batch = (batch_idx + 1) == len(self.train_loader)
            
            if is_accumulation_boundary or is_last_batch:
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)

                grad = nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.train_config["gradient_clip"]
                )
                grad_norm_ema = ema_decay * grad_norm_ema + (1 - ema_decay) * float(grad)

                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()

                self.optimizer.zero_grad(set_to_none=True)
                
                # Only step scheduler on full accumulation boundaries
                if self.scheduler_step_on_batch and is_accumulation_boundary:
                    self.scheduler.step()
                    
                self.global_step += 1

                if self._cuda_graphs_enabled and hasattr(self, "cudagraph_mark_step"):
                    self.cudagraph_mark_step()

                # Reset accumulation tracking
                accumulated_loss = 0.0
                accumulated_batches = 0

            # Track total loss
            batch_loss = loss.item() * accumulation_steps
            total_loss += batch_loss * inputs.size(0)
            total_samples += inputs.size(0)

            # Progress logging
            if time.time() - last_log_time > LOGGING_INTERVAL_SECONDS:
                pct = 100.0 * (batch_idx + 1) / len(self.train_loader)
                self.logger.info(
                    f"Epoch {self.current_epoch:03d} "
                    f"{pct:5.1f}%  Loss {batch_loss:.4e}"
                )
                last_log_time = time.time()

            # Memory management for large batches
            if self.device.type == 'cuda' and (batch_idx + 1) % self.empty_cache_interval == 0:
                torch.cuda.empty_cache()

        avg_loss = total_loss / total_samples if total_samples else float("inf")
        metrics = {
            "grad_norm_ema": grad_norm_ema,
            "learning_rate": self.optimizer.param_groups[0]["lr"],
        }
        
        return avg_loss, metrics
    
    @torch.no_grad()
    def _run_eval_loop(self, loader: DataLoader, enable_logging: bool = True) -> Tuple[float, Dict[str, float]]:
        """
        Run evaluation loop on any dataset with proper empty loader handling.
        
        Args:
            loader: DataLoader to evaluate
            enable_logging: Whether to enable progress logging
            
        Returns:
            Tuple of (average loss, metrics dict)
        """
        # Check for empty loader
        if len(loader) == 0:
            self.logger.error("Evaluation DataLoader is empty!")
            raise ValueError("Cannot evaluate on empty DataLoader. Check data splits and filtering.")
        
        self.model.eval()
        
        total_loss = 0.0
        total_samples = 0
        
        # Time tracking
        eval_start = time.time()
        last_log_time = eval_start
        
        for batch_idx, (inputs, targets) in enumerate(loader):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)
            
            if self.use_amp:
                with autocast(device_type=self.device.type, dtype=self.amp_dtype):
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, targets)
            else:
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)
            
            total_loss += loss.item() * inputs.size(0)
            total_samples += inputs.size(0)
            
            if enable_logging:
                current_time = time.time()
                if current_time - last_log_time > LOGGING_INTERVAL_SECONDS:
                    progress = (batch_idx + 1) / len(loader) * 100
                    elapsed = current_time - eval_start
                    self.logger.info(
                        f"Epoch {self.current_epoch:03d} Val: {progress:.1f}% "
                        f"({batch_idx+1}/{len(loader)} batches), "
                        f"Elapsed: {elapsed:.1f}s"
                    )
                    last_log_time = current_time
        
        elapsed_total = time.time() - eval_start
        if enable_logging:
            self.logger.info(f"Evaluation completed in {elapsed_total:.1f}s")
        
        if total_samples == 0:
            self.logger.error("No samples processed during evaluation!")
            raise ValueError("Evaluation processed zero samples. Check batch processing.")
            
        avg_loss = total_loss / total_samples
            
        return avg_loss, {}
        
    def _validate(self) -> Tuple[float, Dict[str, float]]:
        """Validate the model during training."""
        return self._run_eval_loop(self.val_loader, enable_logging=True)
    
    @torch.no_grad()
    def evaluate_test(self) -> float:
        """Evaluate on test set."""
        self.logger.info("Evaluating on test set...")
        
        # Load best model
        checkpoint_path = self.save_dir / "best_model.pt"
        if checkpoint_path.exists():
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint["model_state_dict"])
            self.logger.info(f"Loaded best model from epoch {checkpoint['epoch']}")
        
        test_loss, _ = self._run_eval_loop(self.test_loader, enable_logging=False)
        
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        return test_loss
    
    def _save_best_model(self):
        """Save best model checkpoint."""
        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "best_val_loss": self.best_val_loss,
            "config": self.config,
            "training_time": self.total_training_time
        }
        
        path = self.save_dir / "best_model.pt"
        torch.save(checkpoint, path)
        self.logger.info(f"Saved best model (epoch {self.current_epoch})")
        
        # Also export model if requested
        if self.system_config.get("save_jit_model", False):
            self._export_model()
    
    def _export_model(self):
        """Export model using modern torch.export with improved error handling."""
        try:
            if len(self.val_loader) == 0:
                self.logger.warning("Validation loader is empty, trying train loader for export")
                if len(self.train_loader) == 0:
                    self.logger.warning("Train loader is also empty, skipping export")
                    return
                loader_to_use = self.train_loader
            else:
                loader_to_use = self.val_loader
            
            example_batch = next(iter(loader_to_use))
            example_input = example_batch[0][:1].to(self.device)  # Single sample
            
            export_path = self.save_dir / "best_model_exported.pt"
            export_model(self.model, example_input, export_path)
            
            example_path = self.save_dir / "export_example_input.pt"
            torch.save(example_input, example_path)
            self.logger.info(f"Saved export example input to {example_path}")
            
        except Exception as e:
            self.logger.warning(f"Model export failed: {e}")
            self.logger.info("Model saved in standard format, export can be done manually later")
    
    def _log_epoch(        
        self,
        train_loss: float,
        val_loss: float,
        train_metrics: Dict[str, float],
        val_metrics: Dict[str, float],
        epoch_time: float,
    ):
        improvement = self.best_val_loss - val_loss
        grad_norm   = train_metrics.get("grad_norm_ema", 0.0)

        msg = (f"Epoch {self.current_epoch:03d} | "
               f"Train {train_loss:.4e} | Val {val_loss:.4e} | "
               f"LR {train_metrics['learning_rate']:.2e} | "
               f"Grad {grad_norm:.3e} | Time {epoch_time:.1f}s")
        if improvement > 0:
            msg += f" | ▲ {improvement:.4e}"
        self.logger.info(msg)

        self.training_history["epochs"].append(
            {
                "epoch": self.current_epoch,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "grad_norm": grad_norm,
                "lr": train_metrics["learning_rate"],
                "time_s": epoch_time,
                "improvement": max(improvement, 0.0),
            }
        )

        if len(self.training_history["epochs"]) > self.max_history_epochs:
            self.training_history["epochs"] = self.training_history["epochs"][-self.max_history_epochs :]

        if self.current_epoch % self.history_save_interval == 0:
            try:
                with open(self.log_file, "w") as f:
                    json.dump(self.training_history, f, indent=2)
            except Exception as exc:
                self.logger.warning(f"Failed to save history: {exc}")

===== /Users/imalsky/Desktop/Chemulator/src/utils/utils.py =====
#!/usr/bin/env python3
"""
General utility functions for the chemical kinetics pipeline.

Provides helpers for:
- Configuration management with JSON5 support
- Logging setup
- Random seed control
- File I/O operations
- Data validation
"""

import json
import logging
import os
import random
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import numpy as np
import torch


def setup_logging(
    level: int = logging.INFO,
    log_file: Optional[Path] = None,
    format_string: Optional[str] = None
) -> None:
    """
    Configure logging for the application.
    
    Args:
        level: Logging level (default: INFO)
        log_file: Optional file path for logging
        format_string: Custom format string for log messages
    """
    # Default format
    if format_string is None:
        format_string = "%(asctime)s | %(levelname)-8s | %(name)s - %(message)s"
    
    # Remove all existing handlers from all loggers
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Also clean up any child logger handlers to prevent duplication
    for logger_name in logging.root.manager.loggerDict:
        logger = logging.getLogger(logger_name)
        logger.handlers.clear()
        logger.propagate = True
    
    # Configure root logger
    root_logger.setLevel(level)
    
    # Create formatter
    formatter = logging.Formatter(format_string, datefmt="%Y-%m-%d %H:%M:%S")
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(level)
    root_logger.addHandler(console_handler)
    
    # File handler (optional)
    if log_file is not None:
        try:
            # Ensure directory exists
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Create file handler
            file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
            file_handler.setFormatter(formatter)
            file_handler.setLevel(level)
            root_logger.addHandler(file_handler)
            
            print(f"Logging to file: {log_file}")
        except Exception as e:
            print(f"Failed to setup file logging: {e}", file=sys.stderr)
    
    # Log initial message
    logger = logging.getLogger(__name__)
    logger.info("Logging system initialized")


def seed_everything(seed: int) -> None:
    """
    Set random seeds for reproducibility across all libraries.
    
    Args:
        seed: Random seed value
    """
    # Python random
    random.seed(seed)
    
    # Numpy
    np.random.seed(seed)
    
    # PyTorch
    torch.manual_seed(seed)
    
    # Only set CUDA seed if CUDA is available
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # Environment variable for hash randomization
    os.environ["PYTHONHASHSEED"] = str(seed)
    
    # For better reproducibility (optional - may impact performance)
    # torch.use_deterministic_algorithms(True)
    # if torch.cuda.is_available():
    #     torch.backends.cudnn.deterministic = True
    #     torch.backends.cudnn.benchmark = False
    
    logger = logging.getLogger(__name__)
    logger.info(f"Random seed set to {seed}")


def load_json_config(path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load JSON configuration file with validation.
    
    Supports both standard JSON and JSON with comments (using json5 if available).
    
    Args:
        path: Path to configuration file
        
    Returns:
        Configuration dictionary
        
    Raises:
        FileNotFoundError: If configuration file doesn't exist
        ValueError: If configuration is invalid
        ImportError: If json5 is needed but not installed
    """
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"Configuration file not found: {path}")
    
    # Try json5 first for comment support
    try:
        import json5
        with open(path, 'r', encoding='utf-8') as f:
            config = json5.load(f)
    except ImportError:
        # Check if file likely contains comments
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
            if '//' in content or '/*' in content:
                raise ImportError(
                    "Configuration file appears to contain comments but json5 is not installed.\n"
                    "Install it with: pip install json5"
                )
        
        # Fallback to standard json
        with open(path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    # Validate configuration
    validate_config(config)
    
    return config


def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate configuration dictionary structure.
    
    Args:
        config: Configuration dictionary to validate
        
    Raises:
        ValueError: If configuration is invalid
    """
    # Required top-level sections
    required_sections = ["paths", "data", "normalization", "model", "training", "system"]
    
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required configuration section: '{section}'")
    
    # Validate paths
    paths = config["paths"]
    if not isinstance(paths.get("raw_data_files"), list):
        raise ValueError("'paths.raw_data_files' must be a list")
    
    # Validate data specification
    data = config["data"]
    if not isinstance(data.get("species_variables"), list) or not data["species_variables"]:
        raise ValueError("'data.species_variables' must be a non-empty list")
    
    if not isinstance(data.get("global_variables"), list):
        raise ValueError("'data.global_variables' must be a list")
    
    # Enhanced validation: Check variable names
    all_variables = data["species_variables"] + data["global_variables"] + [data.get("time_variable", "t_time")]
    duplicates = []
    seen = set()
    
    for var in all_variables:
        # Check for duplicates
        if var in seen:
            duplicates.append(var)
        seen.add(var)
    
    if duplicates:
        raise ValueError(f"Duplicate variable names found: {duplicates}")
    
    # Validate model configuration
    model = config["model"]
    if model.get("type") not in ["siren", "resnet", "deeponet"]:
        raise ValueError("'model.type' must be either 'siren', 'resnet', or 'deeponet'")
    
    # Validate model-specific parameters
    model_type = model.get("type")
    
    if model_type in ["siren", "resnet"]:
        # These models require hidden_dims
        if not isinstance(model.get("hidden_dims"), list) or not model["hidden_dims"]:
            raise ValueError("'model.hidden_dims' must be a non-empty list for siren and resnet models")
    
    elif model_type == "deeponet":
        # DeepONet requires branch_layers, trunk_layers, and basis_dim
        if not isinstance(model.get("branch_layers"), list) or not model["branch_layers"]:
            raise ValueError("'model.branch_layers' must be a non-empty list for deeponet model")
        
        if not isinstance(model.get("trunk_layers"), list) or not model["trunk_layers"]:
            raise ValueError("'model.trunk_layers' must be a non-empty list for deeponet model")
        
        if not isinstance(model.get("basis_dim"), int) or model["basis_dim"] <= 0:
            raise ValueError("'model.basis_dim' must be a positive integer for deeponet model")
        
        # Validate activation function if specified
        if "activation" in model:
            valid_activations = {"gelu", "relu", "tanh"}
            if model["activation"] not in valid_activations:
                raise ValueError(f"'model.activation' must be one of {valid_activations}")
    
    # Validate training parameters
    training = config["training"]
    
    # Check numeric parameters
    numeric_params = {
        "batch_size": (1, None),
        "epochs": (1, None),
        "learning_rate": (0, 1),
        "gradient_clip": (0, None),
        "val_fraction": (0, 1),
        "test_fraction": (0, 1)
    }
    
    for param, (min_val, max_val) in numeric_params.items():
        value = training.get(param)
        if value is None:
            raise ValueError(f"Missing training parameter: '{param}'")
        
        if not isinstance(value, (int, float)):
            raise ValueError(f"'{param}' must be numeric")
        
        if min_val is not None and value < min_val:
            raise ValueError(f"'{param}' must be >= {min_val}")
        
        if max_val is not None and value > max_val:
            raise ValueError(f"'{param}' must be <= {max_val}")
    
    # Check split fractions sum
    if training["val_fraction"] + training["test_fraction"] >= 1.0:
        raise ValueError("Sum of val_fraction and test_fraction must be < 1.0")
    
    # Validate normalization methods
    norm_config = config["normalization"]
    valid_methods = {"standard", "log-standard", "log-min-max", "symlog", "none"}
    
    default_method = norm_config.get("default_method")
    if default_method not in valid_methods:
        raise ValueError(f"Invalid default normalization method: {default_method}")
    
    # Check per-variable methods
    methods = norm_config.get("methods", {})
    for var, method in methods.items():
        if method not in valid_methods:
            raise ValueError(f"Invalid normalization method for '{var}': {method}")


def save_json(
    data: Dict[str, Any],
    path: Union[str, Path],
    indent: int = 2
) -> None:
    """
    Save dictionary to JSON file with pretty printing.
    
    Args:
        data: Dictionary to save
        path: Output file path
        indent: JSON indentation level
    """
    path = Path(path)
    
    # Ensure directory exists
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Custom encoder for special types
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.Tensor):
                return obj.cpu().numpy().tolist()
            elif isinstance(obj, Path):
                return str(obj)
            elif isinstance(obj, datetime):
                return obj.isoformat()
            return super().default(obj)
    
    # Write file
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, cls=NumpyEncoder)


def load_json(path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load JSON file.
    
    Args:
        path: Path to JSON file
        
    Returns:
        Loaded dictionary
    """
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def ensure_directories(*paths: Union[str, Path]) -> None:
    """
    Create directories if they don't exist with thread safety.
    
    Args:
        *paths: Variable number of directory paths to create
    """
    for path in paths:
        Path(path).mkdir(parents=True, exist_ok=True)

===== /Users/imalsky/Desktop/Chemulator/src/utils/hardware.py =====
#!/usr/bin/env python3
"""
Hardware detection and optimization utilities.

This module provides functions to:
- Detect and configure available compute devices
- Optimize settings for specific hardware (especially A100)
- Configure memory and computation settings
"""

import logging
import os
from typing import Dict, Any, Optional

import torch
import numpy as np

# Hardware constants
DEFAULT_CUDA_ALLOC_MB = 512
DEFAULT_THREAD_RATIO = 0.5
MIN_WORKERS = 4
MAX_WORKERS = 8
MIN_CPU_WORKERS = 2
MPS_WORKERS = 0
MEMORY_BUFFER_RATIO = 0.9
DEFAULT_PREFETCH_FACTOR = 2
MAX_PREFETCH_FACTOR = 16  # PyTorch hard limit
MIN_PREFETCH_FACTOR = 2   # PyTorch minimum

# Compute capability thresholds
AMPERE_COMPUTE_CAPABILITY = 8  # For TF32, BFloat16, Flash Attention


def setup_device() -> torch.device:
    """
    Detect and configure the best available compute device.
    
    Priority order:
    1. CUDA (NVIDIA GPUs)
    2. MPS (Apple Silicon)
    3. CPU
    
    Returns:
        Configured torch.device
    """
    logger = logging.getLogger(__name__)
    
    if torch.cuda.is_available():
        # CUDA available
        device = torch.device("cuda")
        
        # Log GPU information
        gpu_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1e9
        
        logger.info(f"Using CUDA device: {gpu_name}")
        logger.info(f"GPU memory: {gpu_memory:.1f} GB")
        logger.info(f"Number of GPUs: {gpu_count}")
        
        # Set default GPU
        torch.cuda.set_device(current_device)
        
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        # Apple Silicon GPU
        device = torch.device("mps")
        logger.info("Using Apple Silicon MPS device")
        logger.warning("MPS backend has limited operator support")
        logger.warning("torch.compile is disabled for MPS devices")
        
    else:
        # CPU fallback
        device = torch.device("cpu")
        logger.info("Using CPU device")
        logger.warning("Training will be significantly slower on CPU")
        
        # Log CPU info
        cpu_count = os.cpu_count()
        logger.info(f"CPU cores: {cpu_count}")
    
    return device


def optimize_hardware(config: Dict[str, Any], device: torch.device) -> None:
    """
    Apply hardware-specific optimizations based on configuration.
    
    Args:
        config: System configuration dictionary
        device: The device being used
    """
    logger = logging.getLogger(__name__)
    
    # Disable torch.compile for MPS devices
    if device.type == "mps" and config.get("use_torch_compile", False):
        logger.warning("Disabling torch.compile for MPS device due to compatibility issues")
        config["use_torch_compile"] = False
    
    # CUDA optimizations
    if torch.cuda.is_available():
        # Enable TensorFloat-32 on Ampere GPUs (A100, RTX 30xx)
        if config.get("tf32", True):
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("TensorFloat-32 (TF32) enabled for matrix operations")
        
        # Enable cuDNN autotuner for optimal convolution algorithms
        if config.get("cudnn_benchmark", True):
            torch.backends.cudnn.benchmark = True
            logger.info("cuDNN autotuner enabled")
        
        # Set CUDA memory allocator settings
        if "PYTORCH_CUDA_ALLOC_CONF" not in os.environ:
            # Use larger allocation blocks to reduce fragmentation
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = f"max_split_size_mb:{DEFAULT_CUDA_ALLOC_MB}"
        
        # Disable CUDA synchronous operations for better performance
        if "CUDA_LAUNCH_BLOCKING" not in os.environ:
            os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
    
    # Set number of threads for CPU operations
    if "OMP_NUM_THREADS" not in os.environ:
        # Use half the CPU cores for OpenMP threads
        cpu_count = os.cpu_count() or 1
        # For systems with E-cores, limit to physical cores
        omp_threads = max(1, min(int(cpu_count * DEFAULT_THREAD_RATIO), cpu_count // 2))
        os.environ["OMP_NUM_THREADS"] = str(omp_threads)
    
    # PyTorch threading settings - must be set before any operations
    torch.set_num_threads(int(os.environ.get("OMP_NUM_THREADS", "1")))
    
    # Enable anomaly detection if requested (debugging only)
    if config.get("detect_anomaly", False):
        torch.autograd.set_detect_anomaly(True)
        logger.warning("Anomaly detection enabled - this will slow down training")


def get_device_info(device: torch.device = None) -> Dict[str, Any]:
    """
    Get detailed information about the current compute device.
    
    Args:
        device: Device to query (defaults to current device)
        
    Returns:
        Dictionary containing device information
    """
    if device is None:
        device = setup_device()
        
    info = {
        "device_type": device.type,
        "device_name": None,
        "memory_gb": None,
        "compute_capability": None,
        "supports_tf32": False,
        "supports_bfloat16": False,
        "supports_flash_attention": False,
        "supports_compile": False
    }
    
    if device.type == "cuda":
        device_props = torch.cuda.get_device_properties(device.index or 0)
        
        info["device_name"] = device_props.name
        info["memory_gb"] = device_props.total_memory / 1e9
        info["compute_capability"] = f"{device_props.major}.{device_props.minor}"
        
        # Check capabilities based on compute capability
        compute_major = device_props.major
        compute_minor = device_props.minor
        
        # TF32 support (Ampere and newer, compute capability 8.0+)
        info["supports_tf32"] = compute_major >= AMPERE_COMPUTE_CAPABILITY
        
        # BFloat16 support (Ampere and newer)
        info["supports_bfloat16"] = compute_major >= AMPERE_COMPUTE_CAPABILITY
        
        # Flash Attention support (Ampere and newer with specific requirements)
        # Note: Ada (8.9) and Hopper (9.0) also support it
        info["supports_flash_attention"] = compute_major >= AMPERE_COMPUTE_CAPABILITY
        
        # Compilation support
        info["supports_compile"] = True
        
    elif device.type == "mps":
        info["device_name"] = "Apple Silicon GPU"
        info["supports_compile"] = False  # MPS has limited compile support
        
    else:
        info["device_name"] = "CPU"
        info["supports_compile"] = True
        
    return info


def optimize_dataloader_settings(
    batch_size: int,
    device_type: str,
    num_workers: Optional[int] = None
) -> Dict[str, Any]:
    """
    Get optimized DataLoader settings based on hardware.
    Fixed prefetch_factor logic to prevent PyTorch errors.
    
    Args:
        batch_size: Batch size for training
        device_type: Type of compute device (cuda/mps/cpu)
        num_workers: Override for number of workers
        
    Returns:
        Dictionary of DataLoader settings
    """
    settings = {
        "batch_size": batch_size,
        "num_workers": 0,
        "pin_memory": False,
        "persistent_workers": False,
        "prefetch_factor": None  # Must be None when num_workers=0
    }
    
    if device_type == "cuda":
        # Enable pinned memory for faster GPU transfer
        settings["pin_memory"] = True
        
        # Use multiple workers for data loading
        if num_workers is None:
            # Use 4-8 workers typically
            cpu_count = os.cpu_count() or 1
            settings["num_workers"] = min(MAX_WORKERS, max(MIN_WORKERS, cpu_count // 2))
        else:
            settings["num_workers"] = num_workers
        
        # Set prefetch_factor only when num_workers > 0
        if settings["num_workers"] > 0:
            settings["persistent_workers"] = True
            # Ensure prefetch_factor is within valid range [2, 16]
            settings["prefetch_factor"] = min(
                MAX_PREFETCH_FACTOR,
                max(MIN_PREFETCH_FACTOR, settings["num_workers"] // 2)
            )
        # prefetch_factor remains None when num_workers=0
            
    elif device_type == "mps":
        # MPS doesn't work well with multiprocessing
        settings["num_workers"] = MPS_WORKERS  # 0
        settings["pin_memory"] = False
        settings["persistent_workers"] = False
        # prefetch_factor must be None when num_workers=0
        
    elif device_type == "cpu":
        # For CPU, use fewer workers to avoid overhead
        if num_workers is None:
            settings["num_workers"] = min(MIN_CPU_WORKERS, os.cpu_count() or 1)
        else:
            settings["num_workers"] = num_workers
            
        # Set prefetch_factor only for CPU workers > 0
        if settings["num_workers"] > 0:
            settings["persistent_workers"] = True
            settings["prefetch_factor"] = MIN_PREFETCH_FACTOR
        # prefetch_factor remains None when num_workers=0
    
    # Log configuration for debugging
    logger = logging.getLogger(__name__)
    logger.debug(
        f"DataLoader settings for {device_type}: "
        f"workers={settings['num_workers']}, "
        f"prefetch={settings['prefetch_factor']}, "
        f"pin_memory={settings['pin_memory']}"
    )
    
    return settings

===== /Users/imalsky/Desktop/Chemulator/src/models/model.py =====
#!/usr/bin/env python3
"""
Model definitions for chemical kinetics prediction.

Provides implementations of:
- FiLM-SIREN
- ResNet
- DeepONet

With optional torch.compile support and model export using torch.export (modern best practice replacing legacy JIT/TorchScript).
"""

import logging
import time
from pathlib import Path
from typing import Dict, Any, List

import torch
import torch.nn as nn

# Constants
MIN_CONCENTRATION = 1e-30

# =============================================================================
# SIREN Model
# =============================================================================

class SIREN(nn.Module):
    """
    SIREN (Sinusoidal Representation Network) for chemical kinetics.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]
        
        self.layers = nn.ModuleList()
        prev_dim = self.num_species + self.num_globals + 1  # inputs + time
        
        for dim in self.hidden_dims:
            self.layers.append(nn.Linear(prev_dim, dim))
            prev_dim = dim
        
        self.output_layer = nn.Linear(prev_dim, self.num_species)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer in self.layers:
            x = torch.sin(layer(x))
        return self.output_layer(x)

# =============================================================================
# ResNet Model
# =============================================================================

class ChemicalResNet(nn.Module):
    """
    ResNet for chemical kinetics prediction.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]
        
        self.input_layer = nn.Linear(self.num_species + self.num_globals + 1, self.hidden_dims[0])
        
        self.res_blocks = nn.ModuleList()
        for i in range(len(self.hidden_dims) - 1):
            self.res_blocks.append(
                nn.Sequential(
                    nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]),
                    nn.ReLU(),
                    nn.Linear(self.hidden_dims[i+1], self.hidden_dims[i+1])
                )
            )
        
        self.output_layer = nn.Linear(self.hidden_dims[-1], self.num_species)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = nn.functional.relu(self.input_layer(x))
        
        for block in self.res_blocks:
            residual = x
            x = nn.functional.relu(block(x) + residual)
        
        return torch.clamp(self.output_layer(x), min=MIN_CONCENTRATION)

# =============================================================================
# DeepONet Model
# =============================================================================

class ChemicalDeepONet(nn.Module):
    """
    Deep Operator Network for chemical kinetics.
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        
        branch_layers = config["model"]["branch_layers"]
        trunk_layers = config["model"]["trunk_layers"]
        self.basis_dim = config["model"]["basis_dim"]
        self.activation = self._get_activation(config["model"].get("activation", "gelu"))
        self.use_residual = config["model"].get("use_residual", True)
        self.output_scale = config["model"].get("output_scale", 1.0)
        
        # Branch net: processes initial conditions, outputs basis for each species
        self.branch_net = self._build_mlp(
            self.num_species + self.num_globals,
            branch_layers,
            self.basis_dim * self.num_species
        )
        
        # Trunk net: processes time
        self.trunk_net = self._build_mlp(1, trunk_layers, self.basis_dim)
        
        # Bias
        self.bias = nn.Parameter(torch.zeros(1, self.num_species))
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(),
            "tanh": nn.Tanh()
        }
        return activations.get(name.lower(), nn.GELU())
    
    def _build_mlp(self, input_dim: int, hidden_layers: List[int], output_dim: int) -> nn.Sequential:
        layers = []
        prev_dim = input_dim
        
        for dim in hidden_layers:
            layers.extend([
                nn.Linear(prev_dim, dim),
                self.activation
            ])
            prev_dim = dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        return nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with corrected multi-output handling.
        
        Args:
            x: Input tensor (batch_size, num_species + num_globals + 1)
            
        Returns:
            Predicted species concentrations (batch_size, num_species)
        """
        initial_conditions = x[:, :-1]
        time = x[:, -1].unsqueeze(1)
        initial_species = x[:, :self.num_species]
        
        # Compute branch and trunk
        branch_out = self.branch_net(initial_conditions)  # (batch, basis_dim * num_species)
        trunk_out = self.trunk_net(time)  # (batch, basis_dim)
        
        # Reshape branch output
        branch_out = branch_out.view(-1, self.num_species, self.basis_dim)  # (batch, num_species, basis_dim)
        
        # Compute output for each species
        output = torch.einsum('bki,bi->bk', branch_out, trunk_out)  # (batch, num_species)
        output = output * self.output_scale + self.bias
        
        if self.use_residual:
            output = initial_species + output
            
        return torch.clamp(output, min=MIN_CONCENTRATION)

# =============================================================================
# Model Factory with Optional Compilation
# =============================================================================

def create_model(
    config: Dict[str, Any],
    device: torch.device
) -> nn.Module:
    """
    Construct the requested network, move it to the target device and – if
    enabled – compile it in default mode.
    """
    kind = config["model"]["type"].lower()
    if kind == "siren":
        model = SIREN(config)
    elif kind == "resnet":
        model = ChemicalResNet(config)
    elif kind == "deeponet":
        model = ChemicalDeepONet(config)
    else:
        raise ValueError(f"Unknown model type: {kind}")

    model = model.to(device)  # move first

    # -------------------------- optional compile -------------------------
    if not config["system"].get("use_torch_compile", False):
        return model

    # Only compile if supported
    if device.type != "cuda":
        logging.info("Compilation only supported on CUDA devices – running eager")
        return model

    # Timing
    compile_start = time.time()
    logging.info("Starting model compilation in default mode…")

    try:
        model = torch.compile(model, mode="default")
    except Exception as e:
        logging.warning(f"Compilation failed: {e} – falling back to eager mode")
        return model
    finally:
        logging.info(f"Compilation finished in {time.time() - compile_start:.1f}s")

    return model


def export_model(
    model: nn.Module,
    example_input: torch.Tensor,
    save_path: Path
):
    """
    Export model using torch.export with proper handling for compiled models.
    
    Args:
        model: Model to export (compiled or eager)
        example_input: Example input tensor
        save_path: Path to save exported model
    """
    logger = logging.getLogger(__name__)
    
    # Set to eval mode
    model.eval()
    
    # Check if model is compiled and get the original model if needed
    is_compiled = hasattr(model, '_orig_mod')
    if is_compiled:
        logger.info("Detected compiled model, extracting original model for export")
        original_model = model._orig_mod
    else:
        original_model = model
    
    with torch.no_grad():
        try:
            # Start export timer for large models
            export_start = time.time()
            logger.info("Starting model export...")
            
            # Use torch.export.export for modern export
            exported = torch.export.export(original_model, (example_input,))
            
            # Save the exported program
            torch.export.save(exported, str(save_path))
            
            export_time = time.time() - export_start
            logger.info(f"Model exported successfully to {save_path} in {export_time:.1f}s")
            
        except Exception as e:
            logger.error(f"Model export failed: {e}")
            logger.error("Common causes: dynamic shapes, unsupported ops, or compilation artifacts")
            logger.info("Try exporting the model before compilation if this persists")
            raise

===== /Users/imalsky/Desktop/Chemulator/src/data/device_prefetch.py =====
#!/usr/bin/env python3
"""
Device prefetching utilities for overlapping data transfer with computation.
"""

import torch
from torch.utils.data import DataLoader
from typing import Iterator, Tuple


class DevicePrefetchLoader:
    """
    Wraps a DataLoader to prefetch batches to GPU in a separate CUDA stream.
    This overlaps data transfer with model computation for better GPU utilization.
    """
    
    def __init__(self, loader: DataLoader, device: torch.device):
        """
        Initialize the prefetch loader.
        
        Args:
            loader: The DataLoader to wrap
            device: Target device (should be cuda)
        """
        self.loader = loader
        self.device = device
        
        # Only use CUDA streams for CUDA devices
        if device.type == 'cuda':
            self.stream = torch.cuda.Stream(device=device)
        else:
            self.stream = None
        
        self.preload_batch = None
    
    def __len__(self):
        return len(self.loader)
    
    def __iter__(self):
        # Start the preload
        loader_iter = iter(self.loader)
        self._preload(loader_iter)
        
        while self.preload_batch is not None:
            # Wait for the preload to complete and get the batch
            if self.stream is not None:
                torch.cuda.current_stream(self.device).wait_stream(self.stream)
            
            batch = self.preload_batch
            
            # Start preloading the next batch
            self._preload(loader_iter)
            
            yield batch
    
    def _preload(self, loader_iter: Iterator):
        """Preload the next batch in a separate stream."""
        try:
            data = next(loader_iter)
        except StopIteration:
            self.preload_batch = None
            return
        
        if self.stream is not None:
            with torch.cuda.stream(self.stream):
                # Move batch to device asynchronously
                self.preload_batch = self._move_to_device(data)
        else:
            # No CUDA stream available, just move normally
            self.preload_batch = self._move_to_device(data)
    
    def _move_to_device(self, data):
        """Move data to device with non_blocking."""
        if isinstance(data, (tuple, list)):
            return tuple(
                d.to(self.device, non_blocking=True) if isinstance(d, torch.Tensor) else d
                for d in data
            )
        elif isinstance(data, dict):
            return {
                k: v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else v
                for k, v in data.items()
            }
        elif isinstance(data, torch.Tensor):
            return data.to(self.device, non_blocking=True)
        else:
            return data
        
    # ──────────────────────────────────────────────────────────────
    def __getattr__(self, name):
        """
        Forward attribute look-ups that this wrapper does not define to the
        underlying DataLoader. This exposes .num_workers, .batch_size, etc.,
        so downstream code can treat DevicePrefetchLoader just like a regular
        DataLoader.

        NOTE: magic (dunder) names are excluded to avoid infinite recursion.
        """
        if name.startswith("__"):
            raise AttributeError(name)
        return getattr(self.loader, name)

===== /Users/imalsky/Desktop/Chemulator/src/data/preprocessor.py =====
#!/usr/bin/env python3
"""
Preprocessor for chemical kinetics data.
Converts raw HDF5 files to normalized chunked HDF5 with train/val/test splits.
"""

import logging
import sys
import random
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple

import h5py
import numpy as np
import torch
from utils.utils import save_json

from .normalizer import DataNormalizer, NormalizationHelper

DEFAULT_TOSS = 1e-25

class DataPreprocessor:
    """
    Preprocess raw HDF5 files to normalized chunked HDF5 with splits.
    """
    
    def __init__(
        self,
        raw_files: List[Path],
        output_dir: Path,
        config: Dict[str, Any]
    ):
        self.raw_files = raw_files
        self.output_dir = output_dir
        self.config = config
        
        self.logger = logging.getLogger(__name__)
        
        self.data_config = config["data"]
        self.species_vars = self.data_config["species_variables"]
        self.global_vars = self.data_config["global_variables"]
        self.time_var = self.data_config["time_variable"]
        self.var_order = self.species_vars + self.global_vars + [self.time_var]
        
        self.n_vars = len(self.var_order)
        self.n_species = len(self.species_vars)
        self.n_globals = len(self.global_vars)
        
        self.chunk_size = self.data_config["chunk_size"]
        self.compression = self.data_config.get("compression", "gzip")
        self.compression_level = self.data_config.get("compression_level", 4)
        
        # Output HDF5 path
        self.output_hdf5 = self.output_dir / config["paths"].get("processed_hdf5_file", "preprocessed_data.h5")
        
        # Initialize summary counters
        self.total_groups = 0
        self.skipped_fraction = 0
        self.skipped_missing = 0
        self.skipped_pattern = 0
        self.skipped_nonfinite = 0
        self.total_nonfinite_values = 0
    
    def process_to_hdf5(self) -> Dict[str, Any]:
        """
        Two-stage pipeline:
        1. Single streaming sweep to accumulate statistics and count rows.
        2. Second streaming sweep to write normalized HDF5 with splits.

        Returns:
            Dictionary with dataset metadata and split information.
            
        Raises:
            SystemExit: If no valid data is found
        """
        self.logger.info("─" * 80)
        self.logger.info("Stage 1 – collecting normalisation statistics")

        ########################
        # Pass 1 – statistics  #
        ########################
        pass1_start = time.time()
        max_timesteps = 0
        total_samples = 0
        accumulators = self._initialize_accumulators()

        for raw_file in self.raw_files:
            file_start = time.time()
            self.logger.info(f"Processing stats from {raw_file}")
            
            with h5py.File(raw_file, "r") as f:
                for gname in f.keys():
                    self.total_groups += 1  # Track all attempted groups

                    grp = f[gname]
                    if not self._accept_group(grp, gname):
                        continue

                    n_t = grp[self.time_var].shape[0]
                    if n_t > 10000:
                        self.logger.debug(f"Processing large group {gname} with {n_t:,} timesteps")
                    
                    max_timesteps = max(max_timesteps, n_t)
                    total_samples += (n_t - 1)  # Exclude t=0
                    
                    profile_np = self._group_to_profile(grp, gname, n_t)
                    # Reshape to 3D (n_profiles=1, n_t, n_vars) for normalizer compatibility
                    profile_3d = profile_np.reshape(1, n_t, self.n_vars)
                    self.normalizer._update_accumulators(
                        profile_3d, accumulators, n_t
                    )
            
            file_time = time.time() - file_start
            self.logger.info(f"Processed stats for file {raw_file} in {file_time:.1f}s")
        
        pass1_time = time.time() - pass1_start
        self.logger.info(f"Stage 1 completed in {pass1_time:.1f}s")
        self.logger.info(f"Total samples to process: {total_samples:,}")

        if total_samples == 0:
            self.logger.error("No valid samples found in data!")
            self.logger.error("Check that:")
            self.logger.error("1. Raw data files contain expected variables")
            self.logger.error("2. Data values are within acceptable ranges")
            self.logger.error("3. Group names match expected pattern")
            sys.exit(1)  # Exit with error instead of returning error dict

        # Continue with rest of processing...
        norm_stats = self.normalizer._finalize_statistics(accumulators)
        save_json(norm_stats, self.output_dir / "normalization.json")

        ########################
        # Pass 2 – HDF5 write  #
        ########################
        self.logger.info("Stage 2 – writing normalized HDF5 with splits")
        pass2_start = time.time()
        
        helper = NormalizationHelper(
            norm_stats, torch.device("cpu"),
            self.species_vars, self.global_vars, self.time_var, self.config
        )
        
        # Calculate split sizes
        val_f = self.config["training"]["val_fraction"]
        test_f = self.config["training"]["test_fraction"]
        
        # Create output HDF5 file
        self.logger.info(f"Creating HDF5 file: {self.output_hdf5}")
        
        with h5py.File(self.output_hdf5, 'w') as out_f:
            # Create split groups
            splits = {
                "train": self._create_split_group(out_f, "train", self.chunk_size),
                "validation": self._create_split_group(out_f, "validation", self.chunk_size),
                "test": self._create_split_group(out_f, "test", self.chunk_size)
            }
            
            # Add global metadata
            out_f.attrs['n_species'] = self.n_species
            out_f.attrs['n_globals'] = self.n_globals
            out_f.attrs['format_version'] = "1.0"
            out_f.attrs['created'] = time.strftime("%Y-%m-%d %H:%M:%S")
            
            # Process all files and write to splits
            split_indices = {"train": 0, "validation": 0, "test": 0}
            global_idx = 0
            
            for raw_file in self.raw_files:
                file_start = time.time()
                self.logger.info(f"Processing {raw_file} to HDF5")
                
                with h5py.File(raw_file, "r") as f:
                    for gname in f.keys():
                        grp = f[gname]
                        if not self._accept_group(grp, gname):
                            continue

                        n_t = grp[self.time_var].shape[0]
                        if n_t > 10000:
                            group_start = time.time()
                            self.logger.debug(f"Starting processing for large group {gname} with {n_t:,} timesteps")
                        
                        # Get and normalize profile
                        profile = self._group_to_profile(grp, gname, n_t)
                        profile_tensor = torch.from_numpy(profile)
                        normalized_profile = helper.normalize_profile(profile_tensor).numpy()
                        
                        # Convert to samples
                        inputs_arr, targets_arr = self._profile_to_samples(normalized_profile, n_t)
                        num_samples = len(inputs_arr)
                        
                        # Assign split for the entire profile
                        r = random.random()
                        if r < test_f:
                            split_name = "test"
                        elif r < test_f + val_f:
                            split_name = "validation"
                        else:
                            split_name = "train"
                        
                        # Write directly to HDF5
                        if num_samples > 0:
                            start_idx = split_indices[split_name]
                            end_idx = start_idx + num_samples
                            
                            splits[split_name]["inputs"][start_idx:end_idx] = inputs_arr
                            splits[split_name]["targets"][start_idx:end_idx] = targets_arr
                            
                            split_indices[split_name] = end_idx
                            global_idx += num_samples
                        
                        if n_t > 10000:
                            group_time = time.time() - group_start
                            self.logger.debug(f"Completed processing for large group {gname} in {group_time:.1f}s")
                
                file_time = time.time() - file_start
                self.logger.info(f"Processed file {raw_file} in {file_time:.1f}s")
            
            # Resize datasets to actual sizes and update metadata
            for split_name, split_data in splits.items():
                actual_size = split_indices[split_name]
                split_data["inputs"].resize((actual_size, self.n_species + self.n_globals + 1))
                split_data["targets"].resize((actual_size, self.n_species))
                split_data["group"].attrs['n_samples'] = actual_size
                
                self.logger.info(f"{split_name} split: {actual_size:,} samples")
        
        pass2_time = time.time() - pass2_start
        self.logger.info(f"Stage 2 completed in {pass2_time:.1f}s")
        
        # Log summary
        skipped_total = self.skipped_fraction + self.skipped_missing + self.skipped_pattern + self.skipped_nonfinite
        self.logger.info(
            f"Preprocessing summary: Total groups attempted: {self.total_groups}, "
            f"Processed: {self.total_groups - skipped_total}, "
            f"Skipped: {skipped_total} ({skipped_total / self.total_groups * 100 if self.total_groups else 0:.1f}%) "
            f"[fraction: {self.skipped_fraction}, missing keys: {self.skipped_missing}, "
            f"pattern mismatch: {self.skipped_pattern}, non-finite: {self.skipped_nonfinite}]"
        )
        
        # Return metadata
        return {
            "output_file": str(self.output_hdf5),
            "total_samples": global_idx,
            "splits": {
                split_name: split_indices[split_name] 
                for split_name in ["train", "validation", "test"]
            },
            "normalization_file": str(self.output_dir / "normalization.json")
        }
    
    def _create_split_group(self, hdf5_file: h5py.File, split_name: str, chunk_size: int) -> Dict[str, Any]:
        """Create a split group in the HDF5 file with chunked datasets."""
        grp = hdf5_file.create_group(split_name)
        
        # Create datasets with chunking and compression
        # Start with a reasonable size, will resize later
        initial_size = 1000000
        
        inputs_dset = grp.create_dataset(
            "inputs",
            shape=(initial_size, self.n_species + self.n_globals + 1),
            maxshape=(None, self.n_species + self.n_globals + 1),
            dtype=np.float32,
            chunks=(chunk_size, self.n_species + self.n_globals + 1),
            compression=self.compression,
            compression_opts=self.compression_level
        )
        
        targets_dset = grp.create_dataset(
            "targets", 
            shape=(initial_size, self.n_species),
            maxshape=(None, self.n_species),
            dtype=np.float32,
            chunks=(chunk_size, self.n_species),
            compression=self.compression,
            compression_opts=self.compression_level
        )
        
        # Add metadata
        grp.attrs['n_species'] = self.n_species
        grp.attrs['n_globals'] = self.n_globals
        grp.attrs['n_samples'] = 0  # Will be updated
        
        return {
            "group": grp,
            "inputs": inputs_dset,
            "targets": targets_dset
        }
    
    def _initialize_accumulators(self):
        """Initialize accumulators for stats collection."""
        self.normalizer = DataNormalizer(self.config, actual_timesteps=1)  # Timesteps updated per group
        return self.normalizer._initialize_accumulators(0)
    
    def _accept_group(self, group: h5py.Group, gname: str) -> bool:
        """Combined acceptance check: validate + pattern match + use_fraction."""
        if random.random() > self.config["training"]["use_fraction"]:
            self.skipped_fraction += 1
            return False
        
        # Check all required variables exist
        required_keys = set(self.species_vars + [self.time_var])
        missing_vars = required_keys - set(group.keys())
        if missing_vars:
            if not hasattr(self, '_warned_missing_vars'):
                self._warned_missing_vars = True
                self.logger.error(f"Missing variables in HDF5: {missing_vars}")
            self.skipped_missing += 1
            return False
        
        if not self._validate_group(group):
            self.skipped_nonfinite += 1
            return False
        
        match = re.match(r"run_T_(?P<T_init>[\d.eE+-]+)_P_(?P<P_init>[\d.eE+-]+)_SEED_\d+", gname)
        if not match:
            self.skipped_pattern += 1
            return False
        
        return True
    
    def _group_to_profile(self, group: h5py.Group, gname: str, n_t: int) -> np.ndarray:
        """Extract profile from group using chunked reading to handle large n_t."""
        match = re.match(r"run_T_(?P<T_init>[\d.eE+-]+)_P_(?P<P_init>[\d.eE+-]+)_SEED_\d+", gname)
        try:
            T_init = float(match.group('T_init'))
            P_init = float(match.group('P_init'))
        except (ValueError, TypeError) as e:
            self.logger.warning(f"Invalid T_init or P_init in group {gname}: {e}")
            raise ValueError(f"Cannot parse initial conditions from {gname}")
        
        profile = np.zeros((n_t, self.n_vars), dtype=np.float32)
        
        # Chunked load for variables from HDF5
        for start in range(0, n_t, self.chunk_size):
            end = min(start + self.chunk_size, n_t)
            chunk_size = end - start
            if chunk_size > 10000:
                self.logger.debug(f"Loading chunk {start}-{end} for group {gname}")
            
            for i, var in enumerate(self.var_order):
                if var in self.species_vars or var == self.time_var:
                    profile[start:end, i] = group[var][start:end]
                elif var == "T_init":
                    profile[start:end, i] = T_init
                elif var == "P_init":
                    profile[start:end, i] = P_init
        
        return profile
    
    def _profile_to_samples(self, normalized_profile: np.ndarray, n_t: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Convert normalized profile to input and target arrays.
        Raises error if insufficient timesteps instead of returning empty arrays.
        """
        if n_t <= 1:
            self.logger.error(f"Insufficient timesteps for training: n_t={n_t}. Need at least 2 timesteps.")
            raise ValueError(f"Cannot create training samples from {n_t} timesteps. Need at least 2.")
        
        # Log processing for large profiles
        if n_t > 10000:
            self.logger.info(f"Converting profile with {n_t:,} timesteps to {n_t-1:,} training samples")
        
        # Initial species (repeat for all samples)
        init_species = np.repeat(normalized_profile[0, :self.n_species][np.newaxis, :], n_t - 1, axis=0)
        
        # Globals (from t=1 to end)
        globals_t = normalized_profile[1:, self.n_species:self.n_species + self.n_globals]
        
        # Time (from t=1 to end)
        time_t = normalized_profile[1:, -1][:, np.newaxis]
        
        # Inputs: init_species + globals_t + time_t
        inputs_arr = np.concatenate([init_species, globals_t, time_t], axis=1)
        
        # Targets (species from t=1 to end)
        targets_arr = normalized_profile[1:, :self.n_species]
        
        return inputs_arr, targets_arr
        
    def _validate_group(self, group: h5py.Group) -> bool:
        """
        Validate group data and log rejection reasons.
        Returns False if any NaN/Inf is found or if any species value is 
        below threshold.
        """
        required_keys = self.species_vars + [self.time_var]
        is_valid = True
        n_t = group[self.time_var].shape[0]
        chunk_size = self.chunk_size
        min_threshold = DEFAULT_TOSS
        
        for key in required_keys:
            bad_count = 0  # For non-finites
            below_threshold_count = 0  # For values below threshold
            
            for start in range(0, n_t, chunk_size):
                end = min(start + chunk_size, n_t)
                data_chunk = group[key][start:end]
                
                # Check for non-finites
                bad_mask = ~np.isfinite(data_chunk)
                bad_count += int(bad_mask.sum())
                
                # Check for species values below threshold
                if key in self.species_vars:
                    below_mask = data_chunk < min_threshold
                    below_threshold_count += int(below_mask.sum())
                    
                    if np.any(below_mask):
                        # Log specific information about rejected data
                        min_val = np.min(data_chunk)
                        self.logger.warning(
                            f"Species {key} has {below_threshold_count} values below threshold {min_threshold} "
                            f"(min value: {min_val:.2e})"
                        )
                        is_valid = False
            
            self.total_nonfinite_values += bad_count
            if bad_count > 0:
                self.logger.warning(f"Variable {key} has {bad_count} non-finite values")
                is_valid = False
        
        return is_valid

===== /Users/imalsky/Desktop/Chemulator/src/data/dataset.py =====
#!/usr/bin/env python3
"""
Optimized dataset for chemical kinetics data using HDF5 format.
Uses chunked HDF5 files for efficient data loading with built-in compression.
"""

import logging
import random
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import time

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader


class HDF5Dataset(Dataset):
    """
    High-performance dataset using chunked HDF5 format.
    Assumes pre-normalized data for zero runtime overhead.
    """

    def __init__(
        self,
        hdf5_path: Path,
        split_name: str,  # "train", "validation", or "test"
        config: Dict[str, Any],
        device: torch.device,
    ):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        
        self.hdf5_path = Path(hdf5_path)
        self.split_name = split_name
        self.config = config
        self.device_type = device.type
        
        # Load metadata
        start_time = time.time()
        self.logger.info(f"Opening HDF5 file for {split_name} split...")
        
        with h5py.File(self.hdf5_path, 'r') as f:
            # Get split group
            if split_name not in f:
                raise ValueError(f"Split '{split_name}' not found in HDF5 file")
            
            split_group = f[split_name]
            
            # Get metadata
            self.n_samples = split_group.attrs['n_samples']
            self.n_species = split_group.attrs['n_species']
            self.n_globals = split_group.attrs['n_globals']
            
            # Verify data exists
            if 'inputs' not in split_group or 'targets' not in split_group:
                raise ValueError(f"Missing 'inputs' or 'targets' dataset in {split_name} split")
        
        # Setup for efficient access
        self._file_handle = None
        self._inputs_dset = None
        self._targets_dset = None
        
        # Cache configuration
        self.cache_size = 32  # Number of chunks to cache
        self._chunk_cache = {}
        self._cache_order = []
        
        load_time = time.time() - start_time
        self.logger.info(
            f"HDF5Dataset ready: {split_name} split with {self.n_samples:,} samples "
            f"(loaded metadata in {load_time:.2f}s)"
        )
    
    def _ensure_file_open(self):
        """Ensure HDF5 file is open with proper error handling."""
        if self._file_handle is None:
            try:
                # Use SWMR mode for concurrent access safety
                self._file_handle = h5py.File(self.hdf5_path, 'r', swmr=True)
                
                # Verify split exists
                if self.split_name not in self._file_handle:
                    raise ValueError(f"Split '{self.split_name}' not found in HDF5 file")
                    
                split_group = self._file_handle[self.split_name]
                
                # Verify datasets exist
                if 'inputs' not in split_group:
                    raise ValueError(f"'inputs' dataset missing in {self.split_name} split")
                if 'targets' not in split_group:
                    raise ValueError(f"'targets' dataset missing in {self.split_name} split")
                    
                self._inputs_dset = split_group['inputs']
                self._targets_dset = split_group['targets']
                
                # Verify dataset shapes
                if len(self._inputs_dset) != len(self._targets_dset):
                    raise ValueError(
                        f"Mismatched dataset sizes: inputs={len(self._inputs_dset)}, "
                        f"targets={len(self._targets_dset)}"
                    )
                
                # Log chunk info for debugging
                self.logger.debug(
                    f"HDF5 chunks - inputs: {self._inputs_dset.chunks}, "
                    f"targets: {self._targets_dset.chunks}"
                )
                
            except OSError as e:
                self.logger.error(f"Failed to open HDF5 file: {self.hdf5_path}")
                self.logger.error(f"Error: {e}")
                raise RuntimeError(f"Cannot open HDF5 file: {e}")
            except Exception as e:
                self.logger.error(f"Unexpected error opening HDF5 file: {e}")
                raise
    
    def __len__(self) -> int:
        """Return total number of samples."""
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a single sample with efficient HDF5 access.
        Optimized to avoid unnecessary copies for A100 GPU performance.
        """
        if idx >= self.n_samples:
            raise IndexError(f"Index {idx} out of range for split with {self.n_samples} samples")
        
        # Ensure file is open
        self._ensure_file_open()
        
        # Get chunk index and local index
        chunk_size = self._inputs_dset.chunks[0] if self._inputs_dset.chunks else 4096
        chunk_idx = idx // chunk_size
        local_idx = idx % chunk_size
        
        # Try to get from cache
        if chunk_idx in self._chunk_cache:
            inputs_chunk, targets_chunk = self._chunk_cache[chunk_idx]
            # Update cache order (LRU)
            self._cache_order.remove(chunk_idx)
            self._cache_order.append(chunk_idx)
        else:
            # Load chunk
            chunk_start = chunk_idx * chunk_size
            chunk_end = min(chunk_start + chunk_size, self.n_samples)
            
            # Read full chunk (more efficient than single row)
            inputs_chunk = self._inputs_dset[chunk_start:chunk_end]
            targets_chunk = self._targets_dset[chunk_start:chunk_end]
            
            # Add to cache
            self._chunk_cache[chunk_idx] = (inputs_chunk, targets_chunk)
            self._cache_order.append(chunk_idx)
            
            # Evict oldest chunk if cache is full
            if len(self._chunk_cache) > self.cache_size:
                oldest_chunk = self._cache_order.pop(0)
                del self._chunk_cache[oldest_chunk]
        
        # Extract the specific sample
        input_arr = inputs_chunk[local_idx]
        target_arr = targets_chunk[local_idx]
        
        # Convert to tensors efficiently
        # For contiguous arrays, torch.from_numpy doesn't copy
        if input_arr.flags['C_CONTIGUOUS']:
            input_tensor = torch.from_numpy(input_arr)
        else:
            input_tensor = torch.from_numpy(np.ascontiguousarray(input_arr))
            
        if target_arr.flags['C_CONTIGUOUS']:
            target_tensor = torch.from_numpy(target_arr)
        else:
            target_tensor = torch.from_numpy(np.ascontiguousarray(target_arr))
        
        # Pin memory only for CUDA devices
        if self.device_type == "cuda":
            input_tensor = input_tensor.pin_memory()
            target_tensor = target_tensor.pin_memory()
        
        return input_tensor, target_tensor
    
    def get_batch_info(self) -> Dict[str, Any]:
        """Get information about dataset structure for logging."""
        return {
            "format": "hdf5_chunked",
            "file": str(self.hdf5_path),
            "split": self.split_name,
            "n_samples": self.n_samples,
            "n_species": self.n_species,
            "n_globals": self.n_globals
        }
    
    def __del__(self):
        """Clean up HDF5 file handle."""
        if self._file_handle is not None:
            try:
                self._file_handle.close()
            except:
                pass


def worker_init_fn(worker_id: int):
    """Initialize worker with proper random seed."""
    worker_seed = torch.initial_seed() % 2**32 + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def create_dataloader(
    dataset: Dataset,
    config: Dict[str, Any],
    *,
    shuffle: bool = True,
    device: Optional[torch.device] = None
) -> DataLoader:
    """
    Construct a DataLoader whose defaults adapt to the host hardware.
    Fixed multiprocessing initialization to avoid conflicts.
    """
    train_cfg = config["training"]
    device_type = (device or torch.device("cuda")).type
    
    from utils.hardware import optimize_dataloader_settings
    opts = optimize_dataloader_settings(
        batch_size=train_cfg["batch_size"],
        device_type=device_type,
        num_workers=train_cfg.get("num_workers"),
    )
    
    # Override with config values if specified
    opts["num_workers"] = train_cfg.get("num_workers", min(16, opts["num_workers"]))
    opts["prefetch_factor"] = None if opts["num_workers"] == 0 else min(
        4, max(2, opts["num_workers"] // 4)
    )
    
    # Setup multiprocessing safely
    if opts["num_workers"] > 0:
        import torch.multiprocessing as mp
        try:
            # Only set if not already set
            if mp.get_start_method(allow_none=True) is None:
                mp.set_start_method("forkserver")
        except RuntimeError:
            # Already set, which is fine
            pass
    
    kwargs = dict(
        dataset=dataset,
        batch_size=opts["batch_size"],
        shuffle=shuffle,
        num_workers=opts["num_workers"],
        pin_memory=opts["pin_memory"],
        persistent_workers=opts["persistent_workers"] if opts["num_workers"] > 0 else False,
        drop_last=True,
        worker_init_fn=worker_init_fn if opts["num_workers"] > 0 else None,
    )
    
    if opts["prefetch_factor"] is not None:
        kwargs["prefetch_factor"] = int(opts["prefetch_factor"])
    
    # Set CUDA memory fraction if using pinned memory
    if opts["pin_memory"] and device_type == "cuda":
        try:
            torch.cuda.set_per_process_memory_fraction(0.9)
        except AttributeError:  # older PyTorch
            pass
    
    logging.getLogger(__name__).info(
        f"DataLoader with {opts['num_workers']} worker(s), "
        f"prefetch={kwargs.get('prefetch_factor', 'N/A')}"
    )
    
    return DataLoader(**kwargs)

===== /Users/imalsky/Desktop/Chemulator/src/data/normalizer.py =====
#!/usr/bin/env python3
"""
Data normalization module for chemical kinetics datasets.

Provides efficient normalization with multiple methods
and numerical stability guarantees.
Used only during preprocessing; no runtime calls.
"""

import logging
import math
from typing import Dict, List, Any, Optional

import numpy as np
import torch
import time


# Normalization constants
DEFAULT_EPSILON = 1e-37
DEFAULT_MIN_STD = 1e-10
DEFAULT_CLAMP = 100.0
DEFAULT_SYMLOG_PERCENTILE = 0.5
FIXED_RESERVOIR_SIZE = 1000000  # Fixed size for symlog quantile estimation

FLOAT32_MAX_EXPONENT = 38  # log10(float32_max) ≈ 38.5

class DataNormalizer:
    """
    Calculate normalization statistics from data during preprocessing.
    """

    def __init__(self,
                 config: Dict[str, Any],
                 *,
                 actual_timesteps: int) -> None:
        """
        Initialize the normalizer.

        Args:
            config: Full configuration dictionary.
            actual_timesteps: Number of timesteps in the data (for reservoir sizing).
        """
        self.config        = config
        self.data_config   = config["data"]
        self.norm_config   = config["normalization"]

        # Variable lists
        self.species_vars  = self.data_config["species_variables"]
        self.global_vars   = self.data_config["global_variables"]
        self.time_var      = self.data_config["time_variable"]
        self.all_vars      = self.species_vars + self.global_vars + [self.time_var]

        # Numerical constants
        self.epsilon       = self.norm_config.get("epsilon", 1e-37)
        self.min_std       = self.norm_config.get("min_std", 1e-10)

        self._actual_timesteps = actual_timesteps

        self.logger = logging.getLogger(__name__)
        
    def _initialize_accumulators(self, n_profiles: int) -> Dict[str, Dict[str, Any]]:
        """
        Initialise per-variable statistics accumulators.
        Uses fixed reservoir size for simplicity and to avoid dynamic estimation issues.
        """
        self.logger.info(
            f"Using fixed reservoir size {FIXED_RESERVOIR_SIZE:,} for symlog quantile estimation"
        )

        accumulators: Dict[str, Dict[str, Any]] = {}
        for i, var in enumerate(self.all_vars):
            method = self._get_method(var)
            if method == "none":
                continue
            acc = dict(
                method     = method,
                index      = i,
                count      = 0,
                mean       = 0.0,
                m2         = 0.0,
                min        = float("inf"),
                max        = float("-inf"),
                reservoir  = ReservoirSampler(FIXED_RESERVOIR_SIZE, seed=self.config["system"]["seed"]) if method == "symlog" else None,
            )
            accumulators[var] = acc
        return accumulators
    
    def _get_method(self, var: str) -> str:
        """Get normalization method for a variable."""
        methods = self.norm_config.get("methods", {})
        return methods.get(var, self.norm_config["default_method"])
    
    def _update_accumulators(
        self, 
        data: np.ndarray,
        accumulators: Dict[str, Dict[str, Any]],
        n_timesteps: int
    ):
        """
        Update accumulators with a chunk of data using Chan et al.'s parallel algorithm.
        Assumes data shape: (n_profiles, n_timesteps, n_vars)
        """
        n_profiles, n_t_check, _ = data.shape
        if n_t_check != n_timesteps:
            raise ValueError(f"Mismatched timesteps: expected {n_timesteps}, got {n_t_check}")
        
        # Optional profiling if anomaly detection enabled
        if self.config["system"].get("detect_anomaly", False):
            with torch.profiler.profile(with_stack=True, profile_memory=True) as prof:
                self._perform_update_accumulators(data, accumulators, n_timesteps)
            prof.export_chrome_trace("update_accumulators_trace.json")
            self.logger.info("Profiling trace saved to update_accumulators_trace.json")
        else:
            self._perform_update_accumulators(data, accumulators, n_timesteps)

    def _perform_update_accumulators(
            self,
            data: np.ndarray,
            accumulators: Dict[str, Dict[str, Any]],
            n_timesteps: int
        ):
            """Helper for core update logic, separable for profiling."""
            n_profiles, n_t_check, _ = data.shape
            if n_t_check != n_timesteps:
                raise ValueError(f"Mismatched timesteps: expected {n_timesteps}, got {n_t_check}")

            if n_timesteps > 10000:
                start_time = time.time()
                self.logger.info(f"Starting accumulator update for large group ({n_timesteps:,} timesteps)")

            for var, acc in accumulators.items():
                var_idx = acc["index"]
                method = acc["method"]
                
                if var in self.global_vars:
                    # Globals are constant, use scalar value for efficiency
                    value = data[0, 0, var_idx]
                    value = float(value)  # Ensure scalar
                    n_b = n_profiles * n_timesteps
                    # Apply transformation if needed
                    if method in ["log-standard", "log-min-max"]:
                        value = np.log10(np.maximum(value, self.epsilon))
                    
                    mean_b = value
                    m2_b = 0.0
                    min_b = max_b = value
                    
                    # Filter invalid (though globals usually valid)
                    if not np.isfinite(value):
                        continue
                    
                    # Update min/max
                    acc["min"] = min(acc["min"], value)
                    acc["max"] = max(acc["max"], value)
                    
                    # Chan update
                    if acc["count"] == 0:
                        acc["count"] = n_b
                        acc["mean"] = mean_b
                        acc["m2"] = 0.0
                    else:
                        n_a = acc["count"]
                        mean_a = acc["mean"]
                        m2_a = acc["m2"]
                        
                        delta = mean_b - mean_a
                        n_ab = n_a + n_b
                        mean_ab = mean_a + delta * n_b / n_ab
                        m2_ab = m2_a + delta**2 * n_a * n_b / n_ab  # m2_b=0
                        
                        acc["count"] = n_ab
                        acc["mean"] = float(mean_ab)
                        acc["m2"] = float(m2_ab)
                    
                    # Reservoir for symlog (add once since constant)
                    if acc["reservoir"] is not None:
                        acc["reservoir"].add_samples(np.array([value]))
                else:
                    # Species/time: original logic
                    var_data = data[:, :, var_idx].flatten().astype(np.float64)
                    
                    valid_mask = np.isfinite(var_data)
                    var_data = var_data[valid_mask]
                    
                    if len(var_data) == 0:
                        continue
                    
                    if method in ["log-standard", "log-min-max"]:
                        var_data = np.log10(np.maximum(var_data, self.epsilon))
                    
                    n_b = len(var_data)
                    mean_b = np.mean(var_data, dtype=np.float64)
                    
                    acc["min"] = min(acc["min"], float(np.min(var_data)))
                    acc["max"] = max(acc["max"], float(np.max(var_data)))
                    
                    # Chan update
                    if acc["count"] == 0:
                        acc["count"] = n_b
                        acc["mean"] = float(mean_b)
                        if n_b > 1:
                            acc["m2"] = float(np.sum((var_data - mean_b)**2, dtype=np.float64))
                        else:
                            acc["m2"] = 0.0
                    else:
                        n_a = acc["count"]
                        mean_a = acc["mean"]
                        m2_a = acc["m2"]
                        
                        delta = mean_b - mean_a
                        mean_ab = mean_a + delta * n_b / (n_a + n_b)
                        
                        if n_b > 1:
                            m2_b = float(np.sum((var_data - mean_b)**2, dtype=np.float64))
                        else:
                            m2_b = 0.0
                        
                        m2_ab = m2_a + m2_b + delta**2 * n_a * n_b / (n_a + n_b)
                        
                        acc["count"] = n_a + n_b
                        acc["mean"] = float(mean_ab)
                        acc["m2"] = float(m2_ab)
                    
                    if method == "symlog" and acc["reservoir"]:
                        acc["reservoir"].add_samples(var_data)
            
            if n_timesteps > 10000:
                end_time = time.time()
                self.logger.info(f"Accumulator update completed in {end_time - start_time:.2f} seconds")
    
    def _finalize_statistics(self, accumulators: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Finalize statistics from accumulators."""
        stats = {
            "normalization_methods": {},
            "per_key_stats": {}
        }
        
        for var, acc in accumulators.items():
            method = acc["method"]
            stats["normalization_methods"][var] = method
            
            if method == "none":
                continue
            
            var_stats = {"method": method}
            
            # Calculate standard deviation
            if acc["count"] > 1:
                variance = acc["m2"] / (acc["count"] - 1)
                std = max(math.sqrt(variance), self.min_std)
            else:
                std = 1.0
            
            # Store statistics based on method
            if method == "standard":
                var_stats["mean"] = acc["mean"]
                var_stats["std"] = std
                
            elif method == "log-standard":
                var_stats["log_mean"] = acc["mean"]
                var_stats["log_std"] = std
                
            elif method == "log-min-max":
                var_stats["min"] = acc["min"]
                var_stats["max"] = acc["max"]
                if acc["max"] - acc["min"] < self.epsilon:
                    var_stats["max"] = acc["min"] + 1.0
                
            elif method == "symlog":
                if acc["reservoir"] and acc["reservoir"].size > 0:
                    values = acc["reservoir"].get_samples()
                    percentile = self.norm_config.get("symlog_percentile", DEFAULT_SYMLOG_PERCENTILE)
                    threshold = np.percentile(np.abs(values), percentile * 100)
                    var_stats["threshold"] = max(float(threshold), self.epsilon)
                    
                    abs_vals = np.abs(values)
                    linear_mask = abs_vals <= var_stats["threshold"]
                    transformed = np.zeros_like(values)
                    
                    transformed[linear_mask] = values[linear_mask] / var_stats["threshold"]
                    log_vals = np.maximum(abs_vals[~linear_mask] / var_stats["threshold"], self.epsilon)
                    transformed[~linear_mask] = np.sign(values[~linear_mask]) * (
                        np.log10(log_vals) + 1
                    )
                    
                    var_stats["scale_factor"] = max(float(np.max(np.abs(transformed))), 1.0)
                else:
                    var_stats["threshold"] = 1.0
                    var_stats["scale_factor"] = 1.0
            
            stats["per_key_stats"][var] = var_stats
        
        # Add methods for variables not in accumulators
        for var in self.all_vars:
            if var not in stats["normalization_methods"]:
                stats["normalization_methods"][var] = "none"
        
        stats["epsilon"] = self.epsilon
        stats["clamp_value"] = self.norm_config.get("clamp_value", DEFAULT_CLAMP)
        
        return stats


class ReservoirSampler:
    """
    Efficient reservoir sampling for quantile estimation.
    Memory-bounded implementation suitable for large datasets.
    """
    
    def __init__(self, capacity: int, seed: int):
        # Validate capacity to prevent memory issues
        if capacity > 10_000_000:  # 10M samples max
            logging.getLogger(__name__).warning(
                f"Reservoir capacity {capacity} is very large, capping at 10M samples"
            )
            capacity = 10_000_000
            
        self.capacity = capacity
        self.reservoir = []
        self.count = 0
        # Initialize RNG with fixed seed for reproducibility
        self.rng = np.random.RandomState(seed)
        
        # Pre-allocate for efficiency if capacity is known
        if capacity <= 1_000_000:
            self.reservoir = [0.0] * capacity
            self.size = 0
        else:
            self.reservoir = []
            self.size = 0
    
    def add_samples(self, samples: np.ndarray):
        """Add samples to the reservoir with memory-efficient batching."""
        # Process in chunks for very large sample arrays
        chunk_size = 100_000
        
        if len(samples) > chunk_size:
            for i in range(0, len(samples), chunk_size):
                chunk = samples[i:i + chunk_size]
                self._add_chunk(chunk)
        else:
            self._add_chunk(samples)
    
    def _add_chunk(self, samples: np.ndarray):
        """Add a chunk of samples to the reservoir."""
        for sample in samples:
            self.count += 1
            
            if self.size < self.capacity:
                # Fill reservoir
                if isinstance(self.reservoir, list) and len(self.reservoir) == self.capacity:
                    # Pre-allocated list
                    self.reservoir[self.size] = float(sample)
                else:
                    # Dynamic list
                    self.reservoir.append(float(sample))
                self.size += 1
            else:
                # Randomly replace
                j = self.rng.randint(0, self.count)
                if j < self.capacity:
                    self.reservoir[j] = float(sample)
    
    def get_samples(self) -> np.ndarray:
        """Get reservoir samples as numpy array."""
        if isinstance(self.reservoir, list) and self.size < len(self.reservoir):
            # Pre-allocated list not fully filled
            return np.array(self.reservoir[:self.size], dtype=np.float64)
        else:
            return np.array(self.reservoir, dtype=np.float64)


class NormalizationHelper:
    """
    Normalization helper for use during preprocessing.
    """

    def __init__(self,
                 stats: Dict[str, Any],
                 device: torch.device,
                 species_vars: List[str],
                 global_vars: List[str],
                 time_var: str,
                 config: Optional[Dict[str, Any]] = None):
        """
        Initialize the normalization helper.

        Args:
            stats: Pre-computed normalization statistics
            device: Target device for operations (usually CPU)
            species_vars: List of species variable names
            global_vars: List of global variable names
            time_var: Time variable name
            config: Full configuration dictionary (optional)
        """
        self.stats = stats
        self.device = device
        self.species_vars = species_vars
        self.global_vars = global_vars
        self.time_var = time_var

        # Use lengths to compute column offsets
        self.n_species = len(species_vars)
        self.n_globals = len(global_vars)
        self.methods = stats["normalization_methods"]
        self.per_key_stats = stats["per_key_stats"]

        # Numerical constants from stats or config or defaults
        self.epsilon = stats.get("epsilon", DEFAULT_EPSILON)

        # Get clamp_value from stats first, then config, then default
        self.clamp_value = stats.get("clamp_value", DEFAULT_CLAMP)
        if config and "normalization" in config:
            self.clamp_value = config["normalization"].get("clamp_value", self.clamp_value)

        # Pre-compute normalization parameters on device
        self._precompute_parameters()

        # Pre-compute for sample batch normalization
        self._precompute_sample_columns()

    def _precompute_parameters(self):
        """Pre-compute normalization parameters for efficiency."""
        self.norm_params = {}

        # Group variables by normalization method for vectorized operations
        self.method_groups = {
            "standard": [],
            "log-standard": [],
            "log-min-max": [],
            "symlog": [],
            "none": []
        }

        # Create parameter tensors for each variable
        for var, method in self.methods.items():
            if method == "none" or var not in self.per_key_stats:
                self.method_groups["none"].append(var)
                continue

            var_stats = self.per_key_stats[var]
            params = {"method": method}

            if method == "standard":
                params["mean"] = torch.tensor(var_stats["mean"], dtype=torch.float32, device=self.device)
                params["std"] = torch.tensor(var_stats["std"], dtype=torch.float32, device=self.device)

            elif method == "log-standard":
                params["log_mean"] = torch.tensor(var_stats["log_mean"], dtype=torch.float32, device=self.device)
                params["log_std"] = torch.tensor(var_stats["log_std"], dtype=torch.float32, device=self.device)

            elif method == "log-min-max":
                params["min"] = torch.tensor(var_stats["min"], dtype=torch.float32, device=self.device)
                params["max"] = torch.tensor(var_stats["max"], dtype=torch.float32, device=self.device)

            elif method == "symlog":
                params["threshold"] = torch.tensor(var_stats["threshold"], dtype=torch.float32, device=self.device)
                params["scale_factor"] = torch.tensor(var_stats["scale_factor"], dtype=torch.float32, device=self.device)

            self.norm_params[var] = params
            self.method_groups[method].append(var)

        # Pre-compute column indices for each method group
        self._compute_column_indices()

    def _compute_column_indices(self):
        """Pre-compute column indices for vectorized operations."""
        self.col_indices = {}

        all_vars = self.species_vars + self.global_vars + [self.time_var]
        var_to_col = {var: i for i, var in enumerate(all_vars)}

        for method, vars_list in self.method_groups.items():
            if vars_list:
                self.col_indices[method] = [var_to_col[var] for var in vars_list]

    def _precompute_sample_columns(self):
        """Pre-compute variable mapping and indices for sample batches."""
        # Sample columns: species_init + globals + time + species_target
        self.sample_col_vars = self.species_vars + self.global_vars + [self.time_var] + self.species_vars

        # Group sample columns by method
        self.sample_col_indices = {}
        for method in self.method_groups:
            self.sample_col_indices[method] = [i for i, var in enumerate(self.sample_col_vars) if self.methods.get(var, "none") == method]

    def normalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """
        Normalize a complete profile tensor using vectorized operations.

        Args:
            profile: Tensor of shape (timesteps, n_species + n_globals + 1)
        Returns:
            Normalized profile tensor
        """
        # Ensure profile is on the same device as normalization parameters
        if profile.device != self.device:
            profile = profile.to(self.device)

        normalized = profile.clone()

        # Apply normalization for each method group (vectorized)
        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none":
                continue

            # Get columns for this method
            cols = normalized[:, col_idxs]

            if method == "standard":
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                normalized[:, col_idxs] = torch.clamp(
                    (cols - means) / stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-standard":
                log_means = torch.stack([self.norm_params[var]["log_mean"] for var in self.method_groups[method]])
                log_stds = torch.stack([self.norm_params[var]["log_std"] for var in self.method_groups[method]])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                normalized[:, col_idxs] = torch.clamp(
                    (log_data - log_means) / log_stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-min-max":
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                ranges = maxs - mins
                ranges = torch.clamp(ranges, min=self.epsilon)
                normalized[:, col_idxs] = torch.clamp((log_data - mins) / ranges, 0.0, 1.0)

            elif method == "symlog":
                for i, var in enumerate(self.method_groups[method]):
                    params = self.norm_params[var]
                    col_idx = col_idxs[i]
                    normalized[:, col_idx] = self._symlog_transform(
                        profile[:, col_idx], 
                        params["threshold"], 
                        params["scale_factor"]
                    )

        return normalized

    def normalize_batch(self, batch: torch.Tensor) -> torch.Tensor:
        """
        Normalize a batch of samples using vectorized operations.

        Args:
            batch: Tensor of shape (batch_size, 2*n_species + n_globals + 1)
        Returns:
            Normalized batch tensor
        """
        if batch.device != self.device:
            batch = batch.to(self.device)

        normalized = batch.clone()

        # Apply normalization for each method group on sample columns (vectorized)
        for method, col_idxs in self.sample_col_indices.items():
            if not col_idxs or method == "none":
                continue

            cols = normalized[:, col_idxs]

            if method == "standard":
                var_list = [self.sample_col_vars[col] for col in col_idxs]
                means = torch.stack([self.norm_params[v]["mean"] for v in var_list])
                stds = torch.stack([self.norm_params[v]["std"] for v in var_list])
                normalized[:, col_idxs] = torch.clamp(
                    (cols - means) / stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-standard":
                var_list = [self.sample_col_vars[col] for col in col_idxs]
                log_means = torch.stack([self.norm_params[v]["log_mean"] for v in var_list])
                log_stds = torch.stack([self.norm_params[v]["log_std"] for v in var_list])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                normalized[:, col_idxs] = torch.clamp(
                    (log_data - log_means) / log_stds,
                    -self.clamp_value, self.clamp_value
                )

            elif method == "log-min-max":
                var_list = [self.sample_col_vars[col] for col in col_idxs]
                mins = torch.stack([self.norm_params[v]["min"] for v in var_list])
                maxs = torch.stack([self.norm_params[v]["max"] for v in var_list])
                log_data = torch.log10(torch.clamp(cols, min=self.epsilon))
                ranges = maxs - mins
                ranges = torch.clamp(ranges, min=self.epsilon)
                normalized[:, col_idxs] = torch.clamp((log_data - mins) / ranges, 0.0, 1.0)

            elif method == "symlog":
                for i, col_idx in enumerate(col_idxs):
                    var = self.sample_col_vars[col_idx]
                    params = self.norm_params[var]
                    normalized[:, col_idx] = self._symlog_transform(
                        batch[:, col_idx],
                        params["threshold"],
                        params["scale_factor"]
                    )

        return normalized

    def denormalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """
        Denormalize a complete profile tensor using vectorized operations.

        Args:
            profile: Normalized tensor of shape (timesteps, n_species + n_globals + 1)
        Returns:
            Denormalized profile tensor in original units
        """
        if profile.device != self.device:
            profile = profile.to(self.device)

        denormalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none":
                continue

            cols = denormalized[:, col_idxs]

            if method == "standard":
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                denormalized[:, col_idxs] = cols * stds + means

            elif method == "log-standard":
                log_means = torch.stack([self.norm_params[var]["log_mean"] for var in self.method_groups[method]])
                log_stds = torch.stack([self.norm_params[var]["log_std"] for var in self.method_groups[method]])
                log_data = cols * log_stds + log_means
                denormalized[:, col_idxs] = torch.pow(10.0, log_data)

            elif method == "log-min-max":
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = maxs - mins
                ranges = torch.clamp(ranges, min=self.epsilon)
                log_data = cols * ranges + mins
                denormalized[:, col_idxs] = torch.pow(10.0, log_data)

            elif method == "symlog":
                for i, var in enumerate(self.method_groups[method]):
                    params = self.norm_params[var]
                    col_idx = col_idxs[i]
                    denormalized[:, col_idx] = self._symlog_inverse(
                        profile[:, col_idx],
                        params["threshold"],
                        params["scale_factor"]
                    )

        return denormalized

    def _symlog_transform(self, data: torch.Tensor, threshold: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
        """Apply symlog transformation with optimized vectorized operations."""
        abs_data = torch.abs(data)
        sign_data = torch.sign(data)

        linear_part = data / threshold
        log_arg = torch.clamp(abs_data / threshold, min=self.epsilon)
        log_part = sign_data * (torch.log10(log_arg) + 1.0)

        result = torch.where(
            abs_data <= threshold,
            linear_part,
            log_part
        ) / scale

        return torch.clamp(result, -self.clamp_value, self.clamp_value)

    def _symlog_inverse(self, data: torch.Tensor, threshold: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
        """
        Apply inverse symlog transformation with robust overflow protection.
        Prevents numerical overflow for extreme values on A100 GPUs.
        """
        scaled_data = data * scale
        abs_scaled = torch.abs(scaled_data)
        sign_scaled = torch.sign(scaled_data)

        # Linear region: |scaled_data| <= 1
        linear_part = scaled_data * threshold
        
        # Log region: |scaled_data| > 1
        # Clamp exponent to prevent overflow
        exponent = abs_scaled - 1.0
        max_safe_exponent = FLOAT32_MAX_EXPONENT - 1  # Leave margin for safety
        
        # Log extreme values that would overflow
        extreme_mask = exponent > max_safe_exponent
        if extreme_mask.any():
            self.logger.warning(
                f"Clamping {extreme_mask.sum().item()} extreme values to prevent overflow "
                f"(max exponent: {exponent.max().item():.2f})"
            )
        
        clamped_exponent = torch.clamp(exponent, max=max_safe_exponent)
        
        # Compute log part with clamped exponent
        log_part = sign_scaled * threshold * torch.pow(10.0, clamped_exponent)

        # Combine linear and log regions
        result = torch.where(
            abs_scaled <= 1.0,
            linear_part,
            log_part
        )

        return result

