===== /Users/imalsky/Desktop/Chemulator/config/config.jsonc =====
{
    // ===== FILE PATHS =====
    "paths": {
        // Raw HDF5 data files to process
        "raw_data_files": [
            "data/raw/run9001-result.h5",
            //"data/raw/run9002-result.h5",
            //"data/raw/run9003-result.h5",
            //"data/raw/run9004-result.h5",
            //"data/raw/run9005-result.h5",
            //"data/raw/run9006-result.h5",
            //"data/raw/run9007-result.h5",
            //"data/raw/run9008-result.h5",
            //"data/raw/run9009-result.h5",
            //"data/raw/run9010-result.h5"
        ],
        // Directory for processed NPY shards
        "processed_data_dir": "data/processed",
        // Directory for saved models
        "model_save_dir": "data/models",
        // Directory for logs
        "log_dir": "logs"
    },
    
    // ===== DATA CONFIGURATION =====
    "data": {
        // Chemical species to predict (order matters!)
        "species_variables": [
            "C2H2_evolution",
            "CH4_evolution",
            "CO2_evolution",
            "CO_evolution",
            "H2O_evolution",
            "H2_evolution",
            "HCN_evolution",
            "H_evolution",
            "N2_evolution",
            "NH3_evolution",
            "OH_evolution",
            "O_evolution"
        ],
        // Global parameters (initial conditions)
        "global_variables": ["P_init", "T_init"],
        // Time variable name in HDF5 files
        "time_variable": "t_time"
    },
    
    // ===== PREPROCESSING SETTINGS =====
    "preprocessing": {
        // Number of samples per NPY shard file
        "shard_size": 1000000,
        // Minimum species concentration threshold
        "min_value_threshold": 1e-25,
        // Compression type: null for raw npy files (faster I/O)
        "compression": null,
        // Number of parallel workers for preprocessing
        "num_workers": 4,
        // Enable parallel preprocessing
        "parallel_enabled": true
    },
    
    // ===== NORMALIZATION SETTINGS =====
    "normalization": {
        // Default normalization method for all variables
        "default_method": "log-min-max",
        
        // Override methods for specific variables
        "methods": {
            "T_init": "standard",
            "P_init": "log-min-max",
            "t_time": "log-min-max"
        },
        
        // This needs to be fixed
        "epsilon": 1e-38,
        // Minimum standard deviation to prevent division by tiny values
        "min_std": 1e-10,
        // Clamp normalized values to [-clamp_value, clamp_value]
        "clamp_value": 50.0
    },
    
    // ===== MODEL ARCHITECTURE =====
    "model": {
        // Model type: "deeponet" or "siren" (both work with absolute mode)
        "type": "deeponet",
        
        // Activation function: "gelu", "relu", "silu", "tanh"
        "activation": "tanh",
        
        // Dropout rate (0.0 = no dropout)
        "dropout": 0.0,
        
        // Output scaling factor (1.0 = no scaling)
        "output_scale": 1.0,
        
        // DeepONet-specific parameters
        "branch_layers": [512, 512, 512, 512],
        "trunk_layers": [64, 64, 64, 64],
        "basis_dim": 32,
        
        // SIREN-specific parameters (when type="siren")
        "hidden_dims": [256, 256, 256],
        "omega_0": 30.0
    },
    
    // ===== FiLM CONDITIONING =====
    "film": {
        // Enable Feature-wise Linear Modulation
        "enabled": false,
        // Hidden layers for FiLM networks
        "hidden_dims": [64],
        // Activation for FiLM networks
        "activation": "gelu"
    },
    
    // ===== PREDICTION SETTINGS =====
    "prediction": {
        // Prediction mode: ABSOLUTE ONLY for this config
        // ratio mode is still in progress
        "mode": "absolute",
        
        // Optional output clamping
        "output_clamp": null
    },
    
    // ===== TRAINING PARAMETERS =====
    "training": {
        // Data splitting
        "val_fraction": 0.15,
        "test_fraction": 0.15,
        "use_fraction": 1.0,
        
        // Training duration
        "epochs": 100,  // Full training epochs (not used in HPO)
        "batch_size": 16384,
        "gradient_accumulation_steps": 1,
        
        // MEMORY-OPTIMIZED DATALOADER SETTINGS
        "num_workers": 8,
        "pin_memory": true,
        "persistent_workers": true,
        "prefetch_factor": 2,
        "drop_last": true,
        "dataset_cache_shards": 64,
        
        // Learning rate and optimizer settings
        "learning_rate": 5e-4,  // Starting LR for HPO
        "weight_decay": 1e-5,
        "betas": [0.9, 0.999],
        "eps": 1e-8,
        "gradient_clip": 1.0,
        
        // Scheduler settings - CRITICAL FOR LR DECAY
        "scheduler": "cosine",
        "scheduler_params": {
            "T_0": 50,  // Restart every 8 epochs (5 cycles in 40 epochs)
            "T_mult": 2,  // Keep cycle length constant
            "eta_min": 1e-8  // ENSURES LR DROPS TO 1e-8
        },
        
        // Loss and training settings
        "loss": "mse",
        "huber_delta": 0.5,
        "use_amp": true,
        "amp_dtype": "bfloat16",
        "early_stopping_patience": 10,  // Stop if no improvement for 8 epochs
        "min_delta": 1e-8,
        "log_interval": 100,
        "save_interval": 10,
        "empty_cache_interval": 500,
        
        // HPO-specific settings
        "hpo_min_epochs": 10,  // Minimum epochs before pruning
        "hpo_max_epochs": 40   // Maximum epochs for best trials
    },
    
    // ===== SYSTEM/HARDWARE SETTINGS =====
    "system": {
        // Random seed for reproducibility
        "seed": 42,
        
        // PyTorch optimizations
        "use_torch_compile": true,
        "compile_mode": "default",
        "use_torch_export": true,
        
        // CUDA optimizations for A100
        "cudnn_benchmark": true,
        "tf32": true,
        "cuda_memory_fraction": 0.90
    },
    
    // ===== HYPERPARAMETER OPTIMIZATION =====
    "optuna": {
        // Enable Optuna integration
        "enabled": true,
        
        // Hyperband settings for 40-hour window
        "algorithm": "hyperband",
        "hyperband_min_resource": 10,  // 1.4 hours minimum
        "hyperband_max_resource": 40,  // 5.5 hours maximum
        "hyperband_reduction_factor": 3,  // Keep top 33% at each stage
        
        // Target number of trials for 40 hours
        "n_trials": 100  // Conservative estimate with pruning
    }
}

===== /Users/imalsky/Desktop/Chemulator/src/main.py =====
#!/usr/bin/env python3
import logging
import sys
import time
from pathlib import Path
import torch
from typing import Dict, Any, Union
import hashlib
import json

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(name)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    stream=sys.stdout
)

# 2. Change the multiprocessing sharing strategy to prevent /dev/shm crashes.
import torch.multiprocessing
try:
    torch.multiprocessing.set_sharing_strategy('file_system')
    logging.info("SUCCESS: Set multiprocessing sharing strategy to 'file_system'.")
except RuntimeError:
    logging.warning("Could not set multiprocessing sharing strategy (already set or not supported).")

import numpy as np
from utils.hardware import setup_device, optimize_hardware
from utils.utils import setup_logging, seed_everything, ensure_directories, load_json_config, save_json, load_json
from data.preprocessor import DataPreprocessor
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer
from data.normalizer import NormalizationHelper


class ChemicalKineticsPipeline:
    """Training pipeline for chemical kinetics prediction."""
    def __init__(self, config_or_path: Union[Path, Dict[str, Any]]):
        """
        Initialize the pipeline with either a config file path or a config dictionary.
        
        Args:
            config_or_path: Either a Path to a config file or a config dictionary
        """
        if isinstance(config_or_path, (Path, str)):
            self.config = load_json_config(Path(config_or_path))
        elif isinstance(config_or_path, dict):
            self.config = config_or_path
        else:
            raise TypeError(f"config_or_path must be a Path, str, or dict, not {type(config_or_path)}")
        
        # Get prediction mode
        self.prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        
        # Setup paths with mode-specific directories
        self.setup_paths()
        
        # Setup logging
        log_file = self.log_dir / f"pipeline_{self.prediction_mode}_{time.strftime('%Y%m%d_%H%M%S')}.log"
        setup_logging(log_file=log_file)
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Chemical Kinetics Pipeline initialized - Mode: {self.prediction_mode}")
        
        seed_everything(self.config["system"]["seed"])
        
        # Setup hardware
        self.device = setup_device()
        optimize_hardware(self.config["system"], self.device)
        
    def setup_paths(self):
        """Create directory structure with mode-specific paths."""
        paths = self.config["paths"]
        
        # Create run directory
        model_type = self.config["model"]["type"]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.run_save_dir = Path(paths["model_save_dir"]) / f"{model_type}_{self.prediction_mode}_{timestamp}"
        
        # Convert paths
        self.raw_data_files = [Path(f) for f in paths["raw_data_files"]]
        
        # Mode-specific processed directory
        base_processed_dir = Path(paths["processed_data_dir"])
        self.processed_dir = base_processed_dir / f"mode_{self.prediction_mode}"
        
        self.log_dir = Path(paths["log_dir"])
        
        # Create directories
        ensure_directories(self.processed_dir, self.run_save_dir, self.log_dir)

    def _compute_data_hash(self) -> str:
        """
        Compute a hash of data-critical parameters.
        Only includes parameters that affect the actual data content.
        """
        data_params = {
            "raw_files": sorted([str(f) for f in self.raw_data_files]),
            "species_variables": self.config["data"]["species_variables"],
            "global_variables": self.config["data"]["global_variables"],
            "time_variable": self.config["data"]["time_variable"],
            "min_value_threshold": self.config["preprocessing"]["min_value_threshold"],
            "use_fraction": self.config["training"]["use_fraction"],
            "prediction_mode": self.prediction_mode,
            "epsilon": self.config["normalization"]["epsilon"],
            "normalization_methods": self.config["normalization"].get("methods", {}),
            "default_norm_method": self.config["normalization"]["default_method"],
        }
        
        # Create stable JSON string
        hash_str = json.dumps(data_params, sort_keys=True)
        return hashlib.sha256(hash_str.encode()).hexdigest()[:16]

    def normalize_only(self):
            """Run only the data preprocessing and normalization step."""
            self.logger.info("Running data normalization only...")
            
            # Check if data already exists with correct hash
            current_hash = self._compute_data_hash()
            hash_file = self.processed_dir / "data_hash.json"
            
            regenerate = True
            if hash_file.exists():
                saved_hash_data = load_json(hash_file)
                if saved_hash_data.get("hash") == current_hash:
                    self.logger.info("Data already preprocessed with matching hash. Skipping regeneration.")
                    regenerate = False
                else:
                    self.logger.info("Data hash mismatch. Regenerating data...")
                    self._clean_all_processed_data()
            
            if regenerate:
                preprocessor = DataPreprocessor(
                    raw_files=self.raw_data_files,
                    output_dir=self.processed_dir,
                    config=self.config
                )
                
                missing = [p for p in self.raw_data_files if not p.exists()]
                if missing:
                    raise FileNotFoundError(f"Missing raw data files: {missing}")
                
                # Process to shards and compute normalization
                preprocessor.process_to_npy_shards()
                
                # Save the hash
                save_json({
                    "hash": current_hash,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "mode": self.prediction_mode
                }, hash_file)
                
                self.logger.info(f"Data normalization complete. Files saved to: {self.processed_dir}")

    def _generate_or_validate_splits(self):
        """Generate or validate train/val/test split indices."""
        split_config = {
            "val_fraction": self.config["training"]["val_fraction"],
            "test_fraction": self.config["training"]["test_fraction"],
            "use_fraction": self.config["training"]["use_fraction"],
            "seed": self.config["system"]["seed"]
        }
        current_split_hash = hashlib.sha256(
            json.dumps(split_config, sort_keys=True).encode('utf-8')
        ).hexdigest()[:16]
        
        split_hash_path = self.processed_dir / "split_hash.json"
        
        regenerate_splits = True
        if split_hash_path.exists():
            saved_split_hash = load_json(split_hash_path).get("hash")
            if saved_split_hash == current_split_hash:
                self.logger.info("Split configuration matches. Reusing existing splits.")
                regenerate_splits = False
        
        if regenerate_splits:
            self.logger.info("Generating new train/val/test splits...")
            preprocessor = DataPreprocessor(
                raw_files=self.raw_data_files,
                output_dir=self.processed_dir,
                config=self.config
            )
            preprocessor.generate_split_indices()
            save_json({"hash": current_split_hash}, split_hash_path)

    def _clean_all_processed_data(self):
        """Remove ALL processed files."""
        self.logger.info("Cleaning ALL old processed files...")
        if not self.processed_dir.exists():
            return
        
        patterns = ["shard_*.npy", "shard_*.npz", "*.json", "*_indices.npy"]
        removed_count = 0
        
        for pattern in patterns:
            for file in self.processed_dir.glob(pattern):
                try:
                    file.unlink()
                    removed_count += 1
                except Exception as e:
                    self.logger.warning(f"Failed to remove {file}: {e}")
        
        self.logger.info(f"Removed {removed_count} old files from {self.processed_dir}")

    def preprocess_data(self):
        """Preprocess data with proper hash checking."""
        self.logger.info(f"Preprocessing data for {self.prediction_mode} mode...")
        self.normalize_only()

    def train_model(self):
        """Train the neural network model."""
        self.logger.info("Starting model training...")

        # Ensure data is preprocessed
        self.preprocess_data()

        # Enforce mode-model compatibility
        prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        model_type = self.config["model"]["type"]
        if prediction_mode == "ratio" and model_type != "deeponet":
            raise ValueError(
                f"Prediction mode 'ratio' is only compatible with model type 'deeponet', "
                f"but '{model_type}' was specified."
            )

        # Save config for this run
        save_json(self.config, self.run_save_dir / "config.json")

        # Create model
        model = create_model(self.config, self.device)

        # Log model info
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"Model: {self.config['model']['type']} - Parameters: {total_params:,}")

        # Load normalization stats and create helper
        norm_stats = load_json(self.processed_dir / "normalization.json")
        norm_helper = NormalizationHelper(
            norm_stats,
            self.device,
            self.config["data"]["species_variables"],
            self.config["data"]["global_variables"],
            self.config["data"]["time_variable"],
            self.config
        )

        # Create datasets
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="train",
            config=self.config,
            device=self.device
        )
        
        val_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="validation",
            config=self.config,
            device=self.device
        ) if self.config["training"]["val_fraction"] > 0 else None
        
        test_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="test",
            config=self.config,
            device=self.device
        ) if self.config["training"]["test_fraction"] > 0 else None
        
        # Initialize trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device,
            norm_helper=norm_helper
        )
        
        # Warm up cache
        _ = train_dataset[0]

        # Train model
        best_val_loss = trainer.train()
        
        # Evaluate on test set
        test_loss = trainer.evaluate_test()
        
        self.logger.info(f"Training complete! Best validation loss: {best_val_loss:.6f}")
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        # Save results
        results = {
            "best_val_loss": best_val_loss,
            "test_loss": test_loss,
            "model_path": str(self.run_save_dir / "best_model.pt"),
            "training_time": trainer.total_training_time,
        }
        
        save_json(results, self.run_save_dir / "results.json")
    
    def run(self):
        """Execute the full training pipeline."""
        try:
            self.train_model()
            self.logger.info("Pipeline completed successfully!")
            self.logger.info(f"Results saved in: {self.run_save_dir}")
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            sys.exit(1)


def main():
    """Main entry point with multiple operation modes."""
    import argparse
    parser = argparse.ArgumentParser(description="Chemical Kinetics Neural Network Training")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/config.jsonc"),
        help="Path to configuration file"
    )
    
    # Operation mode arguments (mutually exclusive)
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument(
        "--normalize",
        action="store_true",
        help="Only preprocess and normalize the data"
    )
    mode_group.add_argument(
        "--train",
        action="store_true",
        help="Train a model using the configuration"
    )
    mode_group.add_argument(
        "--tune",
        action="store_true",
        help="Run hyperparameter optimization"
    )
    
    # Hyperparameter tuning specific arguments
    parser.add_argument(
        "--trials",
        type=int,
        default=100,
        help="Number of Optuna trials for hyperparameter optimization"
    )
    parser.add_argument(
        "--study-name",
        type=str,
        default="chemical_kinetics_opt",
        help="Name for Optuna study"
    )
    
    args = parser.parse_args()
    
    # Validate config path
    if not args.config.exists():
        print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
        sys.exit(1)
    
    # Execute based on mode
    if args.normalize:
        # Just normalize data
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.normalize_only()
        print("\nData normalization complete!")
        
    elif args.train:
        # Train model
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.run()
        
    elif args.tune:
        # Run hyperparameter optimization
        try:
            import optuna
        except ImportError:
            print("Installing optuna...")
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "optuna"])
        
        from hyperparameter_tuning import optimize
        
        print(f"Starting hyperparameter optimization with {args.trials} trials...")
        study = optimize(
            config_path=args.config,
            n_trials=args.trials,
            n_jobs=1,
            study_name=args.study_name
        )
        
        # Print results
        print("\n" + "="*60)
        print("Optimization Complete")
        print("="*60)
        print(f"Best validation loss: {study.best_value:.6f}")
        print(f"Best trial: {study.best_trial.number}")
        print("\nBest parameters:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")
        
        completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
        pruned = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
        print(f"\nTrials: {completed} completed, {pruned} pruned")
        print(f"\nBest configuration saved to: optuna_results/")


if __name__ == "__main__":
    main()

===== /Users/imalsky/Desktop/Chemulator/src/hyperparameter_tuning.py =====
#!/usr/bin/env python3
"""
Hyperparameter tuning for chemical kinetics models using Optuna.
Optimized for ~40 hour runtime with aggressive but smart pruning.
"""

import copy
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional, Callable

import optuna
from optuna.samplers import TPESampler
from optuna.pruners import HyperbandPruner
import torch

from main import ChemicalKineticsPipeline
from utils.hardware import setup_device, optimize_hardware
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer
from data.normalizer import NormalizationHelper
from utils.utils import setup_logging, seed_everything, ensure_directories, load_json_config, save_json, load_json


class OptunaPruningCallback:
    """Callback to report intermediate values to Optuna for pruning."""
    def __init__(self, trial: optuna.Trial, min_epochs: int = 10):
        self.trial = trial
        self.min_epochs = min_epochs
        
    def __call__(self, epoch: int, val_loss: float) -> bool:
        """
        Report intermediate value to Optuna and check if should prune.
        Only allows pruning after min_epochs to avoid conflict with cosine warmup.
        """
        self.trial.report(val_loss, epoch)
        
        # Don't prune during warmup period
        if epoch < self.min_epochs:
            return False
            
        if self.trial.should_prune():
            return True
        return False


class OptunaTrialRunner:
    """Manages the execution of a single Optuna trial."""
    def __init__(self, base_config_path: Path):
        self.base_config_path = base_config_path
        self.base_config = load_json_config(base_config_path)
        self.device = setup_device()
        self.logger = logging.getLogger(__name__)
        self._pipelines = {}
        
        # Preprocess data for all possible modes upfront
        self._prepare_all_modes()

    def _prepare_all_modes(self):
        """Ensure data is preprocessed for all possible prediction modes."""
        mode = self.base_config["prediction"]["mode"]
        self.logger.info(f"Preparing data for '{mode}' mode...")
        
        pipeline = ChemicalKineticsPipeline(self.base_config)
        pipeline.normalize_only()
        
        self._pipelines[mode] = OptunaPipeline(self.base_config, pipeline.processed_dir)

    def run_trial(self, trial: optuna.Trial) -> float:
        """Configures and runs a single trial."""
        config = suggest_model_config(trial, self.base_config)
        prediction_mode = config["prediction"]["mode"]
        pipeline = self._pipelines[prediction_mode]
        return pipeline.execute_trial(config, trial)


class OptunaPipeline:
    """Holds datasets and executes the training for a specific prediction mode."""
    def __init__(self, config: Dict[str, Any], processed_dir: Path):
        self.config = config
        self.device = setup_device()
        self.logger = logging.getLogger(f"OptunaPipeline_{config['prediction']['mode']}")
        
        self.processed_dir = processed_dir
        self.model_save_root = Path(self.config["paths"]["model_save_dir"])
        
        norm_stats_path = self.processed_dir / "normalization.json"
        if not norm_stats_path.exists():
            raise FileNotFoundError(f"Normalization stats not found in {norm_stats_path}")
        norm_stats = load_json(norm_stats_path)
        
        self.norm_helper = NormalizationHelper(
            stats=norm_stats, device=self.device,
            species_vars=self.config["data"]["species_variables"],
            global_vars=self.config["data"]["global_variables"],
            time_var=self.config["data"]["time_variable"],
            config=self.config
        )
        self._load_datasets()

    def _load_datasets(self):
        """Load split-specific datasets."""
        self.logger.info(f"Loading datasets from: {self.processed_dir}")
        
        self.train_dataset = NPYDataset(self.processed_dir, "train", self.config, self.device)
        self.val_dataset = NPYDataset(self.processed_dir, "validation", self.config, self.device)
        
        self.logger.info(
            f"Datasets loaded: train={len(self.train_dataset)}, "
            f"val={len(self.val_dataset)}"
        )

    def execute_trial(self, config: Dict[str, Any], trial: optuna.Trial) -> float:
        """Runs a single trial's training and evaluation with pruning."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        trial_id = f"trial_{trial.number:04d}_{config['prediction']['mode']}"
        save_dir = self.model_save_root / "optuna" / f"{timestamp}_{trial_id}"
        ensure_directories(save_dir)
        
        try:
            seed_everything(config["system"]["seed"])
            optimize_hardware(config["system"], self.device)
            model = create_model(config, self.device)
            
            # Use dynamic epoch allocation from Hyperband
            n_epochs = trial.user_attrs.get("n_epochs", config["training"]["hpo_max_epochs"])
            config["training"]["epochs"] = n_epochs
            
            # Create pruning callback with minimum epochs
            min_epochs = config["training"]["hpo_min_epochs"]
            pruning_callback = OptunaPruningCallback(trial, min_epochs)
            
            # Log trial configuration
            self.logger.info(f"Trial {trial.number}: {n_epochs} epochs allocated by Hyperband")
            self.logger.info(f"Learning rate: {config['training']['learning_rate']:.2e}")
            
            trainer = PrunableTrainer(
                model=model, train_dataset=self.train_dataset,
                val_dataset=self.val_dataset, test_dataset=None,
                config=config, save_dir=save_dir, device=self.device,
                norm_helper=self.norm_helper, epoch_callback=pruning_callback
            )
            
            best_val_loss = trainer.train()

            trial.set_user_attr("full_config", config)
            trial.set_user_attr("final_lr", trainer.optimizer.param_groups[0]['lr'])
            save_json(config, save_dir / "config.json")
            
            self.logger.info(f"Trial {trial.number} completed. Best loss: {best_val_loss:.6f}, "
                             f"Final LR: {trainer.optimizer.param_groups[0]['lr']:.2e}")
            
            return best_val_loss
            
        except optuna.TrialPruned:
            self.logger.info(f"Trial {trial.number} pruned.")
            raise
        except Exception as e:
            self.logger.error(f"Trial {trial.number} failed: {e}", exc_info=True)
            return float("inf")
        finally:
            if self.device.type == "cuda":
                torch.cuda.empty_cache()


class PrunableTrainer(Trainer):
    """Extended Trainer that supports epoch callbacks for Optuna pruning."""
    def __init__(self, *args, epoch_callback: Optional[Callable[[int, float], bool]] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_callback = epoch_callback
        
    def _run_training_loop(self):
        """Main training loop with pruning support."""
        best_train_loss = float("inf")
        
        for epoch in range(1, self.train_config["epochs"] + 1):
            self.current_epoch = epoch
            epoch_start = time.time()
            train_loss, train_metrics = self._train_epoch()
            val_loss, val_metrics = self._validate()

            if self.scheduler and not self.scheduler_step_on_batch:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) and self.has_validation:
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            epoch_time = time.time() - epoch_start
            self.total_training_time += epoch_time
            self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)

            if self.epoch_callback:
                loss_for_pruning = val_loss if self.has_validation and val_loss != float("inf") else train_loss
                
                if self.epoch_callback(epoch, loss_for_pruning):
                    self.logger.info(f"Trial pruned at epoch {epoch} with loss {loss_for_pruning:.6f}")
                    raise optuna.TrialPruned()

            if self.has_validation:
                if val_loss < (self.best_val_loss - self.min_delta):
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1

                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            else:
                if train_loss < (best_train_loss - self.min_delta):
                    best_train_loss = train_loss
                    self.best_val_loss = train_loss
                    self.best_epoch = epoch
                    self._save_best_model()


def suggest_model_config(trial: optuna.Trial, base_config: Dict[str, Any]) -> Dict[str, Any]:
    """Suggests a valid model and training configuration for a trial."""
    config = copy.deepcopy(base_config)

    #config["prediction"]["mode"] = "absolute"
    
    # Model architecture choice
    model_type = trial.suggest_categorical("model_type", ["deeponet"])
    config["model"]["type"] = model_type
    
    # Common hyperparameters - EXPANDED SEARCH SPACE
    config["model"]["activation"] = trial.suggest_categorical("activation", ["gelu", "silu", "relu", "tanh"])
    #config["training"]["learning_rate"] = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    #config["training"]["batch_size"] = trial.suggest_categorical("batch_size", [16384])
    #config["training"]["weight_decay"] = trial.suggest_float("weight_decay", 1e-6, 1e-3, log=True)
    config["model"]["dropout"] = trial.suggest_float("dropout", 0.0, 0.1, step=0.05)
    
    # Scheduler parameters
    #config["training"]["scheduler_params"]["T_0"] = trial.suggest_categorical("cosine_T_0", [10])
    
    # Gradient accumulation adjustment
    """
    if config["training"]["batch_size"] <= 1024:
        config["training"]["gradient_accumulation_steps"] = 8
    elif config["training"]["batch_size"] <= 2048:
        config["training"]["gradient_accumulation_steps"] = 4
    elif config["training"]["batch_size"] <= 4096:
        config["training"]["gradient_accumulation_steps"] = 2
    else:
        config["training"]["gradient_accumulation_steps"] = 1
    """

    if model_type == "deeponet":
        # More flexible architecture search
        n_branch = trial.suggest_int("n_branch_layers", 2, 5)
        branch_layers = []
        for i in range(n_branch):
            if i == 0:
                width = trial.suggest_categorical(f"branch_layer_{i}", [256, 384, 512])
            else:
                # Allow different widths per layer
                width = trial.suggest_categorical(f"branch_layer_{i}", [128, 256, 384])
            branch_layers.append(width)
        config["model"]["branch_layers"] = branch_layers
        
        n_trunk = trial.suggest_int("n_trunk_layers", 2, 4)
        trunk_layers = []
        for i in range(n_trunk):
            width = trial.suggest_categorical(f"trunk_layer_{i}", [64, 128, 192])
            trunk_layers.append(width)
        config["model"]["trunk_layers"] = trunk_layers
        
        config["model"]["basis_dim"] = trial.suggest_categorical("basis_dim", [64, 128, 256])
        config["model"]["output_scale"] = trial.suggest_categorical("output_scale", [0.1, 1.0, 10.0])
        
    # Siren
    else: 
        n_layers = trial.suggest_int("n_hidden_layers", 3, 6)
        hidden_dims = []
        for i in range(n_layers):
            if i == 0:
                width = trial.suggest_categorical(f"hidden_dim_{i}", [256, 384, 512])
            else:
                width = trial.suggest_categorical(f"hidden_dim_{i}", [128, 256, 384])
            hidden_dims.append(width)
        config["model"]["hidden_dims"] = hidden_dims
        config["model"]["omega_0"] = trial.suggest_float("omega_0", 10.0, 50.0)

    # FiLM configuration
    use_film = trial.suggest_categorical("use_film", [True, False])
    config["film"]["enabled"] = use_film
    if use_film:
        film_layers = trial.suggest_int("film_layers", 1, 3)
        film_widths = []
        for i in range(film_layers):
            width = trial.suggest_categorical(f"film_width_{i}", [32, 64, 128])
            film_widths.append(width)
        config["film"]["hidden_dims"] = film_widths
        config["film"]["activation"] = trial.suggest_categorical("film_activation", ["gelu", "relu"])

    # Loss function
    config["training"]["loss"] = trial.suggest_categorical("loss", ["mse"])
    if config["training"]["loss"] == "huber":
        config["training"]["huber_delta"] = trial.suggest_float("huber_delta", 0.1, 2.0)

    # Gradient clipping
    config["training"]["gradient_clip"] = trial.suggest_categorical("gradient_clip", [1.0])

    return config


def optimize(config_path: Path, n_trials: int = 25, n_jobs: int = 1,
             study_name: str = "chemulator_hpo", pruner: Optional[optuna.pruners.BasePruner] = None):
    """
    Main function to run Optuna optimization with Hyperband for 40-hoaur runtime.
    """
    logger = logging.getLogger(__name__)
    base_config = load_json_config(config_path)
    
    # Create trial runner which will prepare data
    trial_runner = OptunaTrialRunner(config_path)
    objective = trial_runner.run_trial

    # Use Hyperband pruner for efficient resource allocation
    if pruner is None:
        min_resource = base_config["training"]["hpo_min_epochs"]
        max_resource = base_config["training"]["hpo_max_epochs"]
        
        pruner = HyperbandPruner(
            min_resource=min_resource,
            max_resource=max_resource,
            reduction_factor=3
        )
        
        logger.info(f"Using Hyperband pruner: min={min_resource} epochs, max={max_resource} epochs")

    study = optuna.create_study(
        direction="minimize",
        sampler=TPESampler(seed=42, n_startup_trials=5),
        pruner=pruner,
        study_name=study_name,
        storage=f"sqlite:///{study_name}.db",
        load_if_exists=True
    )

    # Log expected runtime
    logger.info(f"Starting optimization with target {n_trials} trials")
    logger.info(f"Expected runtime: ~40 hours with aggressive pruning")

    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)

    # Save Results
    results_dir = Path("optuna_results")
    ensure_directories(results_dir)
    timestamp = time.strftime("%Y%m%d_%H%M%S")

    best_config = study.best_trial.user_attrs.get("full_config", {})
    if not best_config:
        logger.warning("Could not retrieve full config from user_attrs.")

    # Compute statistics
    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
    
    epoch_distribution = {}
    for t in completed_trials + pruned_trials:
        n_epochs = t.user_attrs.get("n_epochs", "unknown")
        epoch_distribution[n_epochs] = epoch_distribution.get(n_epochs, 0) + 1

    best_results = {
        "best_value": study.best_value,
        "best_params": study.best_trial.params,
        "best_config": best_config,
        "n_trials_completed": len(completed_trials),
        "n_trials_pruned": len(pruned_trials),
        "epoch_distribution": epoch_distribution,
        "best_trial_final_lr": study.best_trial.user_attrs.get("final_lr", "unknown"),
        "study_db": f"{study_name}.db"
    }
    
    save_json(best_results, results_dir / f"best_config_{study_name}_{timestamp}.json")
    
    print("\n" + "="*60)
    print("OPTIMIZATION COMPLETE")
    print("="*60)
    print(f"Best validation loss: {best_results['best_value']:.6f}")
    print(f"Best trial final LR: {best_results['best_trial_final_lr']}")
    print(f"Trials: {best_results['n_trials_completed']} completed, {best_results['n_trials_pruned']} pruned")
    print("\nEpoch distribution:")
    for epochs, count in sorted(epoch_distribution.items()):
        print(f"  {epochs} epochs: {count} trials")
    print("\nBest parameters:")
    for key, value in best_results['best_params'].items():
        print(f"  {key}: {value}")
    
    return study

===== /Users/imalsky/Desktop/Chemulator/src/training/trainer.py =====
#!/usr/bin/env python3
"""
Training pipeline for chemical kinetics models.
Fixed issues:
1. Correct scheduler stepping with gradient accumulation
2. Removed all MAE calculations
3. Fixed ratio mode loss calculation
4. Fixed no-validation logic
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import math

import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau

from models.model import export_model
from data.normalizer import NormalizationHelper


class Trainer:
    """Trainer for chemical kinetics networks."""
    def __init__(self, model: nn.Module, train_dataset, val_dataset, test_dataset,
                config: Dict[str, Any], save_dir: Path, device: torch.device,
                norm_helper: NormalizationHelper):
        self.logger = logging.getLogger(__name__)
        
        self.model = model
        self.config = config
        self.save_dir = save_dir
        self.device = device
        self.norm_helper = norm_helper
        
        # Extract config sections
        self.train_config = config["training"]
        self.system_config = config["system"]
        self.prediction_config = config.get("prediction", {})
        
        # Prediction mode
        self.prediction_mode = self.prediction_config.get("mode", "absolute")
        self.output_clamp = self.prediction_config.get("output_clamp")
        
        # ADD THIS LINE - Call the validation method
        self.trainer_init_validation()
        
        # Dataset info
        self.n_species = len(config["data"]["species_variables"])
        self.n_globals = len(config["data"]["global_variables"])
        
        # Check for empty validation set
        self.has_validation = val_dataset is not None and len(val_dataset) > 0
        if not self.has_validation:
            self.logger.warning("No validation data – early‑stopping and LR‑plateau will be skipped")
        
        # Create data loaders
        self._setup_dataloaders(train_dataset, val_dataset, test_dataset)

        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.test_dataset = test_dataset

        # Training state
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_loss = float("inf")
        self.best_epoch = -1
        self.total_training_time = 0
        self.patience_counter = 0
        
        # Training parameters
        self.log_interval = self.train_config.get("log_interval", 1000)
        self.early_stopping_patience = self.train_config["early_stopping_patience"]
        self.min_delta = self.train_config["min_delta"]
        self.gradient_accumulation_steps = self.train_config["gradient_accumulation_steps"]
        self.empty_cache_interval = self.train_config.get("empty_cache_interval", 1000)
        
        # Setup training components
        self._setup_optimizer()
        self._setup_scheduler()
        self._setup_loss()
        self._setup_amp()
        
        # Training history
        self.training_history = {
            "config": config,
            "prediction_mode": self.prediction_mode,
            "epochs": []
        }
        
    def trainer_init_validation(self):
        """Add this to Trainer.__init__ after self.prediction_mode is set"""
        # Bug 2 Fix: Validate ratio mode requirements at initialization
        if self.prediction_mode == "ratio":
            if not hasattr(self.norm_helper, 'ratio_stats') or self.norm_helper.ratio_stats is None:
                raise ValueError(
                    "Training in 'ratio' mode requires ratio statistics from preprocessing. "
                    "Ensure data was preprocessed with prediction.mode='ratio' in config. "
                    "Current normalization data does not contain ratio_stats."
                )
            
            # Additional check for model compatibility
            model_type = self.config["model"]["type"]
            if model_type != "deeponet":
                raise ValueError(
                    f"Ratio prediction mode is only compatible with 'deeponet' model, "
                    f"but '{model_type}' was specified. Please use 'deeponet' or switch to 'absolute' mode."
                )
    
    def _setup_dataloaders(self, train_dataset, val_dataset, test_dataset):
        """Setup data loaders."""
        from data.dataset import create_dataloader
        
        # Use shard-aware sampling for the training loader for maximum cache efficiency
        self.train_loader = create_dataloader(
            train_dataset, self.config, shuffle=True, 
            device=self.device, drop_last=True,
            use_shard_aware_sampling=True  # Be explicit
        ) if train_dataset else None
        
        # Shard-aware sampling is not needed for validation and test sets
        self.val_loader = create_dataloader(
            val_dataset, self.config, shuffle=False, 
            device=self.device, drop_last=False,
            use_shard_aware_sampling=False # Be explicit
        ) if val_dataset and len(val_dataset) > 0 else None
        
        self.test_loader = create_dataloader(
            test_dataset, self.config, shuffle=False, 
            device=self.device, drop_last=False,
            use_shard_aware_sampling=False # Be explicit
        ) if test_dataset and len(test_dataset) > 0 else None
    
    def _setup_optimizer(self):
        """Setup AdamW optimizer."""
        # Separate parameters for weight decay
        decay_params = []
        no_decay_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            if param.dim() == 1 or "bias" in name or "norm" in name.lower():
                no_decay_params.append(param)
            else:
                decay_params.append(param)
        
        param_groups = [
            {"params": decay_params, "weight_decay": self.train_config["weight_decay"]},
            {"params": no_decay_params, "weight_decay": 0.0}
        ]
        
        self.optimizer = AdamW(
            param_groups,
            lr=self.train_config["learning_rate"],
            betas=tuple(self.train_config.get("betas", [0.9, 0.999])),
            eps=self.train_config.get("eps", 1e-8)
        )
    
    def _setup_scheduler(self):
        """Create the learning‑rate scheduler as requested in the config."""
        scheduler_type = self.train_config.get("scheduler", "none").lower()

        if scheduler_type == "none" or not self.train_loader:
            self.scheduler = None
            self.scheduler_step_on_batch = False
            return

        steps_per_epoch = math.ceil(
            len(self.train_loader) / self.gradient_accumulation_steps
        )

        params: Dict[str, Any] = self.train_config.get("scheduler_params", {})

        if scheduler_type == "cosine":
            T_0_epochs: int = params.get("T_0", 1)
            if T_0_epochs <= 0:
                raise ValueError("`scheduler_params.T_0` must be > 0 for 'cosine'.")
            T_0_steps = T_0_epochs * steps_per_epoch

            self.scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=T_0_steps,
                T_mult=params.get("T_mult", 2),
                eta_min=params.get("eta_min", 1e-8),
            )
            self.scheduler_step_on_batch = True
            return

        if scheduler_type == "plateau":
            if not self.has_validation:
                self.logger.warning("Plateau scheduler requires validation data, falling back to no scheduler")
                self.scheduler = None
                self.scheduler_step_on_batch = False
                return
                
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                factor=params.get("factor", 0.5),
                patience=params.get("patience", 10),
                min_lr=params.get("min_lr", 1e-7),
            )
            self.scheduler_step_on_batch = False
            return

        raise ValueError(f"Unknown scheduler '{scheduler_type}'")
    
    def _setup_loss(self):
        """Setup loss function."""
        loss_type = self.train_config["loss"]
        
        if loss_type == "mse":
            self.criterion = nn.MSELoss()
        elif loss_type == "mae":
            self.criterion = nn.L1Loss()
        elif loss_type == "huber":
            self.criterion = nn.HuberLoss(delta=self.train_config.get("huber_delta", 0.25))
        else:
            raise ValueError(f"Unknown loss: {loss_type}")
    
    def _setup_amp(self):
        """Setup automatic mixed precision training."""
        self.use_amp = self.train_config.get("use_amp", False)
        if self.device.type not in ('cuda', 'mps', 'cpu'):
            self.use_amp = False
        self.scaler = None
        self.amp_dtype = None
        
        if not self.use_amp:
            return
        
        dtype_str = str(self.train_config.get("amp_dtype", "bfloat16")).lower()
        
        if dtype_str not in ["bfloat16", "float16"]:
            self.logger.warning(f"Invalid amp_dtype '{dtype_str}'. Falling back to 'bfloat16'.")
            dtype_str = "bfloat16"
        
        self.amp_dtype = torch.bfloat16 if dtype_str == "bfloat16" else torch.float16

        if self.device.type == 'cpu' and self.amp_dtype != torch.bfloat16:
            self.logger.warning(f"AMP with {dtype_str} is not supported on CPU. Disabling AMP.")
            self.use_amp = False
            return

        if self.device.type == 'mps' and self.amp_dtype not in [torch.float16, torch.bfloat16]:
            self.logger.warning(f"AMP with {dtype_str} is not supported on MPS. Disabling AMP.")
            self.use_amp = False
            return

        # GradScaler only for float16 on CUDA
        if self.amp_dtype == torch.float16 and self.device.type == 'cuda':
            self.scaler = GradScaler(device_type='cuda')
    
    # --- helper ------------------------------------------------------------
    def _standardize_log_ratios(self, log_ratios: torch.Tensor) -> torch.Tensor:
        if self.prediction_mode == "ratio" and self.norm_helper.ratio_stats is None:
            raise ValueError(
                "Ratio statistics are missing but prediction mode is 'ratio'. "
                "This likely means data was preprocessed in 'absolute' mode. "
                "Please reprocess data with prediction.mode='ratio' in config."
            )
        
        # If not in ratio mode, return as-is (shouldn't happen but defensive)
        if self.norm_helper.ratio_stats is None:
            return log_ratios

        stats = self.norm_helper.ratio_stats
        species = self.config["data"]["species_variables"]
        device = log_ratios.device
        dtype = log_ratios.dtype

        means = torch.tensor([stats[v]["mean"] for v in species],
                            device=device, dtype=dtype)
        stds = torch.tensor([stats[v]["std"] for v in species],
                            device=device, dtype=dtype)

        stds = torch.clamp(stds, min=self.config["normalization"]["min_std"])
        return (log_ratios - means) / stds

    def _compute_loss(self, outputs: torch.Tensor, 
                      targets: torch.Tensor,
                      inputs: torch.Tensor) -> torch.Tensor:
        if self.prediction_mode == "ratio":
            # Direct comparison: both outputs and targets are in standardized log-ratio space
            return self.criterion(outputs, targets)
        else:
            # Absolute mode (unchanged)
            if self.output_clamp is not None:
                outputs = torch.clamp(outputs, min=self.output_clamp)
            return self.criterion(outputs, targets)
    
    def train(self) -> float:
        """Execute the training loop."""
        if not self.train_loader:
            self.logger.error("Training loader not available. Cannot start training.")
            return float("inf")

        self.logger.info(f"Starting training...")
        self.logger.info(f"Train batches: {len(self.train_loader)}")
        if self.has_validation:
            self.logger.info(f"Val batches: {len(self.val_loader)}")
        
        try:
            self._run_training_loop()
            
            self.logger.info(
                f"Training completed. Best validation loss: {self.best_val_loss:.6f} "
                f"at epoch {self.best_epoch}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}", exc_info=True)
            raise
            
        finally:
            # Save final training history
            save_path = self.save_dir / "training_log.json"
            with open(save_path, 'w') as f:
                json.dump(self.training_history, f, indent=2)
            
            # Clear cache
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        return self.best_val_loss
    
    def _run_training_loop(self):
        """Main training loop with fix for no-validation saving."""
        best_train_loss = float("inf")
        
        for epoch in range(1, self.train_config["epochs"] + 1):
            self.current_epoch = epoch
            epoch_start = time.time()

            # Train
            train_loss, train_metrics = self._train_epoch()
            
            # Validate if available
            val_loss, val_metrics = self._validate()

            # Batch-based schedulers are handled correctly in _train_epoch.
            if self.scheduler and not self.scheduler_step_on_batch:
                if isinstance(self.scheduler, ReduceLROnPlateau) and self.has_validation:
                    self.scheduler.step(val_loss)
                # For any other epoch-based scheduler, add logic here.
                # The cosine scheduler is batch-based, so it's handled in _train_epoch

            # Log epoch
            epoch_time = time.time() - epoch_start
            self.total_training_time += epoch_time
            self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)

            # Save best model and early stopping logic
            if self.has_validation:
                if val_loss < (self.best_val_loss - self.min_delta):
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1

                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            else:
                # No validation set: save based on best training loss
                if train_loss < (best_train_loss - self.min_delta):
                    best_train_loss = train_loss
                    self.best_val_loss = train_loss
                    self.best_epoch = epoch
                    self._save_best_model()

            self._after_epoch()

    def _after_epoch(self) -> None:
        """
        House-keeping that should run once every epoch.
        """
        # Then, do the garbage collection
        import gc
        gc.collect()
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()

    def _train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """Run one training epoch and return the average loss."""
        self.model.train()
        total_loss, total_samples = 0.0, 0
        
        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            inputs  = inputs.to(self.device,  non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)

            # Scale loss for gradient accumulation
            scaled_loss = loss / self.gradient_accumulation_steps

            if self.scaler:
                self.scaler.scale(scaled_loss).backward()
            else:
                scaled_loss.backward()

            is_update_step = (
                (batch_idx + 1) % self.gradient_accumulation_steps == 0
                or (batch_idx + 1) == len(self.train_loader)
            )
            
            if is_update_step:
                if self.train_config["gradient_clip"] > 0:
                    if self.scaler:
                        self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.train_config["gradient_clip"]
                    )

                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                self.optimizer.zero_grad(set_to_none=True)

                if self.scheduler and self.scheduler_step_on_batch:
                    self.scheduler.step()
                self.global_step += 1
                
                # Empty cache periodically
                if self.global_step % self.empty_cache_interval == 0 and self.device.type == 'cuda':
                    torch.cuda.empty_cache()

            total_loss += loss.item() * inputs.size(0)
            total_samples += inputs.size(0)

            #if self.global_step > 0 and self.global_step % self.log_interval == 0:
            #    self.logger.info(
            #        f"Epoch {self.current_epoch} | "
            #        f"Batch {batch_idx + 1}/{len(self.train_loader)} | "
            #        f"Loss: {total_loss / total_samples:.3e}"
            #    )
        
        return total_loss / total_samples, {}

    def _validate(self) -> Tuple[float, Dict[str, float]]:
        """Evaluate the model on the validation set and return the average loss."""
        if not self.has_validation or self.val_loader is None:
            return float("inf"), {}
            
        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        
        with torch.no_grad():
            for inputs, targets in self.val_loader:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                    outputs = self.model(inputs)
                    loss = self._compute_loss(outputs, targets, inputs)

                total_loss += loss.item() * inputs.size(0)
                total_samples += inputs.size(0)
        
        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        return avg_loss, {}

    def evaluate_test(self) -> float:
        """Compute the loss on the test set; returns `inf` if no test data."""
        if not self.test_loader:
            self.logger.warning("No test data available, skipping test evaluation.")
            return float("inf")

        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        
        with torch.no_grad():
            for inputs, targets in self.test_loader:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                    outputs = self.model(inputs)
                    loss = self._compute_loss(outputs, targets, inputs)

                total_loss += loss.item() * inputs.size(0)
                total_samples += inputs.size(0)

        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        self.logger.info(f"Test loss: {avg_loss:.6f}")
        return avg_loss

    def cleanup_dataloaders(self):
        """Properly cleanup DataLoader workers to prevent memory leaks."""
        for loader in [self.train_loader, self.val_loader, self.test_loader]:
            if loader is not None:
                try:
                    # Force workers to terminate
                    loader._shutdown_workers()
                    del loader
                except:
                    pass
        
        # Force garbage collection
        import gc
        gc.collect()
        
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()

    def _log_epoch(self, train_loss, val_loss, train_metrics, val_metrics, epoch_time):
        """Log epoch results."""
        lr = self.optimizer.param_groups[0]['lr'] if self.optimizer else 0
        log_entry = {
            "epoch": self.current_epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "epoch_time": epoch_time,
            "lr": lr,
        }
        self.training_history["epochs"].append(log_entry)
        
        val_str = f"Val loss: {val_loss:.3e}" if self.has_validation else "Val loss: N/A"
        self.logger.info(
            f"Epoch {self.current_epoch}/{self.train_config['epochs']} "
            f"Train loss: {train_loss:.3e} {val_str} "
            f"Time: {epoch_time:.1f}s LR: {log_entry['lr']:.2e}"
        )
    
    def _save_best_model(self):
        """Save the best model checkpoint."""
        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict() if self.optimizer else None,
            "scheduler_state_dict": self.scheduler.state_dict() if self.scheduler else None,
            "best_val_loss": self.best_val_loss,
            "config": self.config
        }
        
        checkpoint_path = self.save_dir / "best_model.pt"
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"Saved best model checkpoint to {checkpoint_path}")

        # Export model if enabled
        if self.system_config.get("use_torch_export", False):
            # Get example input from the first available loader
            example_loader = self.val_loader or self.train_loader
            if example_loader:
                # Get a single sample batch for export
                example_inputs, _ = next(iter(example_loader))
                example_inputs = example_inputs.to(self.device)
                
                # Note: We pass the full batch, but export_model will handle
                # making the batch dimension dynamic
                export_path = self.save_dir / "exported_model.pt"
                
                # Log the shape being used for export
                self.logger.info(f"Exporting model with example input shape: {example_inputs.shape}")
                
                export_model(self.model, example_inputs, export_path)
            else:
                self.logger.warning("Cannot export model: no data loader available to create example input.")

===== /Users/imalsky/Desktop/Chemulator/src/utils/utils.py =====
#!/usr/bin/env python3
"""
Simplified utility functions for the chemical kinetics pipeline.
"""

import json
import logging
import os
import random
import sys
from pathlib import Path
from typing import Any, Dict, Union

import numpy as np
import torch


def setup_logging(level: int = logging.INFO, log_file: Path = None) -> None:
    """Configure logging for the application."""
    format_string = "%(asctime)s | %(levelname)-8s | %(name)s - %(message)s"
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    # Clear existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create formatter
    formatter = logging.Formatter(format_string, datefmt="%Y-%m-%d %H:%M:%S")
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler
    if log_file is not None:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)


def seed_everything(seed: int) -> None:
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    os.environ["PYTHONHASHSEED"] = str(seed)
    
    logger = logging.getLogger(__name__)
    logger.info(f"Random seed set to {seed}")


def load_json_config(path: Union[str, Path]) -> Dict[str, Any]:
    """Load JSON configuration file."""
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"Configuration file not found: {path}")
    
    # Try json5 first for comment support
    try:
        import json5
        with open(path, 'r', encoding='utf-8') as f:
            config = json5.load(f)
    except ImportError:
        # Fallback to standard json
        with open(path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    return config


def save_json(data: Dict[str, Any], path: Union[str, Path], indent: int = 2) -> None:
    """Save dictionary to JSON file."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Custom encoder for numpy/torch types
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.Tensor):
                return obj.cpu().numpy().tolist()
            elif isinstance(obj, Path):
                return str(obj)
            return super().default(obj)
    
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, cls=NumpyEncoder)


def load_json(path: Union[str, Path]) -> Dict[str, Any]:
    """Load JSON file."""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def ensure_directories(*paths: Union[str, Path]) -> None:
    """Create directories if they don't exist."""
    for path in paths:
        Path(path).mkdir(parents=True, exist_ok=True)

===== /Users/imalsky/Desktop/Chemulator/src/utils/hardware.py =====
#!/usr/bin/env python3
"""
Hardware detection and optimization utilities.
"""

import logging
import os
from typing import Dict, Any

import torch


def setup_device() -> torch.device:
    """Detect and configure the best available compute device."""
    logger = logging.getLogger(__name__)
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"Using CUDA device: {gpu_name} ({gpu_memory:.1f} GB)")
        
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("Using Apple Silicon MPS device")
        
    else:
        device = torch.device("cpu")
        logger.info(f"Using CPU device ({os.cpu_count()} cores)")
    
    return device


def optimize_hardware(config: Dict[str, Any], device: torch.device) -> None:
    """Apply hardware-specific optimizations."""
    logger = logging.getLogger(__name__)
    
    # CUDA optimizations
    if device.type == "cuda":
        # Enable TensorFloat-32 for faster matmul
        if config.get("tf32", True):
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("TensorFloat-32 enabled")
        
        # Enable cuDNN autotuner
        if config.get("cudnn_benchmark", True):
            torch.backends.cudnn.benchmark = True
            logger.info("cuDNN autotuner enabled")
        
        # Set memory fraction
        memory_fraction = config.get("cuda_memory_fraction", 0.9)
        if memory_fraction < 1.0:
            torch.cuda.set_per_process_memory_fraction(memory_fraction)
            logger.info(f"CUDA memory fraction set to {memory_fraction}")
    
    # Set number of threads for CPU operations
    torch.set_num_threads(min(32, os.cpu_count() or 1))
    logger.info(f"Using {torch.get_num_threads()} CPU threads")

===== /Users/imalsky/Desktop/Chemulator/src/models/model.py =====
#!/usr/bin/env python3
"""
Model definitions for chemical kinetics neural networks with ratio mode support.
Includes dropout regularization for both SIREN and DeepONet architectures.
"""

import logging
import math
from pathlib import Path
from typing import Dict, Any, List, Optional

import torch
import torch.nn as nn
from torch.export import Dim


class FiLMLayer(nn.Module):
    """Feature-wise Linear Modulation layer."""
    
    def __init__(self, condition_dim: int, feature_dim: int, 
                 hidden_dims: List[int], activation: str = "gelu", use_beta: bool = True):
        super().__init__()
        
        self.use_beta = use_beta
        out_multiplier = 2 if use_beta else 1
        
        # Build FiLM MLP
        layers = []
        prev_dim = condition_dim
        
        # Hidden layers
        for dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, dim),
                self._get_activation(activation)
            ])
            prev_dim = dim
        
        # Output layer (2x feature_dim for gamma and beta)
        layers.append(nn.Linear(prev_dim, out_multiplier * feature_dim))
        
        self.film_net = nn.Sequential(*layers)
        self.feature_dim = feature_dim
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(),
            "tanh": nn.Tanh(),
            "silu": nn.SiLU()
        }
        return activations.get(name.lower(), nn.GELU())
    
    def forward(self, features: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """Apply FiLM modulation."""
        # Generate gamma and beta
        params = self.film_net(condition)
        if self.use_beta:
            gamma, beta = params.chunk(2, dim=-1)
        else:
            gamma = params
            beta = torch.zeros_like(gamma)
        
        # Reshape for broadcasting
        shape = [gamma.size(0)] + [1] * (features.dim() - 2) + [self.feature_dim]
        gamma = gamma.view(*shape)
        beta = beta.view(*shape)
        
        # Apply modulation: gamma * features + beta
        return gamma * features + beta


class FiLMSIREN(nn.Module):
    """SIREN with FiLM conditioning and dropout for chemical kinetics."""
    def __init__(self, config: Dict[str, Any]):
        super().__init__()

        # Extract dimensions
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]

        # SIREN parameters
        self.omega_0 = config["model"].get("omega_0", 30.0)
        
        # Dropout
        dropout_rate = config["model"].get("dropout", 0.0)
        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None

        # FiLM configuration
        film_config = config.get("film", {})
        self.use_film = film_config.get("enabled", True)

        # Input dimension
        input_dim = self.num_species + self.num_globals + 1

        # Build network layers
        self.layers = nn.ModuleList()
        self.film_layers = nn.ModuleList() if self.use_film else None

        prev_dim = input_dim
        condition_dim = self.num_species + self.num_globals

        for i, dim in enumerate(self.hidden_dims):
            # Main layer
            self.layers.append(nn.Linear(prev_dim, dim))

            # FiLM layer with SIREN-compatible initialization
            if self.use_film:
                film_layer = FiLMLayer(
                    condition_dim=condition_dim,
                    feature_dim=dim,
                    hidden_dims=film_config.get("hidden_dims", [128, 128]),
                    activation=film_config.get("activation", "gelu")
                )

                # Initialize FiLM to identity mapping
                with torch.no_grad():
                    final_layer = film_layer.film_net[-1]
                    if film_layer.use_beta:
                        final_layer.weight.data.zero_()
                    # Set bias for gamma part to 1
                    final_layer.bias.data[:dim] = 1.0
                    # Set bias for beta part to 0
                    if film_layer.use_beta:
                        final_layer.bias.data[dim:] = 0.0

                self.film_layers.append(film_layer)

            prev_dim = dim

        # Output layer
        self.output_layer = nn.Linear(prev_dim, self.num_species)

        # Initialize SIREN weights
        self._initialize_siren_weights()
    
    def _initialize_siren_weights(self):
        """Initialize weights following SIREN paper."""
        with torch.no_grad():
            # First layer
            if len(self.layers) > 0:
                fan_in = self.layers[0].in_features
                nn.init.uniform_(self.layers[0].weight, -1.0 / fan_in, 1.0 / fan_in)
            
            # Hidden layers
            for layer in self.layers[1:]:
                fan_in = layer.in_features
                bound = math.sqrt(6.0 / fan_in) / self.omega_0
                nn.init.uniform_(layer.weight, -bound, bound)
            
            # Output layer
            fan_in = self.output_layer.in_features
            bound = math.sqrt(6.0 / fan_in) / self.omega_0
            nn.init.uniform_(self.output_layer.weight, -bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass with FiLM conditioning and dropout."""
        # Extract components
        initial_conditions = x[:, :-1]  # All but time
        
        # Process through layers
        h = x
        for i, layer in enumerate(self.layers):
            # Linear transformation
            h = layer(h)
            
            # Apply FiLM before activation
            if self.use_film and self.film_layers is not None:
                h = self.film_layers[i](h, initial_conditions)
            
            # SIREN activation (sine)
            h = torch.sin(self.omega_0 * h)
            
            # Apply dropout after activation (except on last hidden layer)
            if self.dropout is not None and i < len(self.layers) - 1:
                h = self.dropout(h)
        
        # Output (no dropout on output layer)
        output = self.output_layer(h)
        
        return output


class FiLMDeepONet(nn.Module):
    """Deep Operator Network with FiLM conditioning and dropout."""
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        # Extract dimensions
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])

        # Architecture parameters
        branch_layers = config["model"]["branch_layers"]
        trunk_layers = config["model"]["trunk_layers"]
        self.basis_dim = config["model"]["basis_dim"]
        self.activation = self._get_activation(config["model"].get("activation", "gelu"))
        self.output_scale = config["model"].get("output_scale", 1.0)
        
        # Dropout
        dropout_rate = config["model"].get("dropout", 0.0)
        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None
        
        # FiLM configuration
        film_config = config.get("film", {})
        self.use_film = film_config.get("enabled", True)
        
        # For ratio mode, we can use standard bias
        bias = True
        
        # Build branch network (processes initial conditions)
        self.branch_net = self._build_mlp_with_film(
            input_dim=self.num_species + self.num_globals,
            hidden_layers=branch_layers,
            output_dim=self.basis_dim * self.num_species,
            condition_dim=self.num_species + self.num_globals if self.use_film else None,
            film_config=film_config if self.use_film else None
        )
        
        # Build trunk network (processes time)
        self.trunk_net = self._build_mlp_with_film(
            input_dim=1,
            hidden_layers=trunk_layers,
            output_dim=self.basis_dim,
            condition_dim=self.num_species + self.num_globals if self.use_film else None,
            film_config=film_config if self.use_film else None,
            bias=bias
        )
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(),
            "tanh": nn.Tanh(),
            "silu": nn.SiLU()
        }
        return activations.get(name.lower(), nn.GELU())
    
    def _build_mlp_with_film(self, input_dim: int, hidden_layers: List[int],
                            output_dim: int, condition_dim: Optional[int] = None,
                            film_config: Optional[Dict] = None, bias: bool = True) -> nn.Module:
        """
        Build an MLP with optional FiLM layers and dropout.

        Args:
            input_dim: Input dimension for the MLP.
            hidden_layers: List of hidden layer dimensions.
            output_dim: Output dimension for the MLP.
            condition_dim: Dimension of the conditioning vector for FiLM.
            film_config: Configuration dictionary for FiLM layers.
            bias: If True, adds a learnable bias to the linear layers.
        """

        if self.use_film and condition_dim is not None and film_config is not None:
            # Build with FiLM
            layers = nn.ModuleList()
            film_layers = nn.ModuleList()

            prev_dim = input_dim
            for dim in hidden_layers:
                layers.append(nn.Linear(prev_dim, dim, bias=bias))

                film_layers.append(
                    FiLMLayer(
                        condition_dim=condition_dim,
                        feature_dim=dim,
                        hidden_dims=film_config.get("hidden_dims", [128, 128]),
                        activation=film_config.get("activation", "gelu"),
                        use_beta=True
                    )
                )
                prev_dim = dim

            output_layer = nn.Linear(prev_dim, output_dim, bias=bias)

            class MLPWithFiLMAndDropout(nn.Module):
                def __init__(self, layers, film_layers, output_layer, activation, dropout):
                    super().__init__()
                    self.layers = layers
                    self.film_layers = film_layers
                    self.output_layer = output_layer
                    self.activation = activation
                    self.dropout = dropout

                def forward(self, x, condition):
                    h = x
                    for i, (layer, film_layer) in enumerate(zip(self.layers, self.film_layers)):
                        h = layer(h)
                        h = film_layer(h, condition)
                        h = self.activation(h)
                        
                        # Apply dropout after activation (except on last hidden layer)
                        if self.dropout is not None and i < len(self.layers) - 1:
                            h = self.dropout(h)
                            
                    return self.output_layer(h)

            return MLPWithFiLMAndDropout(layers, film_layers, output_layer, self.activation, self.dropout)

        else:
            # Build standard MLP with dropout
            layers = []
            prev_dim = input_dim

            for i, dim in enumerate(hidden_layers):
                layers.append(nn.Linear(prev_dim, dim, bias=bias))
                layers.append(self.activation)
                
                # Add dropout after activation (except on last hidden layer)
                if self.dropout is not None and i < len(hidden_layers) - 1:
                    layers.append(self.dropout)
                    
                prev_dim = dim

            layers.append(nn.Linear(prev_dim, output_dim, bias=bias))
            return nn.Sequential(*layers)
            
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """Forward pass with FiLM conditioning."""
        batch_size = inputs.shape[0]

        # Split inputs
        branch_input = inputs[:, :self.num_species + self.num_globals]
        trunk_input = inputs[:, -1:]  # Time

        # Use branch input as condition for FiLM
        condition = branch_input if self.use_film else None

        # Process through networks
        if self.use_film:
            branch_out = self.branch_net(branch_input, condition)
            trunk_out = self.trunk_net(trunk_input, condition)
        else:
            branch_out = self.branch_net(branch_input)
            trunk_out = self.trunk_net(trunk_input)

        # Reshape branch output
        branch_out = branch_out.view(batch_size, self.num_species, self.basis_dim)

        # Combine with dot product
        output = torch.einsum("bni,bi->bn", branch_out, trunk_out)

        # Optional output scaling
        if self.output_scale != 1.0:
            output = output * self.output_scale

        return output


def create_model(config: Dict[str, Any], device: torch.device) -> nn.Module:
    """Create model based on configuration with mode-model compatibility check."""
    model_type = config["model"]["type"].lower()
    prediction_mode = config.get("prediction", {}).get("mode", "absolute")
    
    # Enforce mode-model compatibility
    if prediction_mode == "ratio" and model_type != "deeponet":
        raise ValueError(
            f"Prediction mode 'ratio' is only compatible with model type 'deeponet', "
            f"but '{model_type}' was specified. Either change model.type to 'deeponet' "
            f"or change prediction.mode to 'absolute'."
        )
    
    if model_type == "siren":
        model = FiLMSIREN(config)
    elif model_type == "deeponet":
        model = FiLMDeepONet(config)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    model = model.to(device)
    
    # Log model configuration
    logger = logging.getLogger(__name__)
    logger.info(f"Created {model_type} model with dropout={config['model'].get('dropout', 0.0)}")
    
    # Compile model if enabled and supported
    if config["system"].get("use_torch_compile", False) and hasattr(torch, 'compile'):
        compile_mode = config["system"].get("compile_mode", "default")
        logger.info(f"Compiling model with mode='{compile_mode}'...")
        
        try:
            model = torch.compile(model, mode=compile_mode)
            logger.info("Model compilation successful")
        except Exception as e:
            logger.warning(f"Model compilation failed: {e}")
    
    return model


def export_model(model: nn.Module, example_input: torch.Tensor, save_path: Path):
    """Export model using torch.export with dynamic batch size support."""
    logger = logging.getLogger(__name__)
    
    model.eval()
    
    # Handle compiled models
    if hasattr(model, '_orig_mod'):
        logger.info("Extracting original model from compiled wrapper")
        model = model._orig_mod
    
    with torch.no_grad():
        try:
            # Use the new torch.export API if available
            if hasattr(torch, 'export'):      
                
                # A large but finite number is sufficient.
                batch_dim = Dim("batch", min=1, max=16384)
                
                # The key MUST match the argument name in the model's forward() method.
                # For FiLMDeepONet, the signature is `forward(self, inputs: ...)`.
                dynamic_shapes = {"inputs": {0: batch_dim}}
                
                # Export the model with dynamic batch dimension
                exported_program = torch.export.export(
                    model, 
                    (example_input,),
                    dynamic_shapes=dynamic_shapes
                )
                torch.export.save(exported_program, str(save_path))
                logger.info(f"Model exported with dynamic batch size to {save_path}")
            else:
                traced_model = torch.jit.trace(model, example_input)
                torch.jit.save(traced_model, str(save_path))
                logger.info(f"Model exported using torch.jit to {save_path}")
                logger.warning("JIT export may not support dynamic batch sizes as well as torch.export")
        except Exception as e:
            logger.error(f"Export failed: {e}")
            raise

===== /Users/imalsky/Desktop/Chemulator/src/data/preprocessor.py =====
#!/usr/bin/env python3
"""
Chemical kinetics data preprocessor.
This version uses a highly efficient, parallelized, two-pass process with an
architecture that minimizes inter-process communication (IPC) and memory overhead.
"""

import hashlib
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

import h5py
import numpy as np
import torch
import os

from .normalizer import DataNormalizer, NormalizationHelper
from utils.utils import save_json, load_json

DEFAULT_EPSILON_MIN = 1e-38
DEFAULT_EPSILON_MAX = 1e38

class CorePreprocessor:
    """A lightweight helper class containing only the logic needed within a worker."""
    def __init__(self, config: Dict[str, Any], norm_stats: Optional[Dict[str, Any]] = None):
        self.data_cfg = config["data"]
        self.norm_cfg = config["normalization"]
        self.train_cfg = config["training"]
        self.pred_cfg = config.get("prediction", {})
        self.proc_cfg = config["preprocessing"]

        self.species_vars = self.data_cfg["species_variables"]
        self.global_vars = self.data_cfg["global_variables"]
        self.time_var = self.data_cfg["time_variable"]
        self.var_order = self.species_vars + self.global_vars + [self.time_var]
        
        self.n_species = len(self.species_vars)
        self.n_globals = len(self.global_vars)
        self.n_vars = self.n_species + self.n_globals + 1
        
        self.min_value_threshold = self.proc_cfg.get("min_value_threshold", 1e-30)
        
        self.prediction_mode = self.pred_cfg.get("mode", "absolute")
        self.normalizer = DataNormalizer(config)
        self.norm_stats = norm_stats or {}
        
        # Create index mappings for robust variable ordering
        self._create_index_mappings()
        
        if norm_stats:
            self.norm_helper = NormalizationHelper(
                norm_stats, torch.device("cpu"), self.species_vars,
                self.global_vars, self.time_var, config
            )
    
    def _create_index_mappings(self):
        """Create index mappings for robust variable access"""
        self.var_to_idx = {var: i for i, var in enumerate(self.var_order)}
        self.species_indices = [self.var_to_idx[var] for var in self.species_vars]
        self.global_indices = [self.var_to_idx[var] for var in self.global_vars]
        self.time_idx = self.var_to_idx[self.time_var]

    def _is_profile_valid(self, group: h5py.Group) -> Tuple[bool, str]:
        """
        Checks if a profile is valid according to strict criteria.
        Returns (is_valid, reason_for_failure_or_success).
        """
        # Validate every variable (species + globals + time)
        required_keys = self.species_vars + [self.time_var]
        if not set(required_keys).issubset(group.keys()):
            return False, "missing_keys"

        # Check each dataset for NaNs, Infs, and value thresholds
        for var in required_keys:
            try:
                data = group[var][:]
            except Exception:
                return False, "read_error"

            if not np.all(np.isfinite(data)):
                return False, "non_finite"

            # Drop profile if any value is less than or equal to threshold
            if np.any(data <= self.min_value_threshold):
                return False, "below_threshold"

        return True, "valid"
    
    def process_file_for_stats(
        self, file_path: Path
    ) -> Tuple[Dict[str, Dict], Dict[str, Dict], int, Dict]:
        accumulators = self.normalizer._initialize_accumulators()

        ratio_accumulators: Dict[str, Dict[str, Any]] = {}
        if self.prediction_mode == "ratio":
            for v in self.species_vars:
                raw_method = self.normalizer._get_method(v)
                ratio_method = raw_method[4:] if raw_method.startswith("log-") else raw_method
                ratio_accumulators[v] = {
                    "method": ratio_method,
                    "count": 0,
                    "mean": 0.0,
                    "m2":   0.0,
                    "min":  float("inf"),
                    "max":  float("-inf"),
                }

        valid_sample_count = 0
        report = {
            "total_profiles":   0,
            "profiles_kept":    0,
            "dropped_reasons":  defaultdict(int),
        }

        with h5py.File(file_path, "r") as f:
            for gname in sorted(f.keys()):
                report["total_profiles"] += 1
                grp = f[gname]

                # dataset‑level validation
                is_ok, reason = self._is_profile_valid(grp)
                if not is_ok:
                    report["dropped_reasons"][reason] += 1
                    continue

                # deterministic down‑sampling
                if self.train_cfg["use_fraction"] < 1.0:
                    h = int(hashlib.sha256(gname.encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
                    if h >= self.train_cfg["use_fraction"]:
                        continue

                n_t = grp[self.time_var].shape[0]
                if n_t <= 1:
                    report["dropped_reasons"]["too_few_timesteps"] += 1
                    continue

                # assemble full profile (species + globals + time)
                profile = self._extract_profile(grp, gname, n_t)
                if profile is None:
                    report["dropped_reasons"]["extract_profile_failed"] += 1
                    continue

                # final profile‑level check
                if (np.any(~np.isfinite(profile)) or
                    np.any(profile <= self.min_value_threshold)):
                    report["dropped_reasons"]["below_threshold"] += 1
                    continue

                # update statistics
                report["profiles_kept"] += 1
                valid_sample_count += (n_t - 1)
                self._update_stats_for_profile(
                    profile, n_t,
                    accumulators,
                    ratio_accumulators,
                )

        return accumulators, ratio_accumulators, valid_sample_count, report

    def process_file_for_shards(self, file_path: Path, output_dir: Path) -> Dict[str, Any]:
        """Process file and write to split-specific shard directories."""
        # Create separate shard writers for each split
        shard_writers = {
            "train": ShardWriter(
                output_dir / "train", 
                self.proc_cfg["shard_size"], 
                file_path.stem
            ),
            "validation": ShardWriter(
                output_dir / "validation",
                self.proc_cfg["shard_size"],
                file_path.stem
            ),
            "test": ShardWriter(
                output_dir / "test",
                self.proc_cfg["shard_size"], 
                file_path.stem
            )
        }
        
        # Track samples per split
        split_counts = {"train": 0, "validation": 0, "test": 0}
        
        # Process file
        with h5py.File(file_path, "r") as f:
            for gname in sorted(f.keys()):
                # Validation checks
                is_valid, _ = self._is_profile_valid(f[gname])
                if not is_valid:
                    continue
                
                # Use fraction check
                use_fraction = self.train_cfg["use_fraction"]
                if use_fraction < 1.0:
                    hash_val = int(hashlib.sha256(gname.encode('utf-8')).hexdigest()[:8], 16) / 0xFFFFFFFF
                    if hash_val >= use_fraction:
                        continue
                
                # Process profile
                grp = f[gname]
                n_t = grp[self.time_var].shape[0]
                if n_t <= 1:
                    continue
                    
                profile = self._extract_profile(grp, gname, n_t)
                if profile is None:
                    continue

                if (np.any(~np.isfinite(profile)) or
                    np.any(profile <= self.min_value_threshold)):
                    continue

                # Determine split
                p = int(hashlib.sha256((gname + "_split").encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
                test_frac = self.train_cfg["test_fraction"]
                val_frac = self.train_cfg["val_fraction"]
                
                if p < test_frac:
                    split_key = "test"
                elif p < test_frac + val_frac:
                    split_key = "validation"
                else:
                    split_key = "train"
                
                # Convert profile to samples
                if self.prediction_mode == "ratio":
                    samples = self._profile_to_samples_ratio(
                        profile, n_t, self.norm_stats.get("ratio_stats", {})
                    )
                else:
                    norm_prof = self.norm_helper.normalize_profile(
                        torch.from_numpy(profile)
                    ).numpy()
                    samples = self._profile_to_samples(norm_prof, n_t)
                
                if samples is not None:
                    # Add to appropriate writer - no need to track indices
                    shard_writers[split_key].add_samples(samples)
                    split_counts[split_key] += samples.shape[0]
        
        # Flush all writers
        for writer in shard_writers.values():
            writer.flush()
        
        # Return metadata
        return {
            "splits": {
                "train": {
                    "shards": shard_writers["train"].get_shard_metadata(),
                    "samples_written": split_counts["train"]
                },
                "validation": {
                    "shards": shard_writers["validation"].get_shard_metadata(),
                    "samples_written": split_counts["validation"]
                },
                "test": {
                    "shards": shard_writers["test"].get_shard_metadata(),
                    "samples_written": split_counts["test"]
                }
            }
        }

    def _update_stats_for_profile(self, profile, n_t, accumulators, ratio_accumulators):
        """Consistent normalization in both modes."""
        import logging
        logger = logging.getLogger(__name__)
        
        if self.prediction_mode == "ratio":
            # Use full profiles for all variables in ratio mode
            for var, acc in accumulators.items():
                idx = acc["index"]
                method = acc["method"]
                
                # Use full profile data for all variables (not just initial timestep)
                if var == self.time_var and n_t > 1:
                    vec = profile[1:, idx]
                else:
                    vec = profile[:, idx]
                
                if vec.size > 0:
                    self.normalizer._update_single_accumulator(acc, vec, var)
            
            # Compute ratio statistics correctly with proper indices
            initial = profile[0, self.species_indices]
            future = profile[1:, self.species_indices]
            
            ratios = future / np.maximum(initial[None, :], self.normalizer.epsilon)
            ratios = np.clip(ratios, -DEFAULT_EPSILON_MAX, DEFAULT_EPSILON_MAX)
            log_ratios = np.sign(ratios) * np.log10( np.clip(np.abs(ratios), DEFAULT_EPSILON_MIN, DEFAULT_EPSILON_MAX))


            for i, var_name in enumerate(self.species_vars):
                self.normalizer._update_single_accumulator(ratio_accumulators[var_name], log_ratios[:, i], var_name)
        else:
            # Absolute mode
            for var, acc in accumulators.items():
                idx = acc["index"]
                vec = profile[1:, idx] if (var == self.time_var and n_t > 1) else profile[:, idx]
                if vec.size > 0:
                    self.normalizer._update_single_accumulator(acc, vec, var)

    def _process_single_group(self, grp, gname, ratio_stats) -> Optional[Tuple[np.ndarray, str]]:
        # The stricter validation is now done before this function is called.
        n_t = grp[self.time_var].shape[0]
        if n_t <= 1: return None
        profile = self._extract_profile(grp, gname, n_t)
        if profile is None: return None
        
        if self.prediction_mode == "ratio": samples = self._profile_to_samples_ratio(profile, n_t, ratio_stats)
        else: samples = self._profile_to_samples(self.norm_helper.normalize_profile(torch.from_numpy(profile)).numpy(), n_t)
        if samples is None: return None
        
        p = int(hashlib.sha256((gname + "_split").encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
        split_key = "test" if p < self.train_cfg["test_fraction"] else "validation" if p < self.train_cfg["test_fraction"] + self.train_cfg["val_fraction"] else "train"
        return samples, split_key

    
    def _extract_profile(self, group: h5py.Group, gname: str, n_t: int) -> Optional[np.ndarray]:
        import re
        globals_dict = {f"{lbl}_init": float(val) for lbl, val in re.findall(r"_([A-Z])_([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", gname) if f"{lbl}_init" in self.global_vars}
        if len(globals_dict) != len(self.global_vars): return None
        profile = np.empty((n_t, self.n_vars), dtype=np.float32)
        try:
            for i, var in enumerate(self.var_order):
                profile[:, i] = group[var][:] if var in group else globals_dict[var]
        except Exception: return None
        return profile

    def _profile_to_samples(self, norm_prof, n_t):
        """Get samples"""
        if n_t <= 1:
            return None
        
        n_inputs = self.n_species + self.n_globals + 1
        samples = np.empty((n_t - 1, n_inputs + self.n_species), dtype=np.float32)
        
        # Initial species
        samples[:, :self.n_species] = norm_prof[0, self.species_indices] 

        # Globals
        samples[:, self.n_species:self.n_species + self.n_globals] = norm_prof[0, self.global_indices]

        # Time
        samples[:, n_inputs - 1] = norm_prof[1:, self.time_idx]

        # Target species
        samples[:, n_inputs:] = norm_prof[1:, self.species_indices]
        
        return samples

    def _profile_to_samples_ratio(
        self, raw_prof: np.ndarray, n_t: int, ratio_stats: Dict[str, Dict]
    ) -> Optional[np.ndarray]:
        """
        Build (n_t‑1) samples for ratio‑prediction mode.
        raw_prof is **unnormalised** profile array  shape = (n_t, n_vars)
        ratio_stats contains mean/std/min/max for log‑ratios (already computed)
        """
        import logging
        logger = logging.getLogger(__name__)

        if n_t <= 1:
            return None
        
        n_inputs  = self.n_species + self.n_globals + 1
        samples   = np.empty((n_t - 1, n_inputs + self.n_species), dtype=np.float32)

        norm_prof = self.norm_helper.normalize_profile(torch.from_numpy(raw_prof)).numpy()
        samples[:, :self.n_species]                   = norm_prof[0, self.species_indices]
        samples[:, self.n_species:self.n_species+self.n_globals] = norm_prof[0, self.global_indices]
        samples[:, n_inputs - 1]                      = norm_prof[1:, self.time_idx]

        initial = raw_prof[0, self.species_indices]
        future  = raw_prof[1:, self.species_indices]

        ratios = future / np.maximum(initial[None, :], self.norm_cfg["epsilon"])
        ratios = np.clip(ratios, -DEFAULT_EPSILON_MAX, DEFAULT_EPSILON_MAX)
        log_ratios = np.sign(ratios) * np.log10(np.clip(np.abs(ratios), DEFAULT_EPSILON_MIN, DEFAULT_EPSILON_MAX))

        methods_cfg = self.norm_cfg.get("methods", {})
        default_m   = self.norm_cfg.get("default_method", "standard")
        clamp_val   = self.norm_cfg.get("clamp_value", 50.0)
        min_std     = self.norm_cfg.get("min_std", 1e-10)

        normd = np.empty_like(log_ratios, dtype=np.float32)

        for i, var in enumerate(self.species_vars):
            method = methods_cfg.get(var, default_m)
            if method.startswith("log-"):
                method = method[4:]

            stats = ratio_stats[var]

            if method == "min-max":
                rng = max(stats["max"] - stats["min"], self.norm_cfg["epsilon"])
                normd[:, i] = (log_ratios[:, i] - stats["min"]) / rng
            else:
                std = max(stats["std"], min_std)
                normd[:, i] = (log_ratios[:, i] - stats["mean"]) / std

        if np.any(np.abs(normd) > clamp_val):
            n_clamped = np.sum(np.abs(normd) > clamp_val)
            logger.warning(f"Clamping {n_clamped} normalised log‑ratio values to ±{clamp_val}")

        samples[:, n_inputs:] = np.clip(normd, -clamp_val, clamp_val)
        return samples

def stats_worker(file_path, config):
    return CorePreprocessor(config).process_file_for_stats(Path(file_path))

def shard_worker(file_path, config, norm_stats, output_dir):
    """Legacy helper kept for completeness; matches shard_worker_split_aware signature."""
    processor = CorePreprocessor(config, norm_stats)
    return processor.process_file_for_shards(Path(file_path), Path(output_dir))

class DataPreprocessor:
    """Main parent class to orchestrate parallel data preprocessing."""
    def __init__(self, raw_files: List[Path], output_dir: Path, config: Dict[str, Any]):
        self.raw_files = sorted(raw_files)
        self.output_dir = output_dir

        self.processed_dir = self.output_dir / "processed"
        self.processed_dir.mkdir(parents=True, exist_ok=True)

        self.config = config
        self.logger = logging.getLogger(__name__)
        self.normalizer = DataNormalizer(config)
        self.num_workers = config["preprocessing"].get("num_workers", 1)
        self.parallel = self.num_workers > 1 and len(self.raw_files) > 1

    def process_to_npy_shards(self) -> None:
        """Main entry point - creates split-specific shard directories."""
        start_time = time.time()
        self.logger.info(f"Starting core data preprocessing with {len(self.raw_files)} files...")
        
        # Collect statistics
        norm_stats, file_sample_counts, summary_report = self._collect_stats_and_counts()
        save_json(norm_stats, self.output_dir / "normalization.json")

        # Write split-specific shards
        split_metadata = self._write_normalized_shards(norm_stats, file_sample_counts)
        
        # Save split-aware shard index
        shard_index = {
            "n_species": len(self.config["data"]["species_variables"]),
            "n_globals": len(self.config["data"]["global_variables"]),
            "samples_per_shard": self.config["preprocessing"]["shard_size"],
            "compression": self.config["preprocessing"].get("compression"),
            "prediction_mode": self.config.get("prediction", {}).get("mode", "absolute"),
            "splits": split_metadata,
            "total_samples": sum(meta["total_samples"] for meta in split_metadata.values())
        }
        save_json(shard_index, self.output_dir / "shard_index.json")

        self._write_summary_log(summary_report, shard_index["total_samples"])
        self.logger.info(f"Core data preprocessing completed in {time.time() - start_time:.1f}s")
        
    def generate_split_indices(self) -> None:
        """Generates train/val/test split indices from an existing shard_index.json. This is a very fast operation."""
        self.logger.info("Generating new train/val/test split indices...")
        shard_index_path = self.output_dir / "shard_index.json"
        if not shard_index_path.exists():
            raise FileNotFoundError(f"Cannot generate splits: shard_index.json not found in {self.output_dir}")
        
        shard_index = load_json(shard_index_path)
        total_samples = shard_index["total_samples"]
        indices = np.arange(total_samples)
        
        seed = self.config.get("system", {}).get("seed", 42)
        np.random.seed(seed)
        np.random.shuffle(indices)
        
        use_fraction = self.config["training"].get("use_fraction", 1.0)
        if use_fraction < 1.0:
            indices = indices[:int(total_samples * use_fraction)]
        
        n = len(indices)
        test_frac = self.config["training"]["test_fraction"]
        val_frac = self.config["training"]["val_fraction"]
        
        test_split_idx = int(n * test_frac)
        val_split_idx = test_split_idx + int(n * val_frac)
        
        split_data = {
            "test": np.sort(indices[:test_split_idx]).astype(np.int64),
            "validation": np.sort(indices[test_split_idx:val_split_idx]).astype(np.int64),
            "train": np.sort(indices[val_split_idx:]).astype(np.int64)
        }
        
        for name, idx_array in split_data.items():
            path = self.output_dir / shard_index["split_files"][name]
            np.save(path, idx_array)
            self.logger.info(f"Saved {name} indices to {path} ({len(idx_array)} samples)")

    def _write_normalized_shards(self, norm_stats, file_sample_counts) -> Dict[str, List[Dict]]:
        """Second pass: write split-specific shards to separate directories."""
        self.logger.info("Writing split-specific shards...")
        
        # Create split directories
        split_dirs = {
            "train": self.processed_dir / "train",
            "validation": self.processed_dir / "validation", 
            "test": self.processed_dir / "test"
        }
        for split_name, dir_path in split_dirs.items():
            dir_path.mkdir(exist_ok=True)
            self.logger.info(f"Created {split_name} directory: {dir_path}")
        
        # Initialize split metadata
        split_metadata = {
            "train": {"shards": [], "total_samples": 0},
            "validation": {"shards": [], "total_samples": 0},
            "test": {"shards": [], "total_samples": 0}
        }
        
        # Process each file
        with ProcessPoolExecutor(max_workers=self.num_workers if self.parallel else 1) as exe:
            futures = []
            
            for file_path in self.raw_files:
                future = exe.submit(
                    shard_worker_split_aware,
                    file_path,
                    self.config,
                    norm_stats,
                    self.processed_dir
                )
                futures.append((future, file_path))
            
            # Collect results
            for future, file_path in futures:
                result = future.result()
                
                # Aggregate metadata by split
                for split_name in ["train", "validation", "test"]:
                    split_meta = result["splits"][split_name]
                    split_metadata[split_name]["shards"].extend(split_meta["shards"])
                    split_metadata[split_name]["total_samples"] += split_meta["samples_written"]
        
        # Sort shards within each split and assign global indices
        for split_name, meta in split_metadata.items():
            # Sort shards by filename
            meta["shards"].sort(key=lambda x: x["filename"])
            
            # Assign sequential start/end indices
            current_idx = 0
            for shard in meta["shards"]:
                shard["start_idx"] = current_idx
                shard["end_idx"] = current_idx + shard["n_samples"]
                current_idx = shard["end_idx"]
            
            self.logger.info(f"{split_name}: {len(meta['shards'])} shards, {meta['total_samples']:,} samples")
        
        return split_metadata
     
    def _collect_stats_and_counts(self) -> Tuple[Dict, Dict, Dict]:
        self.logger.info("Pass 1: Collecting statistics and sample counts...")
        prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        final_accs = self.normalizer._initialize_accumulators()
        final_ratio_accs = {var: {"count": 0,"mean": 0.0,"m2": 0.0,"min": float('inf'),"max": float('-inf')} for var in self.config["data"]["species_variables"]} if prediction_mode == "ratio" else {}
        file_counts = {}
        
        total_report = {
            "total_profiles": 0,
            "profiles_kept": 0,
            "dropped_reasons": defaultdict(int)
        }

        with ProcessPoolExecutor(max_workers=self.num_workers if self.parallel else 1) as executor:
            futures = {executor.submit(stats_worker, fp, self.config): fp for fp in self.raw_files}
            for fut in as_completed(futures):
                accs, ratio_accs, count, worker_report = fut.result()
                
                # Aggregate results
                self.normalizer._merge_accumulators(final_accs, accs)
                if ratio_accs: self.normalizer._merge_accumulators(final_ratio_accs, ratio_accs)
                file_counts[futures[fut].name] = count
                
                # --- NEW: Aggregate the reports ---
                total_report["total_profiles"] += worker_report["total_profiles"]
                total_report["profiles_kept"] += worker_report["profiles_kept"]
                for reason, num in worker_report["dropped_reasons"].items():
                    total_report["dropped_reasons"][reason] += num

        norm_stats = self.normalizer._finalize_statistics(final_accs)
        if final_ratio_accs:
            norm_stats["ratio_stats"] = self.normalizer._finalize_statistics(final_ratio_accs, is_ratio=True)

        file_counts = {Path(k).stem: v for k, v in file_counts.items()}

        return norm_stats, file_counts, total_report

    def _write_summary_log(self, report: Dict, total_samples: int):
        """Writes a human-readable summary of the preprocessing results."""
        log_dir = Path(self.config["paths"]["log_dir"])
        log_dir.mkdir(exist_ok=True)
        summary_path = log_dir / f"preprocessing_summary_{time.strftime('%Y%m%d_%H%M%S')}.txt"
        
        dropped_count = report["total_profiles"] - report["profiles_kept"]
        
        reason_map = {
            "missing_keys": "Required dataset keys were missing",
            "non_finite": "Contained NaN or Infinity values",
            "below_threshold": f"A species value was below the threshold ({self.config['preprocessing']['min_value_threshold']:.1e})",
            "too_few_timesteps": "Contained 1 or fewer time steps",
            "extract_profile_failed": "Failed to extract global variables from name",
            "read_error": "Could not read a dataset from the HDF5 group"
        }

        with open(summary_path, 'w') as f:
            f.write("="*60 + "\n")
            f.write("      Data Preprocessing Summary\n")
            f.write("="*60 + "\n\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Raw Files Processed: {len(self.raw_files)}\n\n")

            f.write("--- Profile Filtering --- \n")
            f.write(f"Total Profiles Found:     {report['total_profiles']:,}\n")
            f.write(f"Profiles Kept:            {report['profiles_kept']:,}\n")
            f.write(f"Profiles Dropped:         {dropped_count:,}\n\n")
            
            if dropped_count > 0:
                f.write("--- Reasons for Dropped Profiles ---\n")
                for reason, count in sorted(report["dropped_reasons"].items()):
                    f.write(f"  - {count:>10,} : {reason_map.get(reason, reason)}\n")
                f.write("\n")

            f.write("--- Final Sample Count ---\n")
            f.write(f"Total Usable Samples:     {total_samples:,}\n")
            f.write("(Train/Val/Test splits generated separately)\n")

        self.logger.info(f"Preprocessing summary saved to: {summary_path}")


class ShardWriter:
    """Writes numpy arrays to shard files, handling buffering and file naming."""
    def __init__(self, output_dir: Path, shard_size: int, shard_idx_base: str):
        self.output_dir = output_dir
        self.shard_size = shard_size
        self.shard_idx_base = shard_idx_base
        self.buffer: List[np.ndarray] = []
        self.buffer_size = 0
        self.local_shard_id = 0
        self.shard_metadata: List[Dict] = []
        
        # Ensure the output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def add_samples(self, samples: np.ndarray):
        """Simplified add_samples without tracking global indices."""
        if samples.dtype != np.float32:
            samples = samples.astype(np.float32)
        
        self.buffer.append(samples)
        self.buffer_size += samples.shape[0]
        
        while self.buffer_size >= self.shard_size:
            self._write_shard()

    def _write_shard(self) -> None:
        """Write one shard of exactly shard_size samples (or less if flushing)."""
        if not self.buffer:
            return

        rows_to_write = []
        size_so_far = 0

        # Collect arrays until we have enough for a shard
        while self.buffer and size_so_far < self.shard_size:
            arr = self.buffer.pop(0)
            needed = self.shard_size - size_so_far
            
            if arr.shape[0] <= needed:
                rows_to_write.append(arr)
                size_so_far += arr.shape[0]
            else:
                # Split the array
                rows_to_write.append(arr[:needed])
                self.buffer.insert(0, arr[needed:])
                size_so_far += needed
        
        # Update buffer size
        self.buffer_size = sum(arr.shape[0] for arr in self.buffer)

        # Write shard
        data = np.concatenate(rows_to_write).astype(np.float32, copy=False)
        
        final_path = self.output_dir / f"shard_{self.shard_idx_base}_{self.local_shard_id:04d}.npy"
        tmp_path = final_path.with_suffix(".tmp.npy")

        np.save(tmp_path, data, allow_pickle=False)
        os.replace(tmp_path, final_path)

        self.shard_metadata.append({
            "filename": final_path.name,
            "n_samples": data.shape[0],
        })
        
        self.local_shard_id += 1

    def flush(self) -> None:
        """Write out any rows left in the buffer (< shard_size)."""
        if self.buffer_size == 0:
            return
        
        # write whatever is left as a final shard
        rows_to_write = self.buffer
        self.buffer = []
        self.buffer_size = 0

        data = np.concatenate(rows_to_write).astype(np.float32, copy=False)
        final_path = self.output_dir / f"shard_{self.shard_idx_base}_{self.local_shard_id:04d}.npy"
        tmp_path   = final_path.with_suffix(".tmp.npy")
        np.save(tmp_path, data, allow_pickle=False)
        os.replace(tmp_path, final_path)

        self.shard_metadata.append({
            "filename": final_path.name,
            "n_samples": data.shape[0],
        })
        self.local_shard_id += 1

    def get_shard_metadata(self) -> List[Dict]:
        """Return metadata collected for all shards."""
        return list(self.shard_metadata)
    
def shard_worker_split_aware(file_path, config, norm_stats, output_dir):
    """Worker function that creates split-specific shards."""
    processor = CorePreprocessor(config, norm_stats)
    return processor.process_file_for_shards(Path(file_path), Path(output_dir))   

===== /Users/imalsky/Desktop/Chemulator/src/data/dataset.py =====
#!/usr/bin/env python3
"""
High-performance dataset implementation for chemical kinetics training.

This module provides efficient data loading from numpy shard files with:
- Intelligent memory-based caching with LRU eviction
- Zero-copy tensor creation for optimal performance
- Multi-worker support with proper memory management
- Binary search for O(log n) sample lookups
- Conservative memory allocation to prevent OOM issues
- Shard-aware sampling to maximize cache efficiency
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple, Optional, List, Iterator
from functools import lru_cache

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, Sampler
import psutil
import os


class ShardAwareSampler(Sampler):
    """
    A sampler that generates indices in a shard-aware manner to maximize cache efficiency.
    
    This sampler:
    1. Groups indices by their shard
    2. Shuffles shards randomly
    3. Within each shard, shuffles indices randomly
    4. Yields batches that primarily come from 1-2 shards at a time
    
    This dramatically reduces cache misses and disk I/O.
    """
    def __init__(self, dataset: 'NPYDataset', batch_size: int, drop_last: bool = True, seed: int = 0):
        self.dataset = dataset
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.seed = seed
        
        # Group indices by shard
        self.shard_to_indices = self._group_indices_by_shard()
        self.total_samples = len(dataset)
        
    def _group_indices_by_shard(self) -> Dict[int, List[int]]:
            """
            Group dataset indices by their shard using robust binary search.
            This correctly handles shards of variable sizes.
            """
            shard_groups = {}
            if not len(self.dataset):
                return shard_groups

            # Get the start indices of all shards in this split
            shard_starts = self.dataset._shard_starts
            
            # Use numpy's highly optimized searchsorted to find the
            # shard index for every sample in the dataset in one go.
            all_shard_indices = np.searchsorted(
                shard_starts, 
                np.arange(len(self.dataset)), 
                side='right'
            ) - 1
            
            # Now, group the dataset indices by their calculated shard index
            for dataset_idx, shard_idx in enumerate(all_shard_indices):
                # The key must be an integer for dictionary access
                shard_idx_int = int(shard_idx)
                if shard_idx_int not in shard_groups:
                    shard_groups[shard_idx_int] = []
                shard_groups[shard_idx_int].append(dataset_idx)
                
            return shard_groups
    
    def __iter__(self) -> Iterator[int]:
        """Generate indices in a cache-friendly order."""
        # Set random seed for reproducibility
        rng = np.random.RandomState(self.seed + torch.utils.data.get_worker_info().id 
                                     if torch.utils.data.get_worker_info() else self.seed)
        
        # Shuffle shard order
        shard_order = list(self.shard_to_indices.keys())
        rng.shuffle(shard_order)
        
        # Collect all indices in shard-aware order
        all_indices = []
        for shard_idx in shard_order:
            # Get indices for this shard and shuffle them
            shard_indices = self.shard_to_indices[shard_idx].copy()
            rng.shuffle(shard_indices)
            all_indices.extend(shard_indices)
        
        # Yield batches
        for i in range(0, len(all_indices) - self.batch_size + 1, self.batch_size):
            yield all_indices[i:i + self.batch_size]
            
        # Handle last batch
        if not self.drop_last and len(all_indices) % self.batch_size != 0:
            yield all_indices[-(len(all_indices) % self.batch_size):]
    
    def __len__(self) -> int:
        """Return the number of batches."""
        if self.drop_last:
            return self.total_samples // self.batch_size
        else:
            return (self.total_samples + self.batch_size - 1) // self.batch_size


class NPYDataset(Dataset):
    """
    PyTorch Dataset for loading chemical kinetics data from numpy shard files.
    
    This dataset efficiently handles large-scale data by:
    - Loading entire shards into memory for fast access (no mmap overhead)
    - Caching frequently accessed shards with LRU eviction
    - Creating PyTorch tensors without data copying
    - Supporting train/validation/test splits via index arrays
    - Conservative memory allocation to prevent OOM issues
    - Providing shard-aware sampling for cache efficiency
    
    Args:
        shard_dir: Directory containing shard files and metadata
        indices: Array of global sample indices for this split
        config: Training configuration dictionary
        device: PyTorch device (used for logging, not data loading)
        split_name: Name of this split (train/val/test) for logging
    """
    def __init__(self, shard_dir: Path, split_name: str, config: Dict[str, Any], device: torch.device):
        """Initialize dataset for a specific split - much simpler!"""
        super().__init__()
        self.shard_dir = Path(shard_dir)
        self.split_dir = self.shard_dir / split_name
        self.split_name = split_name
        self.config = config
        self.device = device
        self.logger = logging.getLogger(__name__)

        # Verify split directory exists
        if not self.split_dir.exists():
            raise FileNotFoundError(f"Split directory not found: {self.split_dir}")

        # Load shard index metadata
        shard_index_path = self.shard_dir / "shard_index.json"
        with open(shard_index_path) as f:
            full_index = json.load(f)
        
        self.shard_index = full_index
        self.split_info = full_index["splits"][split_name]
        
        # Extract dimensions
        self.n_species = self.shard_index["n_species"]
        self.n_globals = self.shard_index["n_globals"]
        self.samples_per_shard = self.shard_index["samples_per_shard"]
        self.prediction_mode = self.shard_index.get("prediction_mode", "absolute")
        self.n_features = self.n_species * 2 + self.n_globals + 1
        
        # Get shard info for this split
        self.shards = self.split_info["shards"]
        self.n_shards = len(self.shards)
        self.n_total_samples = self.split_info["total_samples"]
        
        # Build lookup arrays for O(log n) access
        self._shard_starts = np.array([s["start_idx"] for s in self.shards])
        self._shard_ends = np.array([s["end_idx"] for s in self.shards])
        
        # Memory calculations
        self.bytes_per_sample = self.n_features * 4  # float32
        self.bytes_per_shard = self.samples_per_shard * self.bytes_per_sample
        
        # Initialize caching
        self.cache_is_setup = False
        self._determine_cache_size()
        
        # Log initialization
        self.logger.info(
            f"NPYDataset '{self.split_name}' initialized: "
            f"{self.n_total_samples:,} samples across {self.n_shards} shards "
            f"({self.bytes_per_shard / 1024**2:.1f} MB/shard), "
            f"cache capacity: {self._max_cache_size} shards"
        )


    def _determine_cache_size(self):
        """
        Calculate optimal shard cache size based on available system memory.
        Uses conservative estimates to prevent OOM issues when multiple
        datasets are running simultaneously.
        """
        # Query available system memory
        try:
            mem_info = psutil.virtual_memory()
            available_memory = mem_info.available
            total_memory = mem_info.total
            memory_percent_free = (available_memory / total_memory) * 100
            
            self.logger.debug(
                f"System memory: {total_memory / 1024**3:.1f} GB total, "
                f"{available_memory / 1024**3:.1f} GB available ({memory_percent_free:.1f}% free)"
            )
        except Exception as e:
            self.logger.warning(f"Failed to query system memory: {e}. Using 4GB fallback.")
            available_memory = 4 * 1024**3  # Conservative 4GB fallback

        # Conservative memory allocation (30% of available)
        cache_memory_fraction = 0.3
        total_cache_memory = available_memory * cache_memory_fraction
        
        # Account for multiple datasets running concurrently
        # Assume up to 3 datasets (train/val/test) may be active
        num_concurrent_datasets = 3
        cache_memory_per_dataset = total_cache_memory / num_concurrent_datasets
        
        # Account for workers in this dataset
        num_workers = self.config["training"].get("num_workers", 1)
        
        if num_workers > 0:
            max_cache_memory_per_worker = cache_memory_per_dataset / num_workers
            
            self.logger.debug(
                f"Allocating {cache_memory_per_dataset / 1024**3:.1f} GB cache memory "
                f"for {self.split_name} dataset across {num_workers} workers "
                f"({max_cache_memory_per_worker / 1024**3:.2f} GB each)"
            )
        else:
            max_cache_memory_per_worker = cache_memory_per_dataset

        # Account for memory overhead and fragmentation
        # Each shard needs ~2x its size due to Python overhead, fragmentation, etc.
        memory_overhead_factor = 2.0
        effective_shard_size = self.bytes_per_shard * memory_overhead_factor
        
        # Calculate how many shards fit in allocated memory per worker
        memory_based_shards = int(max_cache_memory_per_worker / max(1, effective_shard_size))
        
        config_limit = self.config["training"].get("dataset_cache_shards", 16)  # Changed default
        
        if num_workers >= 16:
            practical_limit = 64
        elif num_workers >= 8:
            practical_limit = 256
        elif num_workers >= 4:
            practical_limit = 512
        else:
            practical_limit = 1024
        
        # Use the minimum of all constraints, with a floor of 1 shard
        self._max_cache_size = max(1, min(
            memory_based_shards,
            config_limit,
            practical_limit
        ))
        
        # Calculate and log expected memory usage
        expected_cache_gb = (self._max_cache_size * effective_shard_size) / 1024**3
        total_expected_gb = expected_cache_gb * num_workers
        
        self.logger.info(
            f"Cache size for '{self.split_name}': "
            f"{self._max_cache_size} shards per worker "
            f"(memory_based={memory_based_shards}, config={config_limit}, practical={practical_limit}). "
            f"Expected memory: {expected_cache_gb:.1f} GB per worker, "
            f"{total_expected_gb:.1f} GB total for {num_workers} workers"
        )
        
        # Warn if memory usage seems high
        if total_expected_gb > total_memory / 1024**3 * 0.5:
            self.logger.warning(
                f"High memory usage warning: Expected cache memory ({total_expected_gb:.1f} GB) "
                f"exceeds 50% of system RAM. Consider reducing num_workers or dataset_cache_shards."
            )

    def _setup_worker_cache(self) -> None:
        """
        Build an independent LRU cache in each worker process.
        This is called lazily when the dataset is first accessed in a worker.
        """
        if getattr(self, "_cache_is_ready", False):
            return

        # Create the LRU-cached version of the shard loader
        self._get_shard_data = lru_cache(maxsize=self._max_cache_size)(self._get_shard_data_impl)

        # Run memory check once per worker
        if not getattr(self, "_ram_guard_ran", False):
            self.check_memory_requirements()
            self._ram_guard_ran = True

        # Mark cache as ready
        self.cache_is_setup = True
        self._cache_is_ready = True


    def _get_shard_data_impl(self, shard_idx: int) -> np.ndarray:
        """Load a shard from the split-specific directory."""
        shard_info = self.shards[shard_idx]
        shard_path = self.split_dir / shard_info["filename"]
        exp_samples = shard_info["n_samples"]

        self.logger.debug(f"Loading shard {shard_idx} from {self.split_name}: {shard_path}")

        t0 = time.time()
        
        # Load data
        if self.shard_index.get("compression") == "npz":
            with np.load(shard_path) as zf:
                data = zf["data"].astype(np.float32, copy=False)
        else:
            arr = np.load(shard_path)
            data = arr.astype(np.float32) if arr.dtype != np.float32 else arr
            if not data.flags["C_CONTIGUOUS"]:
                data = np.ascontiguousarray(data)

        self.logger.debug(f"Shard loaded in {(time.time()-t0):.3f}s")

        # Validate
        if data.shape[0] != exp_samples:
            raise ValueError(f"Shard sample mismatch ({data.shape[0]} vs {exp_samples})")
        if data.shape[1] != self.n_features:
            raise ValueError(f"Feature dim mismatch ({data.shape[1]} vs {self.n_features})")

        return data

    def __len__(self) -> int:
        """Return the total number of samples in this dataset split."""
        return self.n_total_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Load a single sample - much simpler without index translation!"""
        # Initialize cache on first access
        if not self.cache_is_setup:
            self._setup_worker_cache()

        # Validate index
        if not (0 <= idx < self.n_total_samples):
            raise IndexError(f"Index {idx} out of range [0, {self.n_total_samples})")

        try:
            # Find shard using binary search
            shard_idx = np.searchsorted(self._shard_starts, idx, side='right') - 1
            
            # Calculate local index within shard
            local_idx = idx - self._shard_starts[shard_idx]
            
            # Load shard data (may hit cache)
            shard_data = self._get_shard_data(shard_idx)
            
            # Extract the sample row
            row = shard_data[local_idx]
            
            # Split into input and target
            n_input = self.n_species + self.n_globals + 1
            
            # Create tensors (zero-copy)
            input_tensor = torch.from_numpy(row[:n_input])
            target_tensor = torch.from_numpy(row[n_input:])
            
            return input_tensor, target_tensor

        except Exception as e:
            self.logger.error(
                f"Error accessing sample idx={idx} in {self.split_name}: {e}",
                exc_info=True
            )
            raise

    def check_memory_requirements(self) -> Dict[str, float]:
        """
        Pre-flight check: Estimate memory requirements and validate against available RAM.
        Accounts for the fact that multiple datasets may be running concurrently.
        Returns dict with memory estimates.
        """
        import psutil
        
        # Get current memory state
        mem_info = psutil.virtual_memory()
        available_gb = mem_info.available / 1024**3
        total_gb = mem_info.total / 1024**3
        
        # Calculate per-component memory usage
        num_workers = self.config["training"]["num_workers"]
        cache_shards = self._max_cache_size
        batch_size = self.config["training"]["batch_size"]
        prefetch = self.config["training"].get("prefetch_factor", 2)
        
        # Memory calculations (in GB)
        shard_gb = self.bytes_per_shard / 1024**3
        batch_gb = (batch_size * self.n_features * 4) / 1024**3  # float32
        
        # Component breakdown
        memory_breakdown = {
            "shard_cache_per_worker_gb": cache_shards * shard_gb * 2,  # 2x for overhead
            "prefetch_per_worker_gb": prefetch * batch_gb,
            "num_workers": num_workers,
            "python_overhead_gb": 1.0,  # Base Python/PyTorch overhead
            "dataloader_overhead_gb": num_workers * 0.5,  # Per-worker overhead
        }
        
        # Total expected usage for this dataset
        total_expected_gb = (
            num_workers * memory_breakdown["shard_cache_per_worker_gb"] +
            num_workers * memory_breakdown["prefetch_per_worker_gb"] +
            memory_breakdown["python_overhead_gb"] +
            memory_breakdown["dataloader_overhead_gb"]
        )
        
        memory_breakdown["total_expected_gb"] = total_expected_gb
        memory_breakdown["available_gb"] = available_gb
        memory_breakdown["total_system_gb"] = total_gb
        memory_breakdown["usage_percent"] = (total_expected_gb / available_gb) * 100
        
        # Log detailed breakdown
        #self.logger.info(f"\n{'='*60}")
        #self.logger.info(f"Memory Requirements Check for '{self.split_name}' Dataset")
        #self.logger.info(f"{'='*60}")
        #self.logger.info(f"System Memory: {total_gb:.1f} GB total, {available_gb:.1f} GB available")
        #self.logger.info(f"Configuration: {num_workers} workers, {cache_shards} cached shards, "
        #                f"{batch_size} batch size")
        #self.logger.info(f"\nMemory Breakdown:")
        #self.logger.info(f"  - Shard cache: {memory_breakdown['shard_cache_per_worker_gb']:.2f} GB/worker")
        #self.logger.info(f"  - Prefetch buffer: {memory_breakdown['prefetch_per_worker_gb']:.2f} GB/worker")
        #self.logger.info(f"  - Python overhead: {memory_breakdown['python_overhead_gb']:.2f} GB")
        #self.logger.info(f"  - Worker overhead: {memory_breakdown['dataloader_overhead_gb']:.2f} GB")
        #self.logger.info(f"\nTotal Expected: {total_expected_gb:.1f} GB "
        #                f"({memory_breakdown['usage_percent']:.0f}% of available)")
        #self.logger.info(f"{'='*60}\n")
        
        # Validate memory requirements
        safety_factor = 0.8  # Don't use more than 80% of available memory
        if total_expected_gb > available_gb * safety_factor:
            error_msg = (
                f"Insufficient memory for {self.split_name} dataset: "
                f"need {total_expected_gb:.1f} GB, "
                f"but only {available_gb * safety_factor:.1f} GB safely available. "
                f"Reduce num_workers (currently {num_workers}) or "
                f"dataset_cache_shards (currently {cache_shards})."
            )
            self.logger.error(error_msg)
            raise MemoryError(error_msg)
        
        return memory_breakdown


def create_dataloader(dataset: Dataset,
                      config: Dict[str, Any],
                      shuffle: bool = True,
                      device: Optional[torch.device] = None,
                      drop_last: bool = True,
                      use_shard_aware_sampling: bool = True) -> DataLoader:
    """
    Build a DataLoader with safe, high‑performance defaults.
    
    Args:
        dataset: The dataset to load from
        config: Configuration dictionary
        shuffle: Whether to shuffle the data (ignored if using shard-aware sampling)
        device: PyTorch device for memory checks
        drop_last: Whether to drop the last incomplete batch
        use_shard_aware_sampling: Whether to use shard-aware sampling for better cache efficiency
        
    Returns:
        Configured DataLoader or None if dataset is empty
    """
    if dataset is None or len(dataset) == 0:
        logging.getLogger(__name__).warning("Cannot create DataLoader for empty dataset")
        return None

    log     = logging.getLogger(__name__)
    tcfg    = config["training"]
    bs      = tcfg["batch_size"]
    workers = min(32, tcfg.get("num_workers", 0))

    # Pin memory only if using CUDA and workers
    pin   = (tcfg.get("pin_memory", False) and workers > 0
             and device is not None and device.type == "cuda")
    
    # Persistent workers only if pinning memory
    pers  = tcfg.get("persistent_workers", False) and pin
    
    # Prefetch factor only matters with workers
    pre   = tcfg.get("prefetch_factor", 2) if workers > 0 else 1

    # Validate batch and prefetch settings for GPU memory
    _validate_batch_prefetch(bs, pre, dataset.n_features, device)

    # Determine if we should use shard-aware sampling
    if use_shard_aware_sampling and shuffle and isinstance(dataset, NPYDataset):
        # Use shard-aware sampler for training data
        batch_sampler = ShardAwareSampler(
            dataset=dataset,
            batch_size=bs,
            drop_last=drop_last,
            seed=config.get("system", {}).get("seed", 42)
        )
        
        log.info(f"DataLoader[{dataset.split_name}] using ShardAwareSampler: "
                 f"bs={bs}  workers={workers}  pin={pin}  pers={pers}  prefetch={pre}")
        
        return DataLoader(
            dataset,
            batch_sampler=batch_sampler,  # Use our shard-aware sampler
            num_workers=workers,
            pin_memory=pin,
            persistent_workers=pers,
            prefetch_factor=pre,
            worker_init_fn=_worker_init_fn,
        )
    else:
        # Use standard DataLoader for validation/test or when shard-aware is disabled
        log.info(f"DataLoader[{dataset.split_name}]  bs={bs}  workers={workers}  "
                 f"pin={pin}  pers={pers}  prefetch={pre}")

        return DataLoader(
            dataset,
            batch_size=bs,
            shuffle=shuffle,
            num_workers=workers,
            pin_memory=pin,
            persistent_workers=pers,
            prefetch_factor=pre,
            worker_init_fn=_worker_init_fn,
            drop_last=drop_last,
        )


def _worker_init_fn(worker_id: int):
    """
    Initialization code run once per DataLoader worker process.
    - Disable CUDA in workers (they should only load data)
    - Limit OpenBLAS/MKL threads to prevent oversubscription
    - Set deterministic but distinct RNG seeds
    - Light stagger to avoid simultaneous disk hits
    """
    import os, time, numpy as np, torch, random

    # Prevent workers from using CUDA
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    # Limit CPU threads to prevent oversubscription
    torch.set_num_threads(1)

    # Stagger worker startup to avoid disk contention
    time.sleep(0.25 * worker_id)

    # Set unique but deterministic seeds for each worker
    seed = int(time.time()) + worker_id
    random.seed(seed)
    np.random.seed(seed % (2**32 - 1))
    torch.manual_seed(seed)


def _validate_batch_prefetch(batch_size: int,
                             prefetch_factor: int,
                             feature_dim: int,
                             device: Optional[torch.device]) -> None:
    """
    Validate that prefetched batches won't exceed GPU memory.
    Raises RuntimeError if the configuration would cause OOM.
    """
    if device is None or device.type != "cuda":
        return
    
    import torch
    
    # Calculate memory needed for prefetched batches
    bytes_needed = batch_size * max(prefetch_factor, 1) * feature_dim * 4  # float32
    
    # Get available GPU memory
    free_bytes = torch.cuda.mem_get_info(device.index)[0]
    
    # Check if we'd use more than 80% of free GPU memory
    if bytes_needed > free_bytes * 0.80:
        human = bytes_needed / 1024**3
        raise RuntimeError(
            f"prefetch_factor×batch_size would pre‑queue ≈{human:.1f} GB "
            "→ exceeds safe free GPU memory. Reduce batch_size or prefetch_factor."
        )

===== /Users/imalsky/Desktop/Chemulator/src/data/normalizer.py =====
#!/usr/bin/env python3
"""
Data normalization module for chemical kinetics datasets.
This version preserves all data validation and logging and supports the
parallel preprocessing architecture with _merge_accumulators.
"""

import logging
import math
from typing import Dict, List, Any, Optional

import numpy as np
import torch

DEFAULT_EPSILON = 1e-30
DEFAULT_MIN_STD = 1e-10
DEFAULT_CLAMP = 50.0

class DataNormalizer:
    """Calculates normalization statistics with robust data validation."""
    
    def __init__(self, config: Dict[str, Any]) -> None:
        self.config = config
        self.data_config = config["data"]
        self.norm_config = config["normalization"]

        self.species_vars = self.data_config["species_variables"]
        self.global_vars = self.data_config["global_variables"]
        self.time_var = self.data_config["time_variable"]
        self.all_vars = self.species_vars + self.global_vars + [self.time_var]

        self.epsilon = self.norm_config.get("epsilon", DEFAULT_EPSILON)
        self.min_std = self.norm_config.get("min_std", DEFAULT_MIN_STD)
        self.logger = logging.getLogger(__name__)
        
    def _initialize_accumulators(self) -> Dict[str, Dict[str, Any]]:
        """Initialize per-variable statistics accumulators."""
        accumulators = {}
        for i, var in enumerate(self.all_vars):
            method = self._get_method(var)
            if method == "none":
                continue
            acc = {
                "method": method, "index": i, "count": 0, "mean": 0.0, "m2": 0.0,
                "min": float("inf"), "max": float("-inf"),
            }
            accumulators[var] = acc
        return accumulators
    
    def _get_method(self, var: str) -> str:
        """Get normalization method for a variable."""
        methods = self.norm_config.get("methods", {})
        return methods.get(var, self.norm_config["default_method"])
    
    def _update_single_accumulator(self, acc: Dict[str, Any], vec: np.ndarray, var_name: str):
        """
        Vectorized update for one accumulator, including all validation and logging.
        """
        if vec.size == 0:
            return

        # 1. Filter non-finite values and warn if there are many
        finite_mask = np.isfinite(vec)
        if not np.all(finite_mask):
            n_non_finite = (~finite_mask).sum()
            if n_non_finite / vec.size > 0.01:
                self.logger.warning(f"Variable '{var_name}' has {n_non_finite}/{vec.size} non-finite values.")
            vec = vec[finite_mask]
            if vec.size == 0:
                self.logger.warning(f"Variable '{var_name}' has no finite values after filtering, skipping.")
                return

        # 2. Handle log-transformations with validation checks
        if acc["method"].startswith("log-"):
            below_epsilon = vec < self.epsilon
            if np.any(below_epsilon):
                self.logger.warning(
                    f"Variable '{var_name}' has {below_epsilon.sum()} values below epsilon {self.epsilon} "
                    f"Min value: {vec.min():.2e}"
                )
            vec = np.log10(np.maximum(vec, self.epsilon))
            
            if vec.min() < -30 or vec.max() > 30:
                self.logger.warning(
                    f"Variable '{var_name}' has extreme log values: [{vec.min():.1f}, {vec.max():.1f}]"
                )

        # 3. Perform Chan's parallel update for mean and variance
        n_b = vec.size
        mean_b = float(vec.mean())
        m2_b = float(((vec - mean_b) ** 2).sum()) if n_b > 1 else 0.0

        n_a = acc["count"]
        delta = mean_b - acc["mean"]
        n_ab = n_a + n_b

        if n_ab > 0:
            acc["mean"] = (n_a * acc["mean"] + n_b * mean_b) / n_ab
            acc["m2"] += m2_b + delta**2 * n_a * n_b / n_ab
        
        acc["count"] = n_ab
        acc["min"] = min(acc["min"], float(vec.min()))
        acc["max"] = max(acc["max"], float(vec.max()))

    def _merge_accumulators(
        self,
        main_accs: Dict[str, Dict[str, Any]],
        other_accs: Dict[str, Dict[str, Any]],
    ) -> None:
        """Merge statistics from another set of accumulators into the main one."""
        for var, other_acc in other_accs.items():
            if not other_acc: continue
            if var not in main_accs:
                main_accs[var] = other_acc
                continue

            main_acc = main_accs[var]
            
            n_a, mean_a, m2_a = main_acc["count"], main_acc["mean"], main_acc["m2"]
            n_b, mean_b, m2_b = other_acc["count"], other_acc["mean"], other_acc["m2"]
            
            n_ab = n_a + n_b
            if n_ab == 0: continue
                
            delta = mean_b - mean_a
            
            main_acc["mean"] = (n_a * mean_a + n_b * mean_b) / n_ab
            main_acc["m2"] = m2_a + m2_b + (delta**2 * n_a * n_b) / n_ab
            main_acc["count"] = n_ab
            main_acc["min"] = min(main_acc["min"], other_acc["min"])
            main_acc["max"] = max(main_acc["max"], other_acc["max"])
        
    def _finalize_statistics(self, accumulators: Dict[str, Dict[str, Any]], is_ratio: bool = False) -> Dict[str, Any]:
        """Finalize statistics from accumulators."""
        stats = { "per_key_stats": {} }
        if not is_ratio:
            stats["normalization_methods"] = {}

        for var, acc in accumulators.items():
            method = acc.get("method", "standard")
            if not is_ratio:
                stats["normalization_methods"][var] = method
            
            if method == "none": continue
            
            var_stats = {"method": method}
            
            if acc["count"] > 1:
                variance = acc["m2"] / (acc["count"] - 1)
                std = max(math.sqrt(variance), self.min_std)
            else:
                std = 1.0
            
            if "standard" in method:
                mean_key, std_key = ("log_mean", "log_std") if "log" in method else ("mean", "std")
                var_stats[mean_key], var_stats[std_key] = acc["mean"], std
            elif "min-max" in method:
                var_stats["min"], var_stats["max"] = acc["min"], acc["max"]
                if acc["max"] - acc["min"] < self.epsilon:
                    var_stats["max"] = acc["min"] + 1.0
            
            if is_ratio:
                stats[var] = {"mean": acc["mean"], "std": std, "min": acc["min"], "max": acc["max"], "count": acc["count"]}
            else:
                stats["per_key_stats"][var] = var_stats

        if not is_ratio:
            for var in self.all_vars:
                if var not in stats["normalization_methods"]:
                    stats["normalization_methods"][var] = "none"
            stats["epsilon"] = self.epsilon
            stats["clamp_value"] = self.norm_config.get("clamp_value", DEFAULT_CLAMP)
        
        return stats


class NormalizationHelper:
    """Applies pre-computed normalization statistics to data tensors."""
    
    def __init__(self, stats: Dict[str, Any], device: torch.device, 
                 species_vars: List[str], global_vars: List[str], 
                 time_var: str, config: Optional[Dict[str, Any]] = None):
        self.stats = stats
        self.device = device
        self.species_vars = species_vars
        self.global_vars = global_vars
        self.time_var = time_var

        self.n_species = len(species_vars)
        self.n_globals = len(global_vars)
        self.methods = stats["normalization_methods"]
        self.per_key_stats = stats["per_key_stats"]

        self.epsilon = stats.get("epsilon", DEFAULT_EPSILON)
        self.clamp_value = stats.get("clamp_value", DEFAULT_CLAMP)
        
        self.ratio_stats = stats.get("ratio_stats", None)

        self.logger = logging.getLogger(__name__)
        self._precompute_parameters()

    def _precompute_parameters(self):
        """Pre-compute normalization parameters on the target device for efficiency."""
        self.norm_params = {}
        self.method_groups = { "standard": [], "log-standard": [], "min-max": [], "log-min-max": [], "none": [] }
        var_to_col = {var: i for i, var in enumerate(self.species_vars + self.global_vars + [self.time_var])}

        for var, method in self.methods.items():
            if method == "none" or var not in self.per_key_stats:
                self.method_groups["none"].append(var)
                continue

            var_stats = self.per_key_stats[var]
            params = {"method": method}

            if "standard" in method:
                mean_key, std_key = ("log_mean", "log_std") if "log" in method else ("mean", "std")
                params["mean"] = torch.tensor(var_stats[mean_key], dtype=torch.float32, device=self.device)
                params["std"] = torch.tensor(var_stats[std_key], dtype=torch.float32, device=self.device)
            elif "min-max" in method:
                params["min"] = torch.tensor(var_stats["min"], dtype=torch.float32, device=self.device)
                params["max"] = torch.tensor(var_stats["max"], dtype=torch.float32, device=self.device)

            self.norm_params[var] = params
            self.method_groups[method].append(var)

        self.col_indices = {method: [var_to_col[var] for var in v_list] for method, v_list in self.method_groups.items() if v_list}

    def normalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """Normalize a complete profile tensor using vectorized operations."""
        if profile.device != self.device:
            profile = profile.to(self.device)
        normalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none": continue
            cols = normalized[:, col_idxs]
            if "standard" in method:
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                data = torch.log10(torch.clamp(cols, min=self.epsilon)) if "log" in method else cols
                normalized[:, col_idxs] = torch.clamp((data - means) / stds, -self.clamp_value, self.clamp_value)
            elif "min-max" in method:
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = torch.clamp(maxs - mins, min=self.epsilon)
                data = torch.log10(torch.clamp(cols, min=self.epsilon)) if "log" in method else cols
                normalized[:, col_idxs] = torch.clamp((data - mins) / ranges, 0.0, 1.0)
        return normalized

    def denormalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """Denormalize a complete profile tensor using vectorized operations."""
        if profile.device != self.device:
            profile = profile.to(self.device)
        denormalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none": continue
            cols = denormalized[:, col_idxs]
            if "standard" in method:
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                raw_vals = cols * stds + means
                if "log" in method:
                    raw_vals = torch.pow(10.0, torch.clamp(raw_vals, min=-38.0, max=38.0))
                denormalized[:, col_idxs] = torch.clamp(raw_vals, min=-3.4e38, max=3.4e38)
            elif "min-max" in method:
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = torch.clamp(maxs - mins, min=self.epsilon)
                raw_vals = cols * ranges + mins
                if "log" in method:
                    raw_vals = torch.pow(10.0, torch.clamp(raw_vals, min=-38.0, max=38.0))
                denormalized[:, col_idxs] = raw_vals
        return denormalized
        
    def denormalize_ratio_predictions(self, standardized_log_ratios: torch.Tensor,
                                    initial_species: torch.Tensor) -> torch.Tensor:
        """Convert standardized log-ratio predictions back to absolute species values."""
        if self.ratio_stats is None:
            raise ValueError("Ratio statistics not available for denormalization.")

        device = standardized_log_ratios.device
        initial_species = initial_species.to(device)

        ratio_means = torch.tensor([self.ratio_stats[var]["mean"] for var in self.species_vars], device=device, dtype=torch.float32)
        ratio_stds = torch.tensor([self.ratio_stats[var]["std"] for var in self.species_vars], device=device, dtype=torch.float32)
        
        log_ratios = (standardized_log_ratios * ratio_stds) + ratio_means
        log_ratios = torch.clamp(log_ratios, min=-38.0, max=38.0)
        
        ratios = torch.pow(10.0, log_ratios)
        predicted_species = initial_species * ratios
        return predicted_species

