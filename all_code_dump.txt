===== /Users/imalsky/Desktop/Chemulator/src/main.py =====
#!/usr/bin/env python3
import logging
import sys
import time
from pathlib import Path
import torch
from typing import Dict, Any, Union
import hashlib
import json
import os

# =================================================================
# 1. CONFIGURE LOGGING FIRST
# We set this up at the very top so even the prologue can use it.
# =================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(name)s - %(message)s",
    datefmt="%Y%m%d_%H%M%S",
    stream=sys.stdout
)

# =================================================================
# 2. PROLOGUE FOR MAXIMUM PARALLELISM
# This block now uses the logging system to report its actions.
# =================================================================
try:
    # SLURM_CPUS_PER_TASK is the ground truth for the number of cores allocated
    n_cores_str = os.environ.get("SLURM_CPUS_PER_TASK")
    source = "SLURM"
    
    if n_cores_str is None:
        # Fallback for local execution or other schedulers
        n_cores_str = str(os.cpu_count() or 1)
        source = "os.cpu_count()"

    n_cores = int(n_cores_str)
    
    logging.info(f"CORE PROLOGUE: Detected {n_cores} cores via {source}.")
    
    # Set environment variables to force thread counts
    os.environ["OMP_NUM_THREADS"] = str(n_cores)
    os.environ["MKL_NUM_THREADS"] = str(n_cores)
    logging.info(f"CORE PROLOGUE: Set OMP_NUM_THREADS and MKL_NUM_THREADS to {n_cores}.")
    
    # Set PyTorch's internal thread counts
    torch.set_num_threads(n_cores)
    
    # Final verification log
    logging.info(f"CORE PROLOGUE: Verification: torch.get_num_threads() now reports {torch.get_num_threads()} threads.")

except Exception as e:
    logging.error(f"CORE PROLOGUE: Failed to set thread counts: {e}", exc_info=True)


# =================================================================
# 3. REST OF THE APPLICATION
# (The rest of the file is unchanged)
# =================================================================
# Set multiprocessing sharing strategy
import torch.multiprocessing
try:
    torch.multiprocessing.set_sharing_strategy('file_system')
    logging.info("SUCCESS: Set multiprocessing sharing strategy to 'file_system'.")
except RuntimeError:
    logging.warning("Could not set multiprocessing sharing strategy.")

import numpy as np
from utils.hardware import setup_device, optimize_hardware
from utils.utils import setup_logging, seed_everything, ensure_directories, load_json_config, save_json, load_json
from data.preprocessor import DataPreprocessor
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer, diagnose_performance
from data.normalizer import NormalizationHelper


class ChemicalKineticsPipeline:
    """Optimized training pipeline for A100 GPU."""
    def __init__(self, config_or_path: Union[Path, Dict[str, Any]]):
        """Initialize the pipeline."""
        if isinstance(config_or_path, (Path, str)):
            self.config = load_json_config(Path(config_or_path))
        elif isinstance(config_or_path, dict):
            self.config = config_or_path
        else:
            raise TypeError(f"config_or_path must be a Path, str, or dict")
        
        # Get prediction mode
        self.prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        
        # Setup paths
        self.setup_paths()
        
        # Setup logging
        log_file = self.log_dir / f"pipeline_{self.prediction_mode}_{time.strftime('%Y%m%d_%H%M%S')}.log"
        setup_logging(log_file=log_file)
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Chemical Kinetics Pipeline initialized - Mode: {self.prediction_mode}")
        
        seed_everything(self.config["system"]["seed"])
        
        # Setup hardware
        self.device = setup_device()
        optimize_hardware(self.config["system"], self.device)
        
    def setup_paths(self):
        """Create directory structure."""
        paths = self.config["paths"]
        
        # Create run directory
        model_type = self.config["model"]["type"]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.run_save_dir = Path(paths["model_save_dir"]) / f"{model_type}_{self.prediction_mode}_{timestamp}"
        
        # Convert paths
        self.raw_data_files = [Path(f) for f in paths["raw_data_files"]]
        
        # Mode-specific processed directory
        base_processed_dir = Path(paths["processed_data_dir"])
        self.processed_dir = base_processed_dir / f"mode_{self.prediction_mode}"
        
        self.log_dir = Path(paths["log_dir"])
        
        # Create directories
        ensure_directories(self.processed_dir, self.run_save_dir, self.log_dir)

    def _compute_data_hash(self) -> str:
        """
        Compute a hash of data-critical parameters.
        """
        data_params = {
            "raw_files": sorted([str(f) for f in self.raw_data_files]),
            "species_variables": self.config["data"]["species_variables"],
            "global_variables": self.config["data"]["global_variables"],
            "time_variable": self.config["data"]["time_variable"],
            "min_value_threshold": self.config["preprocessing"]["min_value_threshold"],
            "use_fraction": self.config["training"]["use_fraction"],
            "prediction_mode": self.prediction_mode,
            "normalization_methods": self.config["normalization"].get("methods", {}),
            "default_norm_method": self.config["normalization"]["default_method"],
        }
        
        hash_str = json.dumps(data_params, sort_keys=True)
        return hashlib.sha256(hash_str.encode()).hexdigest()[:16]

    def normalize_only(self):
        """Run only the data preprocessing and normalization step."""
        self.logger.info("Running data normalization only...")
        
        # Check if data already exists
        current_hash = self._compute_data_hash()
        hash_file = self.processed_dir / "data_hash.json"
        
        regenerate = True
        if hash_file.exists():
            saved_hash_data = load_json(hash_file)
            if saved_hash_data.get("hash") == current_hash:
                self.logger.info("Data already preprocessed with matching hash. Skipping regeneration.")
                regenerate = False
            else:
                self.logger.info("Data hash mismatch. Regenerating data...")
        
        if regenerate:
            preprocessor = DataPreprocessor(
                raw_files=self.raw_data_files,
                output_dir=self.processed_dir,
                config=self.config
            )
            
            missing = [p for p in self.raw_data_files if not p.exists()]
            if missing:
                raise FileNotFoundError(f"Missing raw data files: {missing}")
            
            # Process to shards and compute normalization
            preprocessor.process_to_npy_shards()
            
            # Save the hash
            save_json({
                "hash": current_hash,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "mode": self.prediction_mode
            }, hash_file)
            
            self.logger.info(f"Data normalization complete. Files saved to: {self.processed_dir}")

    def preprocess_data(self):
        """Preprocess data if needed."""
        self.logger.info(f"Preprocessing data for {self.prediction_mode} mode...")
        self.normalize_only()

    def _log_memory_status(self):
        """Log current memory status."""
        import psutil
        
        # CPU memory
        mem = psutil.virtual_memory()
        self.logger.info(f"System memory: {mem.total/1024**3:.1f}GB total, "
                         f"{mem.available/1024**3:.1f}GB available ({mem.percent:.1f}% used)")
        
        # GPU memory
        if self.device.type == "cuda":
            free_mem, total_mem = torch.cuda.mem_get_info(self.device.index)
            used_mem = total_mem - free_mem
            self.logger.info(f"GPU memory: {total_mem/1024**3:.1f}GB total, "
                             f"{free_mem/1024**3:.1f}GB free, "
                             f"{used_mem/1024**3:.1f}GB used")

    def train_model(self):
        """Train the neural network model."""
        self.logger.info("Starting model training...")

        # Ensure data is preprocessed
        self.preprocess_data()

        # Save config for this run
        save_json(self.config, self.run_save_dir / "config.json")

        # Create model
        model = create_model(self.config, self.device)

        # Log model info
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"Model: {self.config['model']['type']} - Parameters: {total_params:,}")

        # Load normalization stats
        norm_stats = load_json(self.processed_dir / "normalization.json")
        norm_helper = NormalizationHelper(
            norm_stats,
            self.device,
            self.config["data"]["species_variables"],
            self.config["data"]["global_variables"],
            self.config["data"]["time_variable"],
            self.config
        )
        
        # Log memory status before creating datasets
        self._log_memory_status()

        # Create datasets
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="train",
            config=self.config,
            device=self.device
        )
        
        # **FIXED** Safely log cache status to prevent crash
        cache_info = train_dataset.get_cache_info()
        if cache_info.get("type") == "gpu":
            self.logger.info(
                f"âœ“ GPU caching active for '{train_dataset.split_name}': {cache_info.get('size_gb', 0):.1f}GB loaded."
            )
        elif cache_info.get("type") == "cpu":
            self.logger.warning(
                f"CPU fallback active for '{train_dataset.split_name}'. Reason: {cache_info.get('message', 'N/A')}"
            )
        else:
            self.logger.error(
                f"Cache status error for '{train_dataset.split_name}': {cache_info.get('status', 'unknown')}"
            )

        val_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="validation",
            config=self.config,
            device=self.device
        ) if self.config["training"]["val_fraction"] > 0 else None
        
        test_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="test",
            config=self.config,
            device=self.device
        ) if self.config["training"]["test_fraction"] > 0 else None
        
        # Initialize trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device,
            norm_helper=norm_helper
        )

        # Train model
        best_val_loss = trainer.train()
        
        # Evaluate on test set
        test_loss = trainer.evaluate_test()
        
        self.logger.info(f"Training complete! Best validation loss: {best_val_loss:.6f}")
        self.logger.info(f"Test loss: {test_loss:.6f}")
        
        # Save results
        results = {
            "best_val_loss": best_val_loss,
            "test_loss": test_loss,
            "model_path": str(self.run_save_dir / "best_model.pt"),
            "training_time": trainer.total_training_time,
        }
        
        save_json(results, self.run_save_dir / "results.json")
    
    def run_diagnostics(self, num_batches=20):
        """Run performance diagnostics."""
        self.logger.info("Running performance diagnostics...")
        
        # Ensure data is preprocessed
        self.preprocess_data()
        
        # Create model
        model = create_model(self.config, self.device)
        
        # Load norm stats
        norm_stats = load_json(self.processed_dir / "normalization.json")
        norm_helper = NormalizationHelper(
            norm_stats, self.device,
            self.config["data"]["species_variables"],
            self.config["data"]["global_variables"],
            self.config["data"]["time_variable"],
            self.config
        )
        
        # Create training dataset only
        train_dataset = NPYDataset(
            shard_dir=self.processed_dir,
            split_name="train",
            config=self.config,
            device=self.device
        )
        
        # Create minimal trainer
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=None,
            test_dataset=None,
            config=self.config,
            save_dir=self.run_save_dir,
            device=self.device,
            norm_helper=norm_helper
        )
        
        # Run diagnostics
        times = diagnose_performance(trainer, num_batches=num_batches)
        
        # Print summary
        print("\n" + "="*60)
        print("PERFORMANCE DIAGNOSTICS COMPLETE")
        print("="*60)
        print(f"Batch size: {self.config['training']['batch_size']}")
        print(f"Device: {self.device}")
        print(f"Model compilation: {self.config['system'].get('use_torch_compile', False)}")
        print(f"AMP enabled: {self.config['training'].get('use_amp', False)}")
        print(f"GPU cache: {train_dataset.get_cache_info()['type']}")
    
    def run(self):
        """Execute the full training pipeline."""
        try:
            self.train_model()
            self.logger.info("Pipeline completed successfully!")
            self.logger.info(f"Results saved in: {self.run_save_dir}")
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            sys.exit(1)

def main():
    """Main entry point."""
    import argparse
    parser = argparse.ArgumentParser(description="Chemical Kinetics Neural Network Training")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/config.jsonc"),
        help="Path to configuration file"
    )
    
    # Operation mode arguments
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument(
        "--normalize",
        action="store_true",
        help="Only preprocess and normalize the data"
    )
    mode_group.add_argument(
        "--train",
        action="store_true",
        help="Train a model using the configuration"
    )
    mode_group.add_argument(
        "--tune",
        action="store_true",
        help="Run hyperparameter optimization"
    )
    mode_group.add_argument(
        "--diagnose",
        action="store_true",
        help="Run performance diagnostics"
    )
    
    # Additional arguments
    parser.add_argument(
        "--trials",
        type=int,
        default=100,
        help="Number of Optuna trials for hyperparameter optimization"
    )
    parser.add_argument(
        "--study-name",
        type=str,
        default="chemical_kinetics_opt",
        help="Name for Optuna study"
    )
    parser.add_argument(
        "--diagnose-batches",
        type=int,
        default=20,
        help="Number of batches for diagnostics"
    )
    
    args = parser.parse_args()
    
    # Validate config path
    if not args.config.exists():
        print(f"Error: Configuration file not found: {args.config}", file=sys.stderr)
        sys.exit(1)
    
    # Execute based on mode
    if args.normalize:
        # Just normalize data
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.normalize_only()
        print("\nData normalization complete!")
        
    elif args.train:
        # Train model
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.run()
        
    elif args.diagnose:
        # Run diagnostics
        pipeline = ChemicalKineticsPipeline(args.config)
        pipeline.run_diagnostics(num_batches=args.diagnose_batches)
        
    elif args.tune:
        # Run hyperparameter optimization
        try:
            import optuna
        except ImportError:
            print("Installing optuna...")
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "optuna"])
        
        from hyperparameter_tuning import optimize
        
        print(f"Starting hyperparameter optimization with {args.trials} trials...")
        study = optimize(
            config_path=args.config,
            n_trials=args.trials,
            n_jobs=1,
            study_name=args.study_name
        )
        
        # Print results
        print("\n" + "="*60)
        print("Optimization Complete")
        print("="*60)
        print(f"Best validation loss: {study.best_value:.6f}")
        print(f"Best trial: {study.best_trial.number}")
        print("\nBest parameters:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")


if __name__ == "__main__":
    main()

===== /Users/imalsky/Desktop/Chemulator/src/hyperparameter_tuning.py =====
#!/usr/bin/env python3
"""
Hyperparameter tuning for chemical kinetics models using Optuna.
Optimized for ~40 hour runtime with aggressive but smart pruning.
"""

import copy
import logging
import time
from pathlib import Path
from typing import Dict, Any, Optional, Callable

import optuna
from optuna.samplers import TPESampler
from optuna.pruners import HyperbandPruner
import torch

from main import ChemicalKineticsPipeline
from utils.hardware import setup_device, optimize_hardware
from data.dataset import NPYDataset
from models.model import create_model
from training.trainer import Trainer
from data.normalizer import NormalizationHelper
from utils.utils import setup_logging, seed_everything, ensure_directories, load_json_config, save_json, load_json


class OptunaPruningCallback:
    """Callback to report intermediate values to Optuna for pruning."""
    def __init__(self, trial: optuna.Trial, min_epochs: int = 10):
        self.trial = trial
        self.min_epochs = min_epochs
        
    def __call__(self, epoch: int, val_loss: float) -> bool:
        """
        Report intermediate value to Optuna and check if should prune.
        Only allows pruning after min_epochs to avoid conflict with cosine warmup.
        """
        self.trial.report(val_loss, epoch)
        
        # Don't prune during warmup period
        if epoch < self.min_epochs:
            return False
            
        if self.trial.should_prune():
            return True
        return False


class OptunaTrialRunner:
    """Manages the execution of a single Optuna trial."""
    def __init__(self, base_config_path: Path):
        self.base_config_path = base_config_path
        self.base_config = load_json_config(base_config_path)
        self.device = setup_device()
        self.logger = logging.getLogger(__name__)
        self._pipelines = {}
        
        # Preprocess data for all possible modes upfront
        self._prepare_all_modes()

    def _prepare_all_modes(self):
        """Ensure data is preprocessed for all possible prediction modes."""
        mode = self.base_config["prediction"]["mode"]
        self.logger.info(f"Preparing data for '{mode}' mode...")
        
        pipeline = ChemicalKineticsPipeline(self.base_config)
        pipeline.normalize_only()
        
        self._pipelines[mode] = OptunaPipeline(self.base_config, pipeline.processed_dir)

    def run_trial(self, trial: optuna.Trial) -> float:
        """Configures and runs a single trial."""
        config = suggest_model_config(trial, self.base_config)
        prediction_mode = config["prediction"]["mode"]
        pipeline = self._pipelines[prediction_mode]
        return pipeline.execute_trial(config, trial)


class OptunaPipeline:
    """Holds datasets and executes the training for a specific prediction mode."""
    def __init__(self, config: Dict[str, Any], processed_dir: Path):
        self.config = config
        self.device = setup_device()
        self.logger = logging.getLogger(f"OptunaPipeline_{config['prediction']['mode']}")
        
        self.processed_dir = processed_dir
        self.model_save_root = Path(self.config["paths"]["model_save_dir"])
        
        norm_stats_path = self.processed_dir / "normalization.json"
        if not norm_stats_path.exists():
            raise FileNotFoundError(f"Normalization stats not found in {norm_stats_path}")
        norm_stats = load_json(norm_stats_path)
        
        self.norm_helper = NormalizationHelper(
            stats=norm_stats, device=self.device,
            species_vars=self.config["data"]["species_variables"],
            global_vars=self.config["data"]["global_variables"],
            time_var=self.config["data"]["time_variable"],
            config=self.config
        )
        self._load_datasets()

    def _load_datasets(self):
        """Load split-specific datasets."""
        self.logger.info(f"Loading datasets from: {self.processed_dir}")
        
        self.train_dataset = NPYDataset(self.processed_dir, "train", self.config, self.device)
        self.val_dataset = NPYDataset(self.processed_dir, "validation", self.config, self.device)
        
        self.logger.info(
            f"Datasets loaded: train={len(self.train_dataset)}, "
            f"val={len(self.val_dataset)}"
        )

    def execute_trial(self, config: Dict[str, Any], trial: optuna.Trial) -> float:
        """Runs a single trial's training and evaluation with pruning."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        trial_id = f"trial_{trial.number:04d}_{config['prediction']['mode']}"
        save_dir = self.model_save_root / "optuna" / f"{timestamp}_{trial_id}"
        ensure_directories(save_dir)
        
        try:
            seed_everything(config["system"]["seed"])
            optimize_hardware(config["system"], self.device)
            model = create_model(config, self.device)
            
            # Use dynamic epoch allocation from Hyperband
            n_epochs = trial.user_attrs.get("n_epochs", config["training"]["hpo_max_epochs"])
            config["training"]["epochs"] = n_epochs
            
            # Create pruning callback with minimum epochs
            min_epochs = config["training"]["hpo_min_epochs"]
            pruning_callback = OptunaPruningCallback(trial, min_epochs)
            
            # Log trial configuration
            self.logger.info(f"Trial {trial.number}: {n_epochs} epochs allocated by Hyperband")
            self.logger.info(f"Learning rate: {config['training']['learning_rate']:.2e}")
            
            trainer = PrunableTrainer(
                model=model, train_dataset=self.train_dataset,
                val_dataset=self.val_dataset, test_dataset=None,
                config=config, save_dir=save_dir, device=self.device,
                norm_helper=self.norm_helper, epoch_callback=pruning_callback
            )
            
            best_val_loss = trainer.train()

            trial.set_user_attr("full_config", config)
            trial.set_user_attr("final_lr", trainer.optimizer.param_groups[0]['lr'])
            save_json(config, save_dir / "config.json")
            
            self.logger.info(f"Trial {trial.number} completed. Best loss: {best_val_loss:.6f}, "
                             f"Final LR: {trainer.optimizer.param_groups[0]['lr']:.2e}")
            
            return best_val_loss
            
        except optuna.TrialPruned:
            self.logger.info(f"Trial {trial.number} pruned.")
            raise
        except Exception as e:
            self.logger.error(f"Trial {trial.number} failed: {e}", exc_info=True)
            return float("inf")
        finally:
            if self.device.type == "cuda":
                torch.cuda.empty_cache()


class PrunableTrainer(Trainer):
    """Extended Trainer that supports epoch callbacks for Optuna pruning."""
    def __init__(self, *args, epoch_callback: Optional[Callable[[int, float], bool]] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_callback = epoch_callback
        
    def _run_training_loop(self):
        """Main training loop with pruning support."""
        best_train_loss = float("inf")
        
        for epoch in range(1, self.train_config["epochs"] + 1):
            self.current_epoch = epoch
            epoch_start = time.time()
            train_loss, train_metrics = self._train_epoch()
            val_loss, val_metrics = self._validate()

            if self.scheduler and not self.scheduler_step_on_batch:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) and self.has_validation:
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            epoch_time = time.time() - epoch_start
            self.total_training_time += epoch_time
            self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)

            if self.epoch_callback:
                loss_for_pruning = val_loss if self.has_validation and val_loss != float("inf") else train_loss
                
                if self.epoch_callback(epoch, loss_for_pruning):
                    self.logger.info(f"Trial pruned at epoch {epoch} with loss {loss_for_pruning:.6f}")
                    raise optuna.TrialPruned()

            if self.has_validation:
                if val_loss < (self.best_val_loss - self.min_delta):
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1

                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            else:
                if train_loss < (best_train_loss - self.min_delta):
                    best_train_loss = train_loss
                    self.best_val_loss = train_loss
                    self.best_epoch = epoch
                    self._save_best_model()


def suggest_model_config(trial: optuna.Trial, base_config: Dict[str, Any]) -> Dict[str, Any]:
    """Suggests a valid model and training configuration for a trial."""
    config = copy.deepcopy(base_config)

    config["prediction"]["mode"] = "absolute"
    
    # Model architecture choice
    model_type = trial.suggest_categorical("model_type", ["deeponet", "siren"])
    config["model"]["type"] = model_type
    
    # Common hyperparameters - EXPANDED SEARCH SPACE
    config["model"]["activation"] = trial.suggest_categorical("activation", ["gelu", "silu", "relu", "tanh"])
    #config["training"]["learning_rate"] = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    #config["training"]["batch_size"] = trial.suggest_categorical("batch_size", [16384])
    #config["training"]["weight_decay"] = trial.suggest_float("weight_decay", 1e-6, 1e-3, log=True)
    config["model"]["dropout"] = trial.suggest_float("dropout", 0.0, 0.1, step=0.05)
    

    if model_type == "deeponet":
        # More flexible architecture search
        n_branch = trial.suggest_int("n_branch_layers", 2, 6)
        branch_layers = []
        for i in range(n_branch):
            if i == 0:
                width = trial.suggest_categorical(f"branch_layer_{i}", [256, 384, 512, 1024])
            else:
                # Allow different widths per layer
                width = trial.suggest_categorical(f"branch_layer_{i}", [128, 256, 384, 512, 1024])
            branch_layers.append(width)
        config["model"]["branch_layers"] = branch_layers
        
        n_trunk = trial.suggest_int("n_trunk_layers", 2, 5)
        trunk_layers = []
        for i in range(n_trunk):
            width = trial.suggest_categorical(f"trunk_layer_{i}", [64, 128, 192])
            trunk_layers.append(width)
        config["model"]["trunk_layers"] = trunk_layers
        
        config["model"]["basis_dim"] = trial.suggest_categorical("basis_dim", [64, 128, 256])
        config["model"]["output_scale"] = trial.suggest_categorical("output_scale", [0.1, 1.0, 10.0])
        
    # Siren
    else: 
        n_layers = trial.suggest_int("n_hidden_layers", 3, 6)
        hidden_dims = []
        for i in range(n_layers):
            if i == 0:
                width = trial.suggest_categorical(f"hidden_dim_{i}", [256, 384, 512, 1024, 2048])
            else:
                width = trial.suggest_categorical(f"hidden_dim_{i}", [128, 256, 384, 1024, 2048])
            hidden_dims.append(width)
        config["model"]["hidden_dims"] = hidden_dims
        config["model"]["omega_0"] = trial.suggest_float("omega_0", 10.0, 50.0)

    # FiLM configuration
    use_film = trial.suggest_categorical("use_film", [True, False])
    config["film"]["enabled"] = use_film
    if use_film:
        film_layers = trial.suggest_int("film_layers", 1, 5)
        film_widths = []
        for i in range(film_layers):
            width = trial.suggest_categorical(f"film_width_{i}", [32, 64, 128])
            film_widths.append(width)
        config["film"]["hidden_dims"] = film_widths
        config["film"]["activation"] = trial.suggest_categorical("film_activation", ["gelu", "relu"])

    # Loss function
    config["training"]["loss"] = trial.suggest_categorical("loss", ["mse"])
    if config["training"]["loss"] == "huber":
        config["training"]["huber_delta"] = trial.suggest_float("huber_delta", 0.1, 2.0)

    # Gradient clipping
    config["training"]["gradient_clip"] = trial.suggest_categorical("gradient_clip", [1.0])

    return config


def optimize(config_path: Path, n_trials: int = 25, n_jobs: int = 1,
             study_name: str = "chemulator_hpo", pruner: Optional[optuna.pruners.BasePruner] = None):
    """
    Main function to run Optuna optimization with Hyperband for 40-hoaur runtime.
    """
    logger = logging.getLogger(__name__)
    base_config = load_json_config(config_path)
    
    # Create trial runner which will prepare data
    trial_runner = OptunaTrialRunner(config_path)
    objective = trial_runner.run_trial

    # Use Hyperband pruner for efficient resource allocation
    if pruner is None:
        min_resource = base_config["training"]["hpo_min_epochs"]
        max_resource = base_config["training"]["hpo_max_epochs"]
        
        pruner = HyperbandPruner(
            min_resource=min_resource,
            max_resource=max_resource,
            reduction_factor=3
        )
        
        logger.info(f"Using Hyperband pruner: min={min_resource} epochs, max={max_resource} epochs")

    study = optuna.create_study(
        direction="minimize",
        sampler=TPESampler(seed=42, n_startup_trials=5),
        pruner=pruner,
        study_name=study_name,
        storage=f"sqlite:///{study_name}.db",
        load_if_exists=True
    )

    # Log expected runtime
    logger.info(f"Starting optimization with target {n_trials} trials")
    logger.info(f"Expected runtime: ~40 hours with aggressive pruning")

    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)

    # Save Results
    results_dir = Path("optuna_results")
    ensure_directories(results_dir)
    timestamp = time.strftime("%Y%m%d_%H%M%S")

    best_config = study.best_trial.user_attrs.get("full_config", {})
    if not best_config:
        logger.warning("Could not retrieve full config from user_attrs.")

    # Compute statistics
    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
    
    epoch_distribution = {}
    for t in completed_trials + pruned_trials:
        n_epochs = t.user_attrs.get("n_epochs", "unknown")
        epoch_distribution[n_epochs] = epoch_distribution.get(n_epochs, 0) + 1

    best_results = {
        "best_value": study.best_value,
        "best_params": study.best_trial.params,
        "best_config": best_config,
        "n_trials_completed": len(completed_trials),
        "n_trials_pruned": len(pruned_trials),
        "epoch_distribution": epoch_distribution,
        "best_trial_final_lr": study.best_trial.user_attrs.get("final_lr", "unknown"),
        "study_db": f"{study_name}.db"
    }
    
    save_json(best_results, results_dir / f"best_config_{study_name}_{timestamp}.json")
    
    print("\n" + "="*60)
    print("OPTIMIZATION COMPLETE")
    print("="*60)
    print(f"Best validation loss: {best_results['best_value']:.6f}")
    print(f"Best trial final LR: {best_results['best_trial_final_lr']}")
    print(f"Trials: {best_results['n_trials_completed']} completed, {best_results['n_trials_pruned']} pruned")
    print("\nEpoch distribution:")
    for epochs, count in sorted(epoch_distribution.items()):
        print(f"  {epochs} epochs: {count} trials")
    print("\nBest parameters:")
    for key, value in best_results['best_params'].items():
        print(f"  {key}: {value}")
    
    return study

===== /Users/imalsky/Desktop/Chemulator/src/training/trainer.py =====
#!/usr/bin/env python3
"""
Optimized training pipeline for chemical kinetics models on A100 GPU.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
from contextlib import nullcontext
import math

import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau

from models.model import export_model
from data.normalizer import NormalizationHelper


class Trainer:
    """Optimized trainer for A100 GPU."""
    def __init__(self, model: nn.Module, train_dataset, val_dataset, test_dataset,
                config: Dict[str, Any], save_dir: Path, device: torch.device,
                norm_helper: NormalizationHelper):
        self.logger = logging.getLogger(__name__)
        
        self.model = model
        self.config = config
        self.save_dir = save_dir
        self.device = device
        self.norm_helper = norm_helper
        
        # Extract config sections
        self.train_config = config["training"]
        self.system_config = config["system"]
        self.prediction_config = config.get("prediction", {})
        
        # Prediction mode
        self.prediction_mode = self.prediction_config.get("mode", "absolute")
        self.output_clamp = self.prediction_config.get("output_clamp")
        
        self._validate_trainer_config()
        
        # Dataset info
        self.n_species = len(config["data"]["species_variables"])
        self.n_globals = len(config["data"]["global_variables"])
        
        # Check for validation data
        self.has_validation = val_dataset is not None and len(val_dataset) > 0
        if not self.has_validation:
            self.logger.warning("No validation data â€“ using training loss for checkpointing")
        
        # Create data loaders
        self._setup_dataloaders(train_dataset, val_dataset, test_dataset)

        # Training state
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_loss = float("inf")
        self.best_epoch = -1
        self.total_training_time = 0
        self.patience_counter = 0
        
        # Training parameters
        self.log_interval = self.train_config.get("log_interval", 100)
        self.early_stopping_patience = self.train_config["early_stopping_patience"]
        self.min_delta = self.train_config["min_delta"]
        self.gradient_accumulation_steps = self.train_config["gradient_accumulation_steps"]
        
        # Setup training components
        self._setup_optimizer()
        self._setup_scheduler()
        self._setup_loss()
        self._setup_amp()
        
        # Training history
        self.training_history = {
            "config": config,
            "prediction_mode": self.prediction_mode,
            "epochs": []
        }


    def _validate_trainer_config(self):
        """Validate trainer configuration for correctness."""
        # Fix 2: Validate ratio mode requirements
        if self.prediction_mode == "ratio":
            # Check model compatibility
            model_type = self.config["model"]["type"]
            if model_type != "deeponet":
                raise ValueError(
                    f"Training in 'ratio' mode requires 'deeponet' model, "
                    f"but '{model_type}' was specified. Please use 'deeponet' or switch to 'absolute' mode."
                )
            
            # Fix 3: Check for ratio statistics
            if not hasattr(self.norm_helper, 'ratio_stats') or self.norm_helper.ratio_stats is None:
                raise ValueError(
                    "Training in 'ratio' mode requires ratio statistics from preprocessing. "
                    "Ensure data was preprocessed with prediction.mode='ratio' in config. "
                    "Current normalization data does not contain ratio_stats."
                )
            
            self.logger.info("Ratio mode validation passed: using DeepONet with ratio statistics")
        
        # Validate output clamping configuration
        if self.output_clamp is not None:
            if isinstance(self.output_clamp, (list, tuple)):
                if len(self.output_clamp) != 2:
                    raise ValueError("output_clamp must be None, a single value (min), or a tuple/list of (min, max)")
            elif not isinstance(self.output_clamp, (int, float)):
                raise ValueError("output_clamp must be None, a number, or a tuple/list of two numbers")

    def _setup_dataloaders(self, train_dataset, val_dataset, test_dataset):
        """Setup data loaders for GPU-cached data."""
        from data.dataset import create_dataloader
        
        self.train_loader = create_dataloader(
            train_dataset,
            self.config,
            shuffle=True,
            device=self.device,
            drop_last=True
        ) if train_dataset else None
        
        self.val_loader = create_dataloader(
            val_dataset,
            self.config,
            shuffle=False,
            device=self.device,
            drop_last=False
        ) if val_dataset and len(val_dataset) > 0 else None
        
        self.test_loader = create_dataloader(
            test_dataset,
            self.config,
            shuffle=False,
            device=self.device,
            drop_last=False
        ) if test_dataset and len(test_dataset) > 0 else None
    
    def _setup_optimizer(self):
        """Setup AdamW optimizer with safe feature detection."""
        # Separate parameters for weight decay
        decay_params = []
        no_decay_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            
            if param.dim() == 1 or "bias" in name or "norm" in name.lower():
                no_decay_params.append(param)
            else:
                decay_params.append(param)
        
        param_groups = [
            {"params": decay_params, "weight_decay": self.train_config["weight_decay"]},
            {"params": no_decay_params, "weight_decay": 0.0}
        ]
        
        # Safely check for fused optimizer support
        optimizer_kwargs = {
            "lr": self.train_config["learning_rate"],
            "betas": tuple(self.train_config.get("betas", [0.9, 0.999])),
            "eps": self.train_config.get("eps", 1e-8),
        }
        
        # Only use fused if available and on CUDA
        if self.device.type == "cuda" and hasattr(torch.optim.AdamW, "fused"):
            try:
                # Test if fused parameter actually works
                test_opt = torch.optim.AdamW([torch.zeros(1)], fused=True)
                optimizer_kwargs["fused"] = True
                self.logger.info("Using fused AdamW optimizer")
            except Exception:
                self.logger.info("Fused AdamW not available, using standard implementation")
        
        self.optimizer = AdamW(param_groups, **optimizer_kwargs)
    
    def _setup_scheduler(self):
        """Setup learning rate scheduler."""
        scheduler_type = self.train_config.get("scheduler", "none").lower()

        if scheduler_type == "none" or not self.train_loader:
            self.scheduler = None
            self.scheduler_step_on_batch = False
            return

        steps_per_epoch = len(self.train_loader) // self.gradient_accumulation_steps
        
        # **FIXED**: Guard against division-by-zero or T_0=0 errors on small datasets.
        if steps_per_epoch == 0:
            self.logger.warning(
                f"Number of batches ({len(self.train_loader)}) is smaller than "
                f"gradient_accumulation_steps ({self.gradient_accumulation_steps}). "
                f"Scheduler will step once per epoch."
            )
            steps_per_epoch = 1

        params = self.train_config.get("scheduler_params", {})

        if scheduler_type == "cosine":
            T_0_epochs = params.get("T_0", 10)
            T_0_steps = T_0_epochs * steps_per_epoch

            self.scheduler = CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=T_0_steps,
                T_mult=params.get("T_mult", 2),
                eta_min=params.get("eta_min", 1e-8),
            )
            self.scheduler_step_on_batch = True

        elif scheduler_type == "plateau":
            if not self.has_validation:
                self.logger.warning("Plateau scheduler requires validation data")
                self.scheduler = None
                self.scheduler_step_on_batch = False
                return
                
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                factor=params.get("factor", 0.5),
                patience=params.get("patience", 10),
                min_lr=params.get("min_lr", 1e-7),
            )
            self.scheduler_step_on_batch = False
        else:
            raise ValueError(f"Unknown scheduler '{scheduler_type}'")
    
    def _setup_loss(self):
        """Setup loss function."""
        loss_type = self.train_config["loss"]
        
        if loss_type == "mse":
            self.criterion = nn.MSELoss()
        elif loss_type == "mae":
            self.criterion = nn.L1Loss()
        elif loss_type == "huber":
            self.criterion = nn.HuberLoss(delta=self.train_config.get("huber_delta", 0.5))
        else:
            raise ValueError(f"Unknown loss: {loss_type}")
    
    def _setup_amp(self):
        """Setup automatic mixed precision for A100."""
        self.use_amp = self.train_config.get("use_amp", True)
        
        # Get dtype
        dtype_str = str(self.train_config.get("amp_dtype", "bfloat16")).lower()
        self.amp_dtype = torch.bfloat16 if dtype_str == "bfloat16" else torch.float16
        
        # GradScaler only for float16
        self.scaler = GradScaler(enabled=(self.amp_dtype == torch.float16))
    
    def _compute_loss(self, outputs: torch.Tensor, 
                    targets: torch.Tensor,
                    inputs: torch.Tensor) -> torch.Tensor:
        """
        Compute loss with proper output clamping.
        
        output_clamp can be:
        - None: no clamping
        - Single value: clamp minimum only (backward compatibility)
        - Tuple/list of (min, max): clamp both sides
        """
        if self.output_clamp is not None and self.prediction_mode == "absolute":
            if isinstance(self.output_clamp, (list, tuple)):
                # Two-sided clamp
                outputs = torch.clamp(outputs, min=self.output_clamp[0], max=self.output_clamp[1])
            else:
                # Single value - clamp minimum only (backward compatibility)
                outputs = torch.clamp(outputs, min=self.output_clamp)
                self.logger.warning(
                    "Using single-sided output clamping (min only). "
                    "Consider using (min, max) tuple for two-sided clamping."
                )
        
        return self.criterion(outputs, targets)



    def train(self) -> float:
        """Execute the training loop."""
        if not self.train_loader:
            self.logger.error("Training loader not available")
            return float("inf")

        self.logger.info(f"Starting training...")
        self.logger.info(f"Train batches: {len(self.train_loader)}")
        if self.has_validation:
            self.logger.info(f"Val batches: {len(self.val_loader)}")


        if self.system_config.get("use_torch_compile", False):
            self.logger.info("Compiling model with torch.compile...")
            self.logger.warning("    This is a one-time process that can take several minutes.")


        try:
            self._run_training_loop()
            
            self.logger.info(
                f"Training completed. Best validation loss: {self.best_val_loss:.6f} "
                f"at epoch {self.best_epoch}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("Training interrupted by user")
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}", exc_info=True)
            raise
            
        finally:
            # Save training history
            save_path = self.save_dir / "training_log.json"
            with open(save_path, 'w') as f:
                json.dump(self.training_history, f, indent=2)
        
        return self.best_val_loss
    
    def _run_training_loop(self):
        """Main training loop - optimized for GPU."""
        best_train_loss = float("inf")
        
        for epoch in range(1, self.train_config["epochs"] + 1):
            self.current_epoch = epoch
            epoch_start = time.time()

            # Train
            train_loss, train_metrics = self._train_epoch()
            
            # Validate
            val_loss, val_metrics = self._validate()

            # Update scheduler
            if self.scheduler and not self.scheduler_step_on_batch:
                if isinstance(self.scheduler, ReduceLROnPlateau) and self.has_validation:
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            # Log epoch
            epoch_time = time.time() - epoch_start
            self.total_training_time += epoch_time
            self._log_epoch(train_loss, val_loss, train_metrics, val_metrics, epoch_time)

            # Save best model
            if self.has_validation:
                if val_loss < (self.best_val_loss - self.min_delta):
                    self.best_val_loss = val_loss
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    self._save_best_model()
                else:
                    self.patience_counter += 1

                if self.patience_counter >= self.early_stopping_patience:
                    self.logger.info(f"Early stopping triggered at epoch {epoch}")
                    break
            else:
                # Use training loss if no validation
                if train_loss < (best_train_loss - self.min_delta):
                    best_train_loss = train_loss
                    self.best_val_loss = train_loss
                    self.best_epoch = epoch
                    self._save_best_model()


    def _train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """Optimized training epoch for GPU."""
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        
        accumulation_steps = self.gradient_accumulation_steps
        
        is_gpu_cached = hasattr(self.train_loader.dataset, 'gpu_cache') and self.train_loader.dataset.gpu_cache is not None
        
        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            if not is_gpu_cached:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)
                loss = loss / accumulation_steps
            
            if self.scaler.is_enabled():
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(self.train_loader):
                if self.train_config["gradient_clip"] > 0:
                    if self.scaler.is_enabled():
                        self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.train_config["gradient_clip"]
                    )
                
                if self.scaler.is_enabled():
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.optimizer.zero_grad(set_to_none=True)
                
                if self.scheduler and self.scheduler_step_on_batch:
                    self.scheduler.step()
                
                self.global_step += 1
            
            batch_size = inputs.size(0)
            total_loss += loss.item() * accumulation_steps * batch_size
            total_samples += batch_size
        
        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
        return avg_loss, {}

    @torch.inference_mode()
    def _validate(self) -> Tuple[float, Dict[str, float]]:
        """Optimized validation for GPU."""
        if not self.has_validation or self.val_loader is None:
            return float("inf"), {}
        
        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        
        is_gpu_cached = hasattr(self.val_loader.dataset, 'gpu_cache') and self.val_loader.dataset.gpu_cache is not None
        
        for inputs, targets in self.val_loader:
            if not is_gpu_cached:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
            
            # **FIXED**: device_type is now dynamic, based on the detected hardware.
            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)
            
            batch_size = inputs.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size
        
        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        return avg_loss, {}

    @torch.inference_mode()
    def evaluate_test(self) -> float:
        """Evaluate on test set."""
        if not self.test_loader:
            self.logger.warning("No test data available")
            return float("inf")

        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        
        is_gpu_cached = hasattr(self.test_loader.dataset, 'gpu_cache') and self.test_loader.dataset.gpu_cache is not None

        for inputs, targets in self.test_loader:
            if not is_gpu_cached:
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

            # **FIXED**: device_type is now dynamic, based on the detected hardware.
            with autocast(device_type=self.device.type, enabled=self.use_amp, dtype=self.amp_dtype):
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets, inputs)

            batch_size = inputs.size(0)
            total_loss += loss.item() * batch_size
            total_samples += batch_size

        avg_loss = total_loss / total_samples if total_samples > 0 else float("inf")
        self.logger.info(f"Test loss: {avg_loss:.6f}")
        return avg_loss

    def _log_epoch(self, train_loss, val_loss, train_metrics, val_metrics, epoch_time):
        """Log epoch results."""
        lr = self.optimizer.param_groups[0]['lr']
        log_entry = {
            "epoch": self.current_epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "epoch_time": epoch_time,
            "lr": lr,
        }
        self.training_history["epochs"].append(log_entry)
        
        val_str = f"Val loss: {val_loss:.3e}" if self.has_validation else "Val loss: N/A"
        self.logger.info(
            f"Epoch {self.current_epoch}/{self.train_config['epochs']} "
            f"Train loss: {train_loss:.3e} {val_str} "
            f"Time: {epoch_time:.1f}s LR: {lr:.2e}"
        )
    
    def _save_best_model(self):
        """Save the best model checkpoint."""
        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict() if self.scheduler else None,
            "best_val_loss": self.best_val_loss,
            "config": self.config
        }
        
        checkpoint_path = self.save_dir / "best_model.pt"
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"Saved best model checkpoint to {checkpoint_path}")

        # Export model if enabled
        if self.system_config.get("use_torch_export", False):
            example_loader = self.val_loader or self.train_loader
            if example_loader:
                example_inputs, _ = next(iter(example_loader))
                self.logger.info(f"Exporting model with example input shape: {example_inputs.shape}")
                export_path = self.save_dir / "exported_model.pt"
                export_model(self.model, example_inputs, export_path)


def diagnose_performance(trainer, num_batches=20):
    """Diagnose training performance bottlenecks."""
    import torch.profiler
    
    logger = logging.getLogger(__name__)
    logger.info("Running performance diagnostics...")
    
    trainer.model.train()
    times = {
        'forward': [],
        'backward': [],
        'optimizer': [],
        'total': []
    }
    
    # Warmup
    for i, (inputs, targets) in enumerate(trainer.train_loader):
        if i >= 3:
            break
        outputs = trainer.model(inputs)
        loss = trainer._compute_loss(outputs, targets, inputs)
        loss.backward()
        trainer.optimizer.step()
        trainer.optimizer.zero_grad()
    
    torch.cuda.synchronize()
    
    # Actual timing
    for i, (inputs, targets) in enumerate(trainer.train_loader):
        if i >= num_batches:
            break
        
        torch.cuda.synchronize()
        batch_start = torch.cuda.Event(enable_timing=True)
        forward_end = torch.cuda.Event(enable_timing=True)
        backward_end = torch.cuda.Event(enable_timing=True)
        optimizer_end = torch.cuda.Event(enable_timing=True)
        
        batch_start.record()
        
        # Forward
        outputs = trainer.model(inputs)
        loss = trainer._compute_loss(outputs, targets, inputs)
        forward_end.record()
        
        # Backward
        loss.backward()
        backward_end.record()
        
        # Optimizer
        trainer.optimizer.step()
        trainer.optimizer.zero_grad()
        optimizer_end.record()
        
        torch.cuda.synchronize()
        
        # Record times
        times['forward'].append(batch_start.elapsed_time(forward_end))
        times['backward'].append(forward_end.elapsed_time(backward_end))
        times['optimizer'].append(backward_end.elapsed_time(optimizer_end))
        times['total'].append(batch_start.elapsed_time(optimizer_end))
    
    # Report results
    for key, values in times.items():
        avg_time = sum(values) / len(values)
        logger.info(f"{key}: {avg_time:.2f} ms/batch")
    
    # Calculate throughput
    batch_size = trainer.train_config['batch_size']
    avg_total = sum(times['total']) / len(times['total']) / 1000  # Convert to seconds
    throughput = batch_size / avg_total
    logger.info(f"Throughput: {throughput:.0f} samples/second")
    
    return times

===== /Users/imalsky/Desktop/Chemulator/src/utils/utils.py =====
#!/usr/bin/env python3
"""
Simplified utility functions for the chemical kinetics pipeline.
"""

import json
import logging
import os
import random
import sys
from pathlib import Path
from typing import Any, Dict, Union

import numpy as np
import torch


def setup_logging(level: int = logging.INFO, log_file: Path = None) -> None:
    """Configure logging for the application."""
    format_string = "%(asctime)s | %(levelname)-8s | %(name)s - %(message)s"
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    # Clear existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create formatter
    formatter = logging.Formatter(format_string, datefmt="%Y-%m-%d %H:%M:%S")
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler
    if log_file is not None:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)


def seed_everything(seed: int) -> None:
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    os.environ["PYTHONHASHSEED"] = str(seed)
    
    logger = logging.getLogger(__name__)
    logger.info(f"Random seed set to {seed}")


def load_json_config(path: Union[str, Path]) -> Dict[str, Any]:
    """Load JSON configuration file."""
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"Configuration file not found: {path}")
    
    # Try json5 first for comment support
    try:
        import json5
        with open(path, 'r', encoding='utf-8') as f:
            config = json5.load(f)
    except ImportError:
        # Fallback to standard json
        with open(path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    return config


def save_json(data: Dict[str, Any], path: Union[str, Path], indent: int = 2) -> None:
    """Save dictionary to JSON file."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Custom encoder for numpy/torch types
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.Tensor):
                return obj.cpu().numpy().tolist()
            elif isinstance(obj, Path):
                return str(obj)
            return super().default(obj)
    
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, cls=NumpyEncoder)


def load_json(path: Union[str, Path]) -> Dict[str, Any]:
    """Load JSON file."""
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def ensure_directories(*paths: Union[str, Path]) -> None:
    """Create directories if they don't exist."""
    for path in paths:
        Path(path).mkdir(parents=True, exist_ok=True)

===== /Users/imalsky/Desktop/Chemulator/src/utils/hardware.py =====
#!/usr/bin/env python3
"""
Hardware detection and optimization utilities.
"""

import logging
import os
from typing import Dict, Any

import torch


def setup_device() -> torch.device:
    """Detect and configure the best available compute device."""
    logger = logging.getLogger(__name__)
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"Using CUDA device: {gpu_name} ({gpu_memory:.1f} GB)")
        
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("Using Apple Silicon MPS device")
        
    else:
        device = torch.device("cpu")
        logger.info(f"Using CPU device ({os.cpu_count()} cores)")
    
    return device


def optimize_hardware(config: Dict[str, Any], device: torch.device) -> None:
    """Apply hardware-specific optimizations with safe feature detection."""
    logger = logging.getLogger(__name__)
    
    # CUDA optimizations
    if device.type == "cuda":
        # Enable TensorFloat-32 for faster matmul
        if config.get("tf32", True):
            if hasattr(torch.backends.cuda, "matmul"):
                torch.backends.cuda.matmul.allow_tf32 = True
                logger.info("TensorFloat-32 enabled for matmul")
            if hasattr(torch.backends.cudnn, "allow_tf32"):
                torch.backends.cudnn.allow_tf32 = True
                logger.info("TensorFloat-32 enabled for cuDNN")
        
        # Enable cuDNN autotuner
        if config.get("cudnn_benchmark", True) and hasattr(torch.backends.cudnn, "benchmark"):
            torch.backends.cudnn.benchmark = True
            logger.info("cuDNN autotuner enabled")
        
        # Set memory fraction - safely check if API exists
        memory_fraction = config.get("cuda_memory_fraction", 0.9)
        if memory_fraction < 1.0 and hasattr(torch.cuda, "set_per_process_memory_fraction"):
            try:
                torch.cuda.set_per_process_memory_fraction(memory_fraction)
                logger.info(f"CUDA memory fraction set to {memory_fraction}")
            except Exception as e:
                logger.warning(f"Could not set CUDA memory fraction: {e}")
    
    # Set number of threads for CPU operations
    torch.set_num_threads(min(32, os.cpu_count() or 1))
    logger.info(f"Using {torch.get_num_threads()} CPU threads")

===== /Users/imalsky/Desktop/Chemulator/src/models/model.py =====
#!/usr/bin/env python3
"""
Optimized model definitions for chemical kinetics neural networks.
Includes better compilation and A100-specific optimizations.
"""

import logging
import math
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

import torch
import torch.nn as nn
from torch.export import Dim


class FiLMLayer(nn.Module):
    """Feature-wise Linear Modulation layer - optimized version."""
    
    def __init__(self, condition_dim: int, feature_dim: int, 
                 hidden_dims: List[int], activation: Union[str, List[str]] = "gelu", 
                 use_beta: bool = True):
        super().__init__()
        
        self.use_beta = use_beta
        self.feature_dim = feature_dim
        out_multiplier = 2 if use_beta else 1
        
        # Handle activation specification
        if isinstance(activation, str):
            activations = [activation] * len(hidden_dims)
        else:
            if len(activation) != len(hidden_dims):
                raise ValueError(f"Number of activations ({len(activation)}) must match hidden_dims ({len(hidden_dims)})")
            activations = activation
        
        # Build FiLM MLP
        layers = []
        prev_dim = condition_dim
        
        # Hidden layers with per-layer activation
        for dim, act in zip(hidden_dims, activations):
            layers.extend([
                nn.Linear(prev_dim, dim),
                self._get_activation(act)
            ])
            prev_dim = dim
        
        # Output layer
        layers.append(nn.Linear(prev_dim, out_multiplier * feature_dim))
        
        self.film_net = nn.Sequential(*layers)
        
        # Initialize to identity mapping
        self._initialize_identity()
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(inplace=True),  # Inplace for memory efficiency
            "tanh": nn.Tanh(),
            "silu": nn.SiLU(inplace=True),
            "leakyrelu": nn.LeakyReLU(0.2, inplace=True),
            "elu": nn.ELU(inplace=True)
        }
        if name.lower() not in activations:
            raise ValueError(f"Unknown activation: {name}")
        return activations[name.lower()]
    
    def _initialize_identity(self):
        """Initialize FiLM to identity mapping."""
        with torch.no_grad():
            final_layer = self.film_net[-1]
            # Small weights
            final_layer.weight.data.normal_(0, 0.02)
            # Set bias for gamma=1, beta=0
            if self.use_beta:
                final_layer.bias.data[:self.feature_dim] = 1.0
                final_layer.bias.data[self.feature_dim:] = 0.0
            else:
                final_layer.bias.data.fill_(1.0)
    
    def forward(self, features: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """Apply FiLM modulation - optimized."""
        # Generate parameters
        params = self.film_net(condition)
        
        if self.use_beta:
            gamma = params[:, :self.feature_dim]
            beta = params[:, self.feature_dim:]
        else:
            gamma = params
            beta = 0
        
        # Handle different feature dimensions efficiently
        if features.dim() == 2:
            # Simple 2D case
            return features * gamma + beta
        else:
            # Reshape for broadcasting
            shape = [gamma.size(0)] + [1] * (features.dim() - 2) + [self.feature_dim]
            gamma = gamma.view(*shape)
            if self.use_beta:
                beta = beta.view(*shape)
            return features * gamma + beta


class FiLMSIREN(nn.Module):
    """SIREN with FiLM conditioning - optimized for A100."""
    def __init__(self, config: Dict[str, Any]):
        super().__init__()

        # Extract dimensions
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])
        self.hidden_dims = config["model"]["hidden_dims"]

        # SIREN parameters
        self.omega_0 = config["model"].get("omega_0", 30.0)
        
        # Dropout
        dropout_rate = config["model"].get("dropout", 0.0)
        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None

        # FiLM configuration
        film_config = config.get("film", {})
        self.use_film = film_config.get("enabled", True)

        # Input dimension
        input_dim = self.num_species + self.num_globals + 1

        # Build network layers
        self.layers = nn.ModuleList()
        self.film_layers = nn.ModuleList() if self.use_film else None

        prev_dim = input_dim
        condition_dim = self.num_species + self.num_globals

        # Get FiLM activations
        film_activations = film_config.get("activations", film_config.get("activation", "gelu"))
        if isinstance(film_activations, str):
            film_activations = [film_activations] * len(self.hidden_dims)

        for i, dim in enumerate(self.hidden_dims):
            # Main layer
            self.layers.append(nn.Linear(prev_dim, dim))

            # FiLM layer
            if self.use_film:
                layer_activation = film_activations[i] if isinstance(film_activations, list) else film_activations
                self.film_layers.append(
                    FiLMLayer(
                        condition_dim=condition_dim,
                        feature_dim=dim,
                        hidden_dims=film_config.get("hidden_dims", [128, 128]),
                        activation=layer_activation,
                        use_beta=True
                    )
                )

            prev_dim = dim

        # Output layer
        self.output_layer = nn.Linear(prev_dim, self.num_species)

        # Initialize SIREN weights
        self._initialize_siren_weights()
    
    def _initialize_siren_weights(self):
        """Initialize weights following SIREN paper."""
        with torch.no_grad():
            # First layer
            if len(self.layers) > 0:
                fan_in = self.layers[0].in_features
                self.layers[0].weight.uniform_(-1.0 / fan_in, 1.0 / fan_in)
            
            # Hidden layers
            for layer in self.layers[1:]:
                fan_in = layer.in_features
                bound = math.sqrt(6.0 / fan_in) / self.omega_0
                layer.weight.uniform_(-bound, bound)
            
            # Output layer
            fan_in = self.output_layer.in_features
            bound = math.sqrt(6.0 / fan_in) / self.omega_0
            self.output_layer.weight.uniform_(-bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass - optimized."""
        # Extract initial conditions for FiLM
        initial_conditions = x[:, :-1]  # All but time
        
        # Process through layers
        h = x
        for i, layer in enumerate(self.layers):
            # Linear transformation
            h = layer(h)
            
            # Apply FiLM before activation
            if self.use_film and self.film_layers is not None:
                h = self.film_layers[i](h, initial_conditions)
            
            # SIREN activation
            h = torch.sin(self.omega_0 * h)
            
            # Dropout (except last layer)
            if self.dropout is not None and i < len(self.layers) - 1:
                h = self.dropout(h)
        
        # Output
        return self.output_layer(h)


class FiLMDeepONet(nn.Module):
    """Deep Operator Network with FiLM conditioning - optimized."""
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        # Extract dimensions
        self.num_species = len(config["data"]["species_variables"])
        self.num_globals = len(config["data"]["global_variables"])

        # Architecture parameters
        branch_layers = config["model"]["branch_layers"]
        trunk_layers = config["model"]["trunk_layers"]
        self.basis_dim = config["model"]["basis_dim"]
        self.activation = self._get_activation(config["model"].get("activation", "gelu"))
        self.output_scale = config["model"].get("output_scale", 1.0)
        
        # Dropout
        dropout_rate = config["model"].get("dropout", 0.0)
        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None
        
        # FiLM configuration
        film_config = config.get("film", {})
        self.use_film = film_config.get("enabled", True)
        
        # Build networks
        self.branch_net = self._build_mlp_with_film(
            input_dim=self.num_species + self.num_globals,
            hidden_layers=branch_layers,
            output_dim=self.basis_dim * self.num_species,
            condition_dim=self.num_species + self.num_globals if self.use_film else None,
            film_config=film_config if self.use_film else None
        )
        
        self.trunk_net = self._build_mlp_with_film(
            input_dim=1,
            hidden_layers=trunk_layers,
            output_dim=self.basis_dim,
            condition_dim=self.num_species + self.num_globals if self.use_film else None,
            film_config=film_config if self.use_film else None,
            bias=True
        )
    
    def _get_activation(self, name: str):
        activations = {
            "gelu": nn.GELU(),
            "relu": nn.ReLU(inplace=True),
            "tanh": nn.Tanh(),
            "silu": nn.SiLU(inplace=True),
            "leakyrelu": nn.LeakyReLU(0.2, inplace=True),
            "elu": nn.ELU(inplace=True)
        }
        return activations.get(name.lower(), nn.GELU())
    
    def _build_mlp_with_film(self, input_dim: int, hidden_layers: List[int],
                            output_dim: int, condition_dim: Optional[int] = None,
                            film_config: Optional[Dict] = None, bias: bool = True) -> nn.Module:
        """Build MLP with optional FiLM - optimized."""
        if self.use_film and condition_dim is not None and film_config is not None:
            # Build with FiLM
            layers = nn.ModuleList()
            film_layers = nn.ModuleList()

            # Get FiLM activations
            film_activations = film_config.get("activations", film_config.get("activation", "gelu"))
            if isinstance(film_activations, str):
                film_activations = [film_activations] * len(hidden_layers)

            prev_dim = input_dim
            for i, dim in enumerate(hidden_layers):
                layers.append(nn.Linear(prev_dim, dim, bias=bias))

                layer_activation = film_activations[i] if isinstance(film_activations, list) else film_activations
                film_layers.append(
                    FiLMLayer(
                        condition_dim=condition_dim,
                        feature_dim=dim,
                        hidden_dims=film_config.get("hidden_dims", [128, 128]),
                        activation=layer_activation,
                        use_beta=True
                    )
                )
                prev_dim = dim

            output_layer = nn.Linear(prev_dim, output_dim, bias=bias)

            class MLPWithFiLM(nn.Module):
                def __init__(self, layers, film_layers, output_layer, activation, dropout):
                    super().__init__()
                    self.layers = layers
                    self.film_layers = film_layers
                    self.output_layer = output_layer
                    self.activation = activation
                    self.dropout = dropout

                def forward(self, x, condition):
                    h = x
                    for i, (layer, film_layer) in enumerate(zip(self.layers, self.film_layers)):
                        h = layer(h)
                        h = film_layer(h, condition)
                        h = self.activation(h)
                        
                        # Dropout (except last layer)
                        if self.dropout is not None and i < len(self.layers) - 1:
                            h = self.dropout(h)
                            
                    return self.output_layer(h)

            return MLPWithFiLM(layers, film_layers, output_layer, self.activation, self.dropout)

        else:
            # Build standard MLP
            layers = []
            prev_dim = input_dim

            for i, dim in enumerate(hidden_layers):
                layers.append(nn.Linear(prev_dim, dim, bias=bias))
                layers.append(self.activation)
                
                # Dropout (except last layer)
                if self.dropout is not None and i < len(hidden_layers) - 1:
                    layers.append(self.dropout)
                    
                prev_dim = dim

            layers.append(nn.Linear(prev_dim, output_dim, bias=bias))
            return nn.Sequential(*layers)
            
    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """Forward pass - optimized."""
        batch_size = inputs.shape[0]

        # Split inputs
        branch_input = inputs[:, :self.num_species + self.num_globals]
        trunk_input = inputs[:, -1:]  # Time

        # Process through networks
        if self.use_film:
            branch_out = self.branch_net(branch_input, branch_input)
            trunk_out = self.trunk_net(trunk_input, branch_input)
        else:
            branch_out = self.branch_net(branch_input)
            trunk_out = self.trunk_net(trunk_input)

        # Reshape and combine efficiently
        branch_out = branch_out.view(batch_size, self.num_species, self.basis_dim)
        
        # Efficient matrix multiplication
        output = torch.bmm(branch_out, trunk_out.unsqueeze(2)).squeeze(2)

        # Optional output scaling
        if self.output_scale != 1.0:
            output = output * self.output_scale

        return output


def create_model(config: Dict[str, Any], device: torch.device) -> nn.Module:
    """Create and compile model with validation."""
    model_type = config["model"]["type"].lower()
    prediction_mode = config.get("prediction", {}).get("mode", "absolute")
    
    # Validate model-mode compatibility
    if prediction_mode == "ratio" and model_type != "deeponet":
        raise ValueError(
            f"Prediction mode 'ratio' is only compatible with model type 'deeponet', "
            f"but '{model_type}' was specified. Either:\n"
            f"  1. Change model.type to 'deeponet' in your config, or\n"
            f"  2. Change prediction.mode to 'absolute'"
        )
    
    if model_type == "siren":
        model = FiLMSIREN(config)
    elif model_type == "deeponet":
        model = FiLMDeepONet(config)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    model = model.to(device)
    
    logger = logging.getLogger(__name__)
    logger.info(f"Created {model_type} model for {prediction_mode} mode")
    
    # Compile model for performance
    if config["system"].get("use_torch_compile", False) and hasattr(torch, 'compile'):
        compile_mode = config["system"].get("compile_mode", "default")
        logger.info(f"Compiling model with mode='{compile_mode}'...")
        
        try:
            # A100-optimized compilation
            compile_options = {
                "mode": compile_mode,
                "fullgraph": False,
                "dynamic": False,
            }
            
            if compile_mode == "max-autotune":
                # Additional options for maximum performance
                compile_options["options"] = {
                    "triton.cudagraphs": True,
                    "triton.max_autotune": True,
                }
            
            model = torch.compile(model, **compile_options)
            logger.info("Model compilation successful")
            
        except Exception as e:
            logger.warning(f"Model compilation failed: {e}. Running in eager mode.")
    
    return model


def export_model(model: nn.Module, example_input: torch.Tensor, save_path: Path):
    """Export model with robust unwrapping and dynamic batch support."""
    logger = logging.getLogger(__name__)
    
    model.eval()
    
    # Safely handle compiled models with multiple fallback attempts
    original_model = model
    if hasattr(model, '_orig_mod'):
        logger.info("Extracting original model from compiled wrapper (_orig_mod)")
        model = model._orig_mod
    elif hasattr(model, '_module'):
        logger.info("Extracting original model from compiled wrapper (_module)")
        model = model._module
    elif hasattr(model, 'module'):
        logger.info("Extracting original model from DataParallel/DistributedDataParallel wrapper")
        model = model.module
    else:
        # Try to detect if it's a compiled model by checking for graph attributes
        if hasattr(model, '_graph') or hasattr(model, '_code'):
            logger.warning(
                "Model appears to be compiled but cannot find unwrapping attribute. "
                "Export may fail or produce suboptimal results."
            )
    
    with torch.no_grad():
        try:
            if hasattr(torch, 'export') and hasattr(torch.export, 'export'):
                # Dynamic batch dimension
                batch_dim = Dim("batch", min=1, max=131072)
                
                # Safely detect parameter name
                import inspect
                try:
                    sig = inspect.signature(model.forward)
                    param_names = [p for p in sig.parameters.keys() if p != 'self']
                    param_name = param_names[0] if param_names else 'x'
                except Exception:
                    # Fallback if signature inspection fails
                    param_name = 'x' if hasattr(model, 'forward') else 'input'
                    logger.warning(f"Could not inspect forward signature, using '{param_name}' as parameter name")
                
                logger.info(f"Detected forward method parameter name: '{param_name}'")
                
                dynamic_shapes = {param_name: {0: batch_dim}}
                
                # Export with error handling
                try:
                    exported_program = torch.export.export(
                        model, 
                        (example_input,),
                        dynamic_shapes=dynamic_shapes
                    )
                    torch.export.save(exported_program, str(save_path))
                    logger.info(f"Model exported with torch.export to {save_path}")
                except Exception as e:
                    logger.warning(f"torch.export failed: {e}. Falling back to torch.jit")
                    raise  # Re-raise to trigger JIT fallback
            else:
                # Direct to JIT if torch.export not available
                raise AttributeError("torch.export not available")
                
        except Exception:
            # Fallback to JIT tracing
            try:
                # Try with the original model if unwrapping failed
                model_to_trace = model
                traced_model = torch.jit.trace(model_to_trace, example_input)
                torch.jit.save(traced_model, str(save_path))
                logger.info(f"Model exported with torch.jit to {save_path}")
                logger.warning("JIT export may not support dynamic batch sizes as well as torch.export")
            except Exception as jit_error:
                # Last resort: try with the original compiled model
                if model is not original_model:
                    logger.warning("Trying JIT export with original (possibly compiled) model")
                    try:
                        traced_model = torch.jit.trace(original_model, example_input)
                        torch.jit.save(traced_model, str(save_path))
                        logger.info(f"Model exported with torch.jit (compiled version) to {save_path}")
                    except Exception as final_error:
                        logger.error(f"All export methods failed. Last error: {final_error}")
                        raise
                else:
                    logger.error(f"JIT export failed: {jit_error}")
                    raise

===== /Users/imalsky/Desktop/Chemulator/src/data/preprocessor.py =====
#!/usr/bin/env python3
"""
Chemical kinetics data preprocessor.
This version uses a highly efficient, parallelized, two-pass process with an
architecture that minimizes inter-process communication (IPC) and memory overhead.
"""

import hashlib
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

import h5py
import numpy as np
import torch
import os

from .normalizer import DataNormalizer, NormalizationHelper
from utils.utils import save_json, load_json

DEFAULT_EPSILON_MIN = 1e-38
DEFAULT_EPSILON_MAX = 1e38

class CorePreprocessor:
    """A lightweight helper class containing only the logic needed within a worker."""
    def __init__(self, config: Dict[str, Any], norm_stats: Optional[Dict[str, Any]] = None):
        self.data_cfg = config["data"]
        self.norm_cfg = config["normalization"]
        self.train_cfg = config["training"]
        self.pred_cfg = config.get("prediction", {})
        self.proc_cfg = config["preprocessing"]

        self.species_vars = self.data_cfg["species_variables"]
        self.global_vars = self.data_cfg["global_variables"]
        self.time_var = self.data_cfg["time_variable"]
        self.var_order = self.species_vars + self.global_vars + [self.time_var]
        
        self.n_species = len(self.species_vars)
        self.n_globals = len(self.global_vars)
        self.n_vars = self.n_species + self.n_globals + 1
        
        self.min_value_threshold = self.proc_cfg.get("min_value_threshold", 1e-30)
        
        self.prediction_mode = self.pred_cfg.get("mode", "absolute")
        self.normalizer = DataNormalizer(config)
        self.norm_stats = norm_stats or {}
        
        # Create index mappings for robust variable ordering
        self._create_index_mappings()
        
        if norm_stats:
            self.norm_helper = NormalizationHelper(
                norm_stats, torch.device("cpu"), self.species_vars,
                self.global_vars, self.time_var, config
            )
    
    def _create_index_mappings(self):
        """Create index mappings for robust variable access"""
        self.var_to_idx = {var: i for i, var in enumerate(self.var_order)}
        self.species_indices = [self.var_to_idx[var] for var in self.species_vars]
        self.global_indices = [self.var_to_idx[var] for var in self.global_vars]
        self.time_idx = self.var_to_idx[self.time_var]

    def _is_profile_valid(self, group: h5py.Group) -> Tuple[bool, str]:
        """
        Checks if a profile is valid according to strict criteria.
        Returns (is_valid, reason_for_failure_or_success).
        """
        # Validate every variable (speciesâ€¯+â€¯globalsâ€¯+â€¯time)
        required_keys = self.species_vars + [self.time_var]
        if not set(required_keys).issubset(group.keys()):
            return False, "missing_keys"

        # Check each dataset for NaNs, Infs, and value thresholds
        for var in required_keys:
            try:
                data = group[var][:]
            except Exception:
                return False, "read_error"

            if not np.all(np.isfinite(data)):
                return False, "non_finite"

            # Drop profile if any value is less than or equal to threshold
            if np.any(data <= self.min_value_threshold):
                return False, "below_threshold"

        return True, "valid"
    
    def process_file_for_stats(
        self, file_path: Path
    ) -> Tuple[Dict[str, Dict], Dict[str, Dict], int, Dict]:
        accumulators = self.normalizer._initialize_accumulators()

        ratio_accumulators: Dict[str, Dict[str, Any]] = {}
        if self.prediction_mode == "ratio":
            for v in self.species_vars:
                raw_method = self.normalizer._get_method(v)
                ratio_method = raw_method[4:] if raw_method.startswith("log-") else raw_method
                ratio_accumulators[v] = {
                    "method": ratio_method,
                    "count": 0,
                    "mean": 0.0,
                    "m2":   0.0,
                    "min":  float("inf"),
                    "max":  float("-inf"),
                }

        valid_sample_count = 0
        report = {
            "total_profiles":   0,
            "profiles_kept":    0,
            "dropped_reasons":  defaultdict(int),
        }

        with h5py.File(file_path, "r") as f:
            for gname in sorted(f.keys()):
                report["total_profiles"] += 1
                grp = f[gname]

                # datasetâ€‘level validation
                is_ok, reason = self._is_profile_valid(grp)
                if not is_ok:
                    report["dropped_reasons"][reason] += 1
                    continue

                # deterministic downâ€‘sampling
                if self.train_cfg["use_fraction"] < 1.0:
                    h = int(hashlib.sha256(gname.encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
                    if h >= self.train_cfg["use_fraction"]:
                        continue

                n_t = grp[self.time_var].shape[0]
                if n_t <= 1:
                    report["dropped_reasons"]["too_few_timesteps"] += 1
                    continue

                # assemble full profile (species + globals + time)
                profile = self._extract_profile(grp, gname, n_t)
                if profile is None:
                    report["dropped_reasons"]["extract_profile_failed"] += 1
                    continue

                # final profileâ€‘level check
                if (np.any(~np.isfinite(profile)) or
                    np.any(profile <= self.min_value_threshold)):
                    report["dropped_reasons"]["below_threshold"] += 1
                    continue

                # update statistics
                report["profiles_kept"] += 1
                valid_sample_count += (n_t - 1)
                self._update_stats_for_profile(
                    profile, n_t,
                    accumulators,
                    ratio_accumulators,
                )

        return accumulators, ratio_accumulators, valid_sample_count, report

    def process_file_for_shards(self, file_path: Path, output_dir: Path) -> Dict[str, Any]:
        """Process file and write to split-specific shard directories."""
        # Create separate shard writers for each split
        shard_writers = {
            "train": ShardWriter(
                output_dir / "train", 
                self.proc_cfg["shard_size"], 
                file_path.stem
            ),
            "validation": ShardWriter(
                output_dir / "validation",
                self.proc_cfg["shard_size"],
                file_path.stem
            ),
            "test": ShardWriter(
                output_dir / "test",
                self.proc_cfg["shard_size"], 
                file_path.stem
            )
        }
        
        # Track samples per split
        split_counts = {"train": 0, "validation": 0, "test": 0}
        
        # Process file
        with h5py.File(file_path, "r") as f:
            for gname in sorted(f.keys()):
                # Validation checks
                is_valid, _ = self._is_profile_valid(f[gname])
                if not is_valid:
                    continue
                
                # Use fraction check
                use_fraction = self.train_cfg["use_fraction"]
                if use_fraction < 1.0:
                    hash_val = int(hashlib.sha256(gname.encode('utf-8')).hexdigest()[:8], 16) / 0xFFFFFFFF
                    if hash_val >= use_fraction:
                        continue
                
                # Process profile
                grp = f[gname]
                n_t = grp[self.time_var].shape[0]
                if n_t <= 1:
                    continue
                    
                profile = self._extract_profile(grp, gname, n_t)
                if profile is None:
                    continue

                if (np.any(~np.isfinite(profile)) or
                    np.any(profile <= self.min_value_threshold)):
                    continue

                # Determine split
                p = int(hashlib.sha256((gname + "_split").encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
                test_frac = self.train_cfg["test_fraction"]
                val_frac = self.train_cfg["val_fraction"]
                
                if p < test_frac:
                    split_key = "test"
                elif p < test_frac + val_frac:
                    split_key = "validation"
                else:
                    split_key = "train"
                
                # Convert profile to samples
                if self.prediction_mode == "ratio":
                    samples = self._profile_to_samples_ratio(
                        profile, n_t, self.norm_stats.get("ratio_stats", {})
                    )
                else:
                    norm_prof = self.norm_helper.normalize_profile(
                        torch.from_numpy(profile)
                    ).numpy()
                    samples = self._profile_to_samples(norm_prof, n_t)
                
                if samples is not None:
                    # Add to appropriate writer - no need to track indices
                    shard_writers[split_key].add_samples(samples)
                    split_counts[split_key] += samples.shape[0]
        
        # Flush all writers
        for writer in shard_writers.values():
            writer.flush()
        
        # Return metadata
        return {
            "splits": {
                "train": {
                    "shards": shard_writers["train"].get_shard_metadata(),
                    "samples_written": split_counts["train"]
                },
                "validation": {
                    "shards": shard_writers["validation"].get_shard_metadata(),
                    "samples_written": split_counts["validation"]
                },
                "test": {
                    "shards": shard_writers["test"].get_shard_metadata(),
                    "samples_written": split_counts["test"]
                }
            }
        }

    def _update_stats_for_profile(self, profile, n_t, accumulators, ratio_accumulators):
        """Consistent normalization in both modes."""
        import logging
        logger = logging.getLogger(__name__)
        
        if self.prediction_mode == "ratio":
            # Use full profiles for all variables in ratio mode
            for var, acc in accumulators.items():
                idx = acc["index"]
                method = acc["method"]
                
                # Use full profile data for all variables (not just initial timestep)
                if var == self.time_var and n_t > 1:
                    vec = profile[1:, idx]
                else:
                    vec = profile[:, idx]
                
                if vec.size > 0:
                    self.normalizer._update_single_accumulator(acc, vec, var)
            
            # Compute ratio statistics correctly with proper indices
            initial = profile[0, self.species_indices]
            future = profile[1:, self.species_indices]
            
            ratios = future / np.maximum(initial[None, :], self.normalizer.epsilon)
            ratios = np.clip(ratios, -DEFAULT_EPSILON_MAX, DEFAULT_EPSILON_MAX)
            log_ratios = np.sign(ratios) * np.log10( np.clip(np.abs(ratios), DEFAULT_EPSILON_MIN, DEFAULT_EPSILON_MAX))


            for i, var_name in enumerate(self.species_vars):
                self.normalizer._update_single_accumulator(ratio_accumulators[var_name], log_ratios[:, i], var_name)
        else:
            # Absolute mode
            for var, acc in accumulators.items():
                idx = acc["index"]
                vec = profile[1:, idx] if (var == self.time_var and n_t > 1) else profile[:, idx]
                if vec.size > 0:
                    self.normalizer._update_single_accumulator(acc, vec, var)

    def _process_single_group(self, grp, gname, ratio_stats) -> Optional[Tuple[np.ndarray, str]]:
        # The stricter validation is now done before this function is called.
        n_t = grp[self.time_var].shape[0]
        if n_t <= 1: return None
        profile = self._extract_profile(grp, gname, n_t)
        if profile is None: return None
        
        if self.prediction_mode == "ratio": samples = self._profile_to_samples_ratio(profile, n_t, ratio_stats)
        else: samples = self._profile_to_samples(self.norm_helper.normalize_profile(torch.from_numpy(profile)).numpy(), n_t)
        if samples is None: return None
        
        p = int(hashlib.sha256((gname + "_split").encode("utf-8")).hexdigest()[:8], 16) / 0xFFFFFFFF
        split_key = "test" if p < self.train_cfg["test_fraction"] else "validation" if p < self.train_cfg["test_fraction"] + self.train_cfg["val_fraction"] else "train"
        return samples, split_key

    
    def _extract_profile(self, group: h5py.Group, gname: str, n_t: int) -> Optional[np.ndarray]:
        import re
        globals_dict = {f"{lbl}_init": float(val) for lbl, val in re.findall(r"_([A-Z])_([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", gname) if f"{lbl}_init" in self.global_vars}
        if len(globals_dict) != len(self.global_vars): return None
        profile = np.empty((n_t, self.n_vars), dtype=np.float32)
        try:
            for i, var in enumerate(self.var_order):
                profile[:, i] = group[var][:] if var in group else globals_dict[var]
        except Exception: return None
        return profile

    def _profile_to_samples(self, norm_prof, n_t):
        """Get samples"""
        if n_t <= 1:
            return None
        
        n_inputs = self.n_species + self.n_globals + 1
        samples = np.empty((n_t - 1, n_inputs + self.n_species), dtype=np.float32)
        
        # Initial species
        samples[:, :self.n_species] = norm_prof[0, self.species_indices] 

        # Globals
        samples[:, self.n_species:self.n_species + self.n_globals] = norm_prof[0, self.global_indices]

        # Time
        samples[:, n_inputs - 1] = norm_prof[1:, self.time_idx]

        # Target species
        samples[:, n_inputs:] = norm_prof[1:, self.species_indices]
        
        return samples

    def _profile_to_samples_ratio(
        self, raw_prof: np.ndarray, n_t: int, ratio_stats: Dict[str, Dict]
    ) -> Optional[np.ndarray]:
        """
        Build (n_tâ€‘1) samples for ratioâ€‘prediction mode.
        raw_prof is **unnormalised** profile array  shape = (n_t, n_vars)
        ratio_stats contains mean/std/min/max for logâ€‘ratios (already computed)
        """
        import logging
        logger = logging.getLogger(__name__)

        if n_t <= 1:
            return None
        
        n_inputs  = self.n_species + self.n_globals + 1
        samples   = np.empty((n_t - 1, n_inputs + self.n_species), dtype=np.float32)

        norm_prof = self.norm_helper.normalize_profile(torch.from_numpy(raw_prof)).numpy()
        samples[:, :self.n_species]                   = norm_prof[0, self.species_indices]
        samples[:, self.n_species:self.n_species+self.n_globals] = norm_prof[0, self.global_indices]
        samples[:, n_inputs - 1]                      = norm_prof[1:, self.time_idx]

        initial = raw_prof[0, self.species_indices]
        future  = raw_prof[1:, self.species_indices]

        ratios = future / np.maximum(initial[None, :], self.norm_cfg["epsilon"])
        ratios = np.clip(ratios, -DEFAULT_EPSILON_MAX, DEFAULT_EPSILON_MAX)
        log_ratios = np.sign(ratios) * np.log10(np.clip(np.abs(ratios), DEFAULT_EPSILON_MIN, DEFAULT_EPSILON_MAX))

        methods_cfg = self.norm_cfg.get("methods", {})
        default_m   = self.norm_cfg.get("default_method", "standard")
        clamp_val   = self.norm_cfg.get("clamp_value", 50.0)
        min_std     = self.norm_cfg.get("min_std", 1e-10)

        normd = np.empty_like(log_ratios, dtype=np.float32)

        for i, var in enumerate(self.species_vars):
            method = methods_cfg.get(var, default_m)
            if method.startswith("log-"):
                method = method[4:]

            stats = ratio_stats[var]

            if method == "min-max":
                rng = max(stats["max"] - stats["min"], self.norm_cfg["epsilon"])
                normd[:, i] = (log_ratios[:, i] - stats["min"]) / rng
            else:
                std = max(stats["std"], min_std)
                normd[:, i] = (log_ratios[:, i] - stats["mean"]) / std

        if np.any(np.abs(normd) > clamp_val):
            n_clamped = np.sum(np.abs(normd) > clamp_val)
            logger.warning(f"Clamping {n_clamped} normalised logâ€‘ratio values to Â±{clamp_val}")

        samples[:, n_inputs:] = np.clip(normd, -clamp_val, clamp_val)
        return samples

def stats_worker(file_path, config):
    return CorePreprocessor(config).process_file_for_stats(Path(file_path))

def shard_worker(file_path, config, norm_stats, output_dir):
    """Legacy helper kept for completeness; matches shard_worker_split_aware signature."""
    processor = CorePreprocessor(config, norm_stats)
    return processor.process_file_for_shards(Path(file_path), Path(output_dir))

class DataPreprocessor:
    """Main parent class to orchestrate parallel data preprocessing."""
    def __init__(self, raw_files: List[Path], output_dir: Path, config: Dict[str, Any]):
        self.raw_files = sorted(raw_files)
        self.output_dir = output_dir

        self.processed_dir = self.output_dir
        self.processed_dir.mkdir(parents=True, exist_ok=True)

        self.config = config
        self.logger = logging.getLogger(__name__)
        self.normalizer = DataNormalizer(config)
        self.num_workers = config["preprocessing"].get("num_workers", 1)
        self.parallel = self.num_workers > 1 and len(self.raw_files) > 1

    def process_to_npy_shards(self) -> None:
        """Main entry point - creates split-specific shard directories."""
        start_time = time.time()
        self.logger.info(f"Starting core data preprocessing with {len(self.raw_files)} files...")
        
        # Collect statistics
        norm_stats, file_sample_counts, summary_report = self._collect_stats_and_counts()
        save_json(norm_stats, self.output_dir / "normalization.json")

        # Write split-specific shards
        split_metadata = self._write_normalized_shards(norm_stats, file_sample_counts)
        
        # Save split-aware shard index
        shard_index = {
            "n_species": len(self.config["data"]["species_variables"]),
            "n_globals": len(self.config["data"]["global_variables"]),
            "samples_per_shard": self.config["preprocessing"]["shard_size"],
            "compression": self.config["preprocessing"].get("compression"),
            "prediction_mode": self.config.get("prediction", {}).get("mode", "absolute"),
            "splits": split_metadata,
            "total_samples": sum(meta["total_samples"] for meta in split_metadata.values())
        }
        save_json(shard_index, self.output_dir / "shard_index.json")

        self._write_summary_log(summary_report, shard_index["total_samples"])
        self.logger.info(f"Core data preprocessing completed in {time.time() - start_time:.1f}s")
        
    def generate_split_indices(self) -> None:
        """Generates train/val/test split indices from an existing shard_index.json. This is a very fast operation."""
        self.logger.info("Generating new train/val/test split indices...")
        shard_index_path = self.output_dir / "shard_index.json"
        if not shard_index_path.exists():
            raise FileNotFoundError(f"Cannot generate splits: shard_index.json not found in {self.output_dir}")
        
        shard_index = load_json(shard_index_path)
        total_samples = shard_index["total_samples"]
        indices = np.arange(total_samples)
        
        seed = self.config.get("system", {}).get("seed", 42)
        np.random.seed(seed)
        np.random.shuffle(indices)
        
        use_fraction = self.config["training"].get("use_fraction", 1.0)
        if use_fraction < 1.0:
            indices = indices[:int(total_samples * use_fraction)]
        
        n = len(indices)
        test_frac = self.config["training"]["test_fraction"]
        val_frac = self.config["training"]["val_fraction"]
        
        test_split_idx = int(n * test_frac)
        val_split_idx = test_split_idx + int(n * val_frac)
        
        split_data = {
            "test": np.sort(indices[:test_split_idx]).astype(np.int64),
            "validation": np.sort(indices[test_split_idx:val_split_idx]).astype(np.int64),
            "train": np.sort(indices[val_split_idx:]).astype(np.int64)
        }
        
        for name, idx_array in split_data.items():
            path = self.output_dir / shard_index["split_files"][name]
            np.save(path, idx_array)
            self.logger.info(f"Saved {name} indices to {path} ({len(idx_array)} samples)")

    def _write_normalized_shards(self, norm_stats, file_sample_counts) -> Dict[str, List[Dict]]:
        """Second pass: write split-specific shards to separate directories."""
        self.logger.info("Writing split-specific shards...")
        
        # Create split directories
        split_dirs = {
            "train": self.processed_dir / "train",
            "validation": self.processed_dir / "validation", 
            "test": self.processed_dir / "test"
        }
        for split_name, dir_path in split_dirs.items():
            dir_path.mkdir(exist_ok=True)
            self.logger.info(f"Created {split_name} directory: {dir_path}")
        
        # Initialize split metadata
        split_metadata = {
            "train": {"shards": [], "total_samples": 0},
            "validation": {"shards": [], "total_samples": 0},
            "test": {"shards": [], "total_samples": 0}
        }
        
        # Process each file
        with ProcessPoolExecutor(max_workers=self.num_workers if self.parallel else 1) as exe:
            futures = []
            
            for file_path in self.raw_files:
                future = exe.submit(
                    shard_worker_split_aware,
                    file_path,
                    self.config,
                    norm_stats,
                    self.processed_dir
                )
                futures.append((future, file_path))
            
            # Collect results
            for future, file_path in futures:
                result = future.result()
                
                # Aggregate metadata by split
                for split_name in ["train", "validation", "test"]:
                    split_meta = result["splits"][split_name]
                    split_metadata[split_name]["shards"].extend(split_meta["shards"])
                    split_metadata[split_name]["total_samples"] += split_meta["samples_written"]
        
        # Sort shards within each split and assign global indices
        for split_name, meta in split_metadata.items():
            # Sort shards by filename
            meta["shards"].sort(key=lambda x: x["filename"])
            
            # Assign sequential start/end indices
            current_idx = 0
            for shard in meta["shards"]:
                shard["start_idx"] = current_idx
                shard["end_idx"] = current_idx + shard["n_samples"]
                current_idx = shard["end_idx"]
            
            self.logger.info(f"{split_name}: {len(meta['shards'])} shards, {meta['total_samples']:,} samples")
        
        return split_metadata
     
    def _collect_stats_and_counts(self) -> Tuple[Dict, Dict, Dict]:
        self.logger.info("Pass 1: Collecting statistics and sample counts...")
        prediction_mode = self.config.get("prediction", {}).get("mode", "absolute")
        final_accs = self.normalizer._initialize_accumulators()
        final_ratio_accs = {var: {"count": 0,"mean": 0.0,"m2": 0.0,"min": float('inf'),"max": float('-inf')} for var in self.config["data"]["species_variables"]} if prediction_mode == "ratio" else {}
        file_counts = {}
        
        total_report = {
            "total_profiles": 0,
            "profiles_kept": 0,
            "dropped_reasons": defaultdict(int)
        }

        with ProcessPoolExecutor(max_workers=self.num_workers if self.parallel else 1) as executor:
            futures = {executor.submit(stats_worker, fp, self.config): fp for fp in self.raw_files}
            for fut in as_completed(futures):
                accs, ratio_accs, count, worker_report = fut.result()
                
                # Aggregate results
                self.normalizer._merge_accumulators(final_accs, accs)
                if ratio_accs: self.normalizer._merge_accumulators(final_ratio_accs, ratio_accs)
                file_counts[futures[fut].name] = count
                
                # --- NEW: Aggregate the reports ---
                total_report["total_profiles"] += worker_report["total_profiles"]
                total_report["profiles_kept"] += worker_report["profiles_kept"]
                for reason, num in worker_report["dropped_reasons"].items():
                    total_report["dropped_reasons"][reason] += num

        norm_stats = self.normalizer._finalize_statistics(final_accs)
        if final_ratio_accs:
            norm_stats["ratio_stats"] = self.normalizer._finalize_statistics(final_ratio_accs, is_ratio=True)

        file_counts = {Path(k).stem: v for k, v in file_counts.items()}

        return norm_stats, file_counts, total_report

    def _write_summary_log(self, report: Dict, total_samples: int):
        """Writes a human-readable summary of the preprocessing results."""
        log_dir = Path(self.config["paths"]["log_dir"])
        log_dir.mkdir(exist_ok=True)
        summary_path = log_dir / f"preprocessing_summary_{time.strftime('%Y%m%d_%H%M%S')}.txt"
        
        dropped_count = report["total_profiles"] - report["profiles_kept"]
        
        reason_map = {
            "missing_keys": "Required dataset keys were missing",
            "non_finite": "Contained NaN or Infinity values",
            "below_threshold": f"A species value was below the threshold ({self.config['preprocessing']['min_value_threshold']:.1e})",
            "too_few_timesteps": "Contained 1 or fewer time steps",
            "extract_profile_failed": "Failed to extract global variables from name",
            "read_error": "Could not read a dataset from the HDF5 group"
        }

        with open(summary_path, 'w') as f:
            f.write("="*60 + "\n")
            f.write("      Data Preprocessing Summary\n")
            f.write("="*60 + "\n\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Raw Files Processed: {len(self.raw_files)}\n\n")

            f.write("--- Profile Filtering --- \n")
            f.write(f"Total Profiles Found:     {report['total_profiles']:,}\n")
            f.write(f"Profiles Kept:            {report['profiles_kept']:,}\n")
            f.write(f"Profiles Dropped:         {dropped_count:,}\n\n")
            
            if dropped_count > 0:
                f.write("--- Reasons for Dropped Profiles ---\n")
                for reason, count in sorted(report["dropped_reasons"].items()):
                    f.write(f"  - {count:>10,} : {reason_map.get(reason, reason)}\n")
                f.write("\n")

            f.write("--- Final Sample Count ---\n")
            f.write(f"Total Usable Samples:     {total_samples:,}\n")
            f.write("(Train/Val/Test splits generated separately)\n")

        self.logger.info(f"Preprocessing summary saved to: {summary_path}")


class ShardWriter:
    """Writes numpy arrays to shard files, handling buffering and file naming."""
    def __init__(self, output_dir: Path, shard_size: int, shard_idx_base: str):
        self.output_dir = output_dir
        self.shard_size = shard_size
        self.shard_idx_base = shard_idx_base
        self.buffer: List[np.ndarray] = []
        self.buffer_size = 0
        self.local_shard_id = 0
        self.shard_metadata: List[Dict] = []
        
        # Ensure the output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def add_samples(self, samples: np.ndarray):
        """Simplified add_samples without tracking global indices."""
        if samples.dtype != np.float32:
            samples = samples.astype(np.float32)
        
        self.buffer.append(samples)
        self.buffer_size += samples.shape[0]
        
        while self.buffer_size >= self.shard_size:
            self._write_shard()

    def _write_shard(self) -> None:
        """Write one shard of exactly shard_size samples (or less if flushing)."""
        if not self.buffer:
            return

        rows_to_write = []
        size_so_far = 0

        # Collect arrays until we have enough for a shard
        while self.buffer and size_so_far < self.shard_size:
            arr = self.buffer.pop(0)
            needed = self.shard_size - size_so_far
            
            if arr.shape[0] <= needed:
                rows_to_write.append(arr)
                size_so_far += arr.shape[0]
            else:
                # Split the array
                rows_to_write.append(arr[:needed])
                self.buffer.insert(0, arr[needed:])
                size_so_far += needed
        
        # Update buffer size
        self.buffer_size = sum(arr.shape[0] for arr in self.buffer)

        # Write shard
        data = np.concatenate(rows_to_write).astype(np.float32, copy=False)
        
        final_path = self.output_dir / f"shard_{self.shard_idx_base}_{self.local_shard_id:04d}.npy"
        tmp_path = final_path.with_suffix(".tmp.npy")

        np.save(tmp_path, data, allow_pickle=False)
        os.replace(tmp_path, final_path)

        self.shard_metadata.append({
            "filename": final_path.name,
            "n_samples": data.shape[0],
        })
        
        self.local_shard_id += 1

    def flush(self) -> None:
        """Write out any rows left in the buffer (< shard_size)."""
        if self.buffer_size == 0:
            return
        
        # write whatever is left as a final shard
        rows_to_write = self.buffer
        self.buffer = []
        self.buffer_size = 0

        data = np.concatenate(rows_to_write).astype(np.float32, copy=False)
        final_path = self.output_dir / f"shard_{self.shard_idx_base}_{self.local_shard_id:04d}.npy"
        tmp_path   = final_path.with_suffix(".tmp.npy")
        np.save(tmp_path, data, allow_pickle=False)
        os.replace(tmp_path, final_path)

        self.shard_metadata.append({
            "filename": final_path.name,
            "n_samples": data.shape[0],
        })
        self.local_shard_id += 1

    def get_shard_metadata(self) -> List[Dict]:
        """Return metadata collected for all shards."""
        return list(self.shard_metadata)
    
def shard_worker_split_aware(file_path, config, norm_stats, output_dir):
    """Worker function that creates split-specific shards."""
    processor = CorePreprocessor(config, norm_stats)
    return processor.process_file_for_shards(Path(file_path), Path(output_dir))   

===== /Users/imalsky/Desktop/Chemulator/src/data/dataset.py =====
#!/usr/bin/env python3
"""
High-performance dataset implementation for chemical kinetics training.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple, Optional, List
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import math
from functools import lru_cache


class NPYDataset(Dataset):
    """
    PyTorch Dataset with GPU caching and CPU fallback.
    """
    def __init__(self, shard_dir: Path, split_name: str, config: Dict[str, Any], 
                 device: torch.device):
        """Initialize dataset for a specific split."""
        super().__init__()
        self.shard_dir = Path(shard_dir)
        self.split_dir = self.shard_dir / split_name
        self.split_name = split_name
        self.config = config
        self.device = device
        self.logger = logging.getLogger(__name__)

        # Verify split directory exists
        if not self.split_dir.exists():
            raise FileNotFoundError(f"Split directory not found: {self.split_dir}")

        # Load shard index metadata
        shard_index_path = self.shard_dir / "shard_index.json"
        with open(shard_index_path) as f:
            full_index = json.load(f)
        
        self.shard_index = full_index
        self.split_info = full_index["splits"][split_name]
        
        # Extract dimensions
        self.n_species = self.shard_index["n_species"]
        self.n_globals = self.shard_index["n_globals"]
        self.samples_per_shard = self.shard_index["samples_per_shard"]
        self.prediction_mode = self.shard_index.get("prediction_mode", "absolute")
        self.n_features = self.n_species * 2 + self.n_globals + 1
        self.n_inputs = self.n_species + self.n_globals + 1
        
        # Get shard info for this split
        self.shards = self.split_info["shards"]
        self.n_shards = len(self.shards)
        self.n_total_samples = self.split_info["total_samples"]
        
        # Build lookup arrays
        self._shard_starts = np.array([s["start_idx"] for s in self.shards])
        self._shard_ends = np.array([s["end_idx"] for s in self.shards])
        
        # Memory info
        self.bytes_per_sample = self.n_features * 4  # float32
        self.total_bytes = self.n_total_samples * self.bytes_per_sample
        
        # Initialize caching
        self.gpu_cache = None
        self.cpu_fallback = False
        self._try_gpu_cache()
        
    def _try_gpu_cache(self):
        """Load the entire split to GPU memory; fall back to CPU if needed."""
        gpu_cache_setting = self.config.get("training", {}).get("gpu_cache_dataset", "auto")
        if gpu_cache_setting is False:
            self.logger.info(f"GPU caching disabled for {self.split_name}")
            self.cpu_fallback = True
            return

        if self.device.type != "cuda":
            self.logger.info(f"GPU caching not available on {self.device.type}")
            self.cpu_fallback = True
            return

        free_mem, _ = torch.cuda.mem_get_info(self.device.index)
        needed_gb = self.total_bytes / 1024 ** 3
        free_gb   = free_mem       / 1024 ** 3
        if needed_gb > free_gb * 0.85:     
            self.logger.warning(
                f"Insufficient GPU memory for {self.split_name}: "
                f"need {needed_gb:.1f}â€¯GB, have {free_gb:.1f}â€¯GB.  Falling back to CPU."
            )
            self.cpu_fallback = True
            return

        self.logger.info(f"Loading {self.split_name} dataset to GPU ({needed_gb:.1f}â€¯GB)â€¦")
        start = time.time()

        self.gpu_cache = torch.empty(
            (self.n_total_samples, self.n_features),
            dtype=torch.float32,
            device=self.device,
        )

        cur = 0
        for shard in self.shards:
            shard_path = self.split_dir / shard["filename"]
            if self.shard_index.get("compression") == "npz":
                with np.load(shard_path) as zf:
                    data = zf["data"].astype(np.float32, copy=False)
            else:
                data = np.load(shard_path).astype(np.float32, copy=False)

            n = data.shape[0]
            self.gpu_cache[cur:cur + n] = torch.from_numpy(data).pin_memory().to(
                self.device, non_blocking=True
            )
            cur += n
            del data                  

        torch.cuda.synchronize()
        t = time.time() - start
        self.logger.info(f"GPU cache loaded in {t:.1f}s ({needed_gb/t:.1f}â€¯GB/s)")

    def _load_shard_cpu(self, shard_idx: int) -> np.ndarray:
        """Load a shard from disk (CPU fallback)."""
        # ====================================================================
        # FIXED: Added simple per-worker LRU cache to avoid re-reading files
        # ====================================================================
        # Check if the cache exists on this worker process
        if not hasattr(self, '_shard_cache'):
            # functools.lru_cache is a simpler way to do this
            from functools import lru_cache
            # Cache up to 2 shards per worker, a common scenario
            self._shard_cache = lru_cache(maxsize=2)(self._load_shard_from_disk)
        
        return self._shard_cache(shard_idx)

    def _load_shard_from_disk(self, shard_idx: int) -> np.ndarray:
        """Helper for the LRU cache that actually hits the disk."""
        shard_info = self.shards[shard_idx]
        shard_path = self.split_dir / shard_info["filename"]
        
        if self.shard_index.get("compression") == "npz":
            with np.load(shard_path) as zf:
                return zf["data"].astype(np.float32, copy=False)
        else:
            return np.load(shard_path).astype(np.float32, copy=False)

    def __len__(self) -> int:
        """Return the total number of samples."""
        return self.n_total_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get a single sample by index."""
        if self.gpu_cache is not None:
            # Fast path: Direct GPU slicing, no change needed here.
            row = self.gpu_cache[idx]
            return row[:self.n_inputs], row[self.n_inputs:]
        else:
            # CPU fallback path
            if not self.cpu_fallback:
                raise RuntimeError("Dataset not properly initialized")
            
            # Find which shard contains this index
            shard_idx = np.searchsorted(self._shard_starts, idx, side='right') - 1
            local_idx = idx - self._shard_starts[shard_idx]
            
            # Load shard and get sample
            shard_data = self._load_shard_cpu(shard_idx)
            row = shard_data[local_idx]
            
            # Return CPU tensors. The batch will be moved to the GPU
            # all at once by the dataloader / training loop.
            input_tensor = torch.from_numpy(row[:self.n_inputs].copy())
            target_tensor = torch.from_numpy(row[self.n_inputs:].copy())
            
            return input_tensor, target_tensor

    def get_cache_info(self) -> Dict[str, Any]:
        """Get cache statistics with consistent structure."""
        if self.gpu_cache is not None:
            return {
                "type": "gpu",
                "size_gb": self.total_bytes / 1024**3,
                "device": str(self.device),
                "status": "active"
            }
        elif self.cpu_fallback:
            return {
                "type": "cpu",
                "size_gb": 0,
                "device": str(self.device),
                "status": "fallback",
                "message": "Using CPU loading due to insufficient GPU memory"
            }
        else:
            return {
                "type": "none",
                "size_gb": 0,
                "device": str(self.device),
                "status": "error",
                "message": "Dataset caching failed"
            }

def create_dataloader(dataset: Dataset,
                      config: Dict[str, Any],
                      shuffle: bool = True,
                      device: Optional[torch.device] = None,
                      drop_last: bool = True,
                      **_) -> DataLoader:
    """
    Build a DataLoader that never loops Pythonâ€‘side over 32â€¯768 items.
    If the dataset lives on the GPU, we wrap it in GPUBatchDataset so each
    __getitem__ delivers a full batch slice in one shot.
    """
    if dataset is None or len(dataset) == 0:
        logging.getLogger(__name__).warning("Cannot create DataLoader for empty dataset")
        return None

    log  = logging.getLogger(__name__)
    tcfg = config["training"]
    bs   = tcfg["batch_size"]

    is_gpu_cached  = getattr(dataset, "gpu_cache", None) is not None
    is_cpu_fallback = getattr(dataset, "cpu_fallback", False)

    # ---------- GPUâ€‘cached fast path ----------
    if is_gpu_cached:
        log.info(f"DataLoader[{dataset.split_name}] GPUâ€‘batch mode: bs={bs}")

        class GPUBatchDataset(torch.utils.data.Dataset):
            def __init__(self, gpu_tensor: torch.Tensor, n_inputs: int,
                         batch_size: int, shuffle_batches: bool):
                self.gpu_tensor  = gpu_tensor
                self.n_inputs    = n_inputs
                self.batch_size  = batch_size
                self.shuffle     = shuffle_batches
                self.total       = gpu_tensor.size(0)
                self.n_batches   = math.ceil(self.total / batch_size)
                self.permutation = None

            def __len__(self):
                return self.n_batches

            def __getitem__(self, batch_idx: int):
                if self.shuffle:
                    if self.permutation is None or batch_idx == 0:
                        self.permutation = torch.randperm(
                            self.total, device=self.gpu_tensor.device
                        )
                    idx = self.permutation
                else:
                    idx = None  # straight slice

                start = batch_idx * self.batch_size
                end   = min(start + self.batch_size, self.total)

                if idx is None:
                    batch = self.gpu_tensor[start:end]
                else:
                    batch = self.gpu_tensor.index_select(0, idx[start:end])

                return batch[:, :self.n_inputs], batch[:, self.n_inputs:]

        gpu_ds = GPUBatchDataset(
            dataset.gpu_cache, dataset.n_inputs, bs, shuffle
        )
        return DataLoader(
            gpu_ds,
            batch_size=None,          # dataset already returns a full batch
            shuffle=False,            # handled inside GPUBatchDataset
            num_workers=0,
            pin_memory=False,
            drop_last=False,
        )

    # ---------- CPU fallback ----------
    log.warning(f"DataLoader[{dataset.split_name}] CPU fallback mode: bs={bs}")
    workers = tcfg.get("num_workers") or min(16, (os.cpu_count() or 1))
    return DataLoader(
        dataset,
        batch_size=bs,
        shuffle=shuffle,
        num_workers=workers,
        pin_memory=(device and device.type == "cuda"),
        drop_last=drop_last,
        persistent_workers=(workers > 0),
        prefetch_factor=2 if workers > 0 else None,
    )

===== /Users/imalsky/Desktop/Chemulator/src/data/normalizer.py =====
#!/usr/bin/env python3
"""
Data normalization module for chemical kinetics datasets.
This version preserves all data validation and logging and supports the
parallel preprocessing architecture with _merge_accumulators.
"""

import logging
import math
from typing import Dict, List, Any, Optional

import numpy as np
import torch

DEFAULT_EPSILON = 1e-30
DEFAULT_MIN_STD = 1e-10
DEFAULT_CLAMP = 50.0
# Safe epsilon for float32
SAFE_EPSILON = 1e-38
MIN_RANGE = 1e-10

class DataNormalizer:
    """Calculates normalization statistics with robust data validation."""
    
    def __init__(self, config: Dict[str, Any]) -> None:
        self.config = config
        self.data_config = config["data"]
        self.norm_config = config["normalization"]

        self.species_vars = self.data_config["species_variables"]
        self.global_vars = self.data_config["global_variables"]
        self.time_var = self.data_config["time_variable"]
        self.all_vars = self.species_vars + self.global_vars + [self.time_var]

        self.epsilon = self.norm_config.get("epsilon", DEFAULT_EPSILON)
        self.min_std = self.norm_config.get("min_std", DEFAULT_MIN_STD)
        self.logger = logging.getLogger(__name__)
        
    def _initialize_accumulators(self) -> Dict[str, Dict[str, Any]]:
        """Initialize per-variable statistics accumulators."""
        accumulators = {}
        for i, var in enumerate(self.all_vars):
            method = self._get_method(var)
            if method == "none":
                continue
            acc = {
                "method": method, "index": i, "count": 0, "mean": 0.0, "m2": 0.0,
                "min": float("inf"), "max": float("-inf"),
            }
            accumulators[var] = acc
        return accumulators
    
    def _get_method(self, var: str) -> str:
        """Get normalization method for a variable."""
        methods = self.norm_config.get("methods", {})
        return methods.get(var, self.norm_config["default_method"])
    
    def _update_single_accumulator(self, acc: Dict[str, Any], vec: np.ndarray, var_name: str):
        """
        Vectorized update for one accumulator with safe epsilon handling.
        """
        if vec.size == 0:
            return

        # 1. Filter non-finite values and warn if there are many
        finite_mask = np.isfinite(vec)
        if not np.all(finite_mask):
            n_non_finite = (~finite_mask).sum()
            if n_non_finite / vec.size > 0.01:
                self.logger.warning(f"Variable '{var_name}' has {n_non_finite}/{vec.size} non-finite values.")
            vec = vec[finite_mask]
            if vec.size == 0:
                self.logger.warning(f"Variable '{var_name}' has no finite values after filtering, skipping.")
                return

        # 2. Handle log-transformations with safe epsilon
        if acc["method"].startswith("log-"):
            below_epsilon = vec < SAFE_EPSILON
            if np.any(below_epsilon):
                self.logger.warning(
                    f"Variable '{var_name}' has {below_epsilon.sum()} values below epsilon {SAFE_EPSILON} "
                    f"Min value: {vec.min():.2e}"
                )
            vec = np.log10(np.maximum(vec, SAFE_EPSILON))
            
            # Check for extreme values after log transform
            if vec.min() < -25 or vec.max() > 25:
                self.logger.warning(
                    f"Variable '{var_name}' has extreme log values: [{vec.min():.1f}, {vec.max():.1f}]"
                )

        # 3. Perform Chan's parallel update for mean and variance
        n_b = vec.size
        mean_b = float(vec.mean())
        m2_b = float(((vec - mean_b) ** 2).sum()) if n_b > 1 else 0.0

        n_a = acc["count"]
        delta = mean_b - acc["mean"]
        n_ab = n_a + n_b

        if n_ab > 0:
            acc["mean"] = (n_a * acc["mean"] + n_b * mean_b) / n_ab
            acc["m2"] += m2_b + delta**2 * n_a * n_b / n_ab
        
        acc["count"] = n_ab
        acc["min"] = min(acc["min"], float(vec.min()))
        acc["max"] = max(acc["max"], float(vec.max()))

    def _merge_accumulators(
        self,
        main_accs: Dict[str, Dict[str, Any]],
        other_accs: Dict[str, Dict[str, Any]],
    ) -> None:
        """Merge statistics from another set of accumulators into the main one."""
        for var, other_acc in other_accs.items():
            if not other_acc: continue
            if var not in main_accs:
                main_accs[var] = other_acc
                continue

            main_acc = main_accs[var]
            
            n_a, mean_a, m2_a = main_acc["count"], main_acc["mean"], main_acc["m2"]
            n_b, mean_b, m2_b = other_acc["count"], other_acc["mean"], other_acc["m2"]
            
            n_ab = n_a + n_b
            if n_ab == 0: continue
                
            delta = mean_b - mean_a
            
            main_acc["mean"] = (n_a * mean_a + n_b * mean_b) / n_ab
            main_acc["m2"] = m2_a + m2_b + (delta**2 * n_a * n_b) / n_ab
            main_acc["count"] = n_ab
            main_acc["min"] = min(main_acc["min"], other_acc["min"])
            main_acc["max"] = max(main_acc["max"], other_acc["max"])
        
    def _finalize_statistics(self, accumulators: Dict[str, Dict[str, Any]], is_ratio: bool = False) -> Dict[str, Any]:
        """Finalize statistics from accumulators with improved numerical stability."""
        stats = { "per_key_stats": {} }
        if not is_ratio:
            stats["normalization_methods"] = {}
        
        for var, acc in accumulators.items():
            method = acc.get("method", "standard")
            if not is_ratio:
                stats["normalization_methods"][var] = method
            
            if method == "none":
                continue
            
            # Check if we have any valid samples
            if acc["count"] == 0:
                self.logger.warning(f"Variable '{var}' has no valid samples. Setting normalization to 'none'.")
                if not is_ratio:
                    stats["normalization_methods"][var] = "none"
                continue
            
            var_stats = {"method": method}
            
            # Calculate std with safeguards
            if acc["count"] > 1:
                variance = acc["m2"] / (acc["count"] - 1)
                std = max(math.sqrt(variance), self.min_std)
            else:
                std = 1.0
            
            if "standard" in method:
                mean_key, std_key = ("log_mean", "log_std") if "log" in method else ("mean", "std")
                var_stats[mean_key], var_stats[std_key] = acc["mean"], std
                
                # Warn if std is very small (constant variable)
                if std < MIN_RANGE:
                    self.logger.warning(f"Variable '{var}' appears constant (std={std:.2e}). Consider using 'none' normalization.")
                    
            elif "min-max" in method:
                var_stats["min"], var_stats["max"] = acc["min"], acc["max"]
                
                # Handle constant or near-constant variables
                range_val = acc["max"] - acc["min"]
                if range_val < MIN_RANGE:
                    self.logger.warning(f"Variable '{var}' has very small range ({range_val:.2e}). Adjusting to prevent instability.")
                    # Expand the range artificially to prevent division by tiny numbers
                    center = (acc["max"] + acc["min"]) / 2
                    var_stats["min"] = center - MIN_RANGE / 2
                    var_stats["max"] = center + MIN_RANGE / 2
            
            if is_ratio:
                stats[var] = {"mean": acc["mean"], "std": std, "min": acc["min"], "max": acc["max"], "count": acc["count"]}
            else:
                stats["per_key_stats"][var] = var_stats

        if not is_ratio:
            # Set any missing variables to "none" normalization
            for var in self.all_vars:
                if var not in stats["normalization_methods"]:
                    self.logger.warning(f"Variable '{var}' not found in statistics. Setting normalization to 'none'.")
                    stats["normalization_methods"][var] = "none"
            
            stats["epsilon"] = SAFE_EPSILON  # Use safe epsilon
            stats["clamp_value"] = self.norm_config.get("clamp_value", DEFAULT_CLAMP)
        
        return stats


class NormalizationHelper:
    """Applies pre-computed normalization statistics to data tensors."""
    
    def __init__(self, stats: Dict[str, Any], device: torch.device, 
                 species_vars: List[str], global_vars: List[str], 
                 time_var: str, config: Optional[Dict[str, Any]] = None):
        self.stats = stats
        self.device = device
        self.species_vars = species_vars
        self.global_vars = global_vars
        self.time_var = time_var

        self.n_species = len(species_vars)
        self.n_globals = len(global_vars)
        self.methods = stats["normalization_methods"]
        self.per_key_stats = stats["per_key_stats"]

        self.epsilon = stats.get("epsilon", DEFAULT_EPSILON)
        self.clamp_value = stats.get("clamp_value", DEFAULT_CLAMP)
        
        self.ratio_stats = stats.get("ratio_stats", None)

        self.logger = logging.getLogger(__name__)
        self._precompute_parameters()

    def _precompute_parameters(self):
        """Pre-compute normalization parameters on the target device for efficiency."""
        self.norm_params = {}
        self.method_groups = { "standard": [], "log-standard": [], "min-max": [], "log-min-max": [], "none": [] }
        var_to_col = {var: i for i, var in enumerate(self.species_vars + self.global_vars + [self.time_var])}

        for var, method in self.methods.items():
            if method == "none" or var not in self.per_key_stats:
                self.method_groups["none"].append(var)
                continue

            var_stats = self.per_key_stats[var]
            params = {"method": method}

            if "standard" in method:
                mean_key, std_key = ("log_mean", "log_std") if "log" in method else ("mean", "std")
                params["mean"] = torch.tensor(var_stats[mean_key], dtype=torch.float32, device=self.device)
                params["std"] = torch.tensor(var_stats[std_key], dtype=torch.float32, device=self.device)
            elif "min-max" in method:
                params["min"] = torch.tensor(var_stats["min"], dtype=torch.float32, device=self.device)
                params["max"] = torch.tensor(var_stats["max"], dtype=torch.float32, device=self.device)

            self.norm_params[var] = params
            self.method_groups[method].append(var)

        self.col_indices = {method: [var_to_col[var] for var in v_list] for method, v_list in self.method_groups.items() if v_list}

    def normalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """Normalize a complete profile tensor using vectorized operations."""
        if profile.device != self.device:
            profile = profile.to(self.device)
        normalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none": continue
            cols = normalized[:, col_idxs]
            if "standard" in method:
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                data = torch.log10(torch.clamp(cols, min=self.epsilon)) if "log" in method else cols
                normalized[:, col_idxs] = torch.clamp((data - means) / stds, -self.clamp_value, self.clamp_value)
            elif "min-max" in method:
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = torch.clamp(maxs - mins, min=self.epsilon)
                data = torch.log10(torch.clamp(cols, min=self.epsilon)) if "log" in method else cols
                normalized[:, col_idxs] = torch.clamp((data - mins) / ranges, 0.0, 1.0)
        return normalized

    def denormalize_profile(self, profile: torch.Tensor) -> torch.Tensor:
        """Denormalize a complete profile tensor using vectorized operations."""
        if profile.device != self.device:
            profile = profile.to(self.device)
        denormalized = profile.clone()

        for method, col_idxs in self.col_indices.items():
            if not col_idxs or method == "none": continue
            cols = denormalized[:, col_idxs]
            if "standard" in method:
                means = torch.stack([self.norm_params[var]["mean"] for var in self.method_groups[method]])
                stds = torch.stack([self.norm_params[var]["std"] for var in self.method_groups[method]])
                raw_vals = cols * stds + means
                if "log" in method:
                    raw_vals = torch.pow(10.0, torch.clamp(raw_vals, min=-38.0, max=38.0))
                denormalized[:, col_idxs] = torch.clamp(raw_vals, min=-3.4e38, max=3.4e38)
            elif "min-max" in method:
                mins = torch.stack([self.norm_params[var]["min"] for var in self.method_groups[method]])
                maxs = torch.stack([self.norm_params[var]["max"] for var in self.method_groups[method]])
                ranges = torch.clamp(maxs - mins, min=self.epsilon)
                raw_vals = cols * ranges + mins
                if "log" in method:
                    raw_vals = torch.pow(10.0, torch.clamp(raw_vals, min=-38.0, max=38.0))
                denormalized[:, col_idxs] = raw_vals
        return denormalized
        
    def denormalize_ratio_predictions(self, standardized_log_ratios: torch.Tensor,
                                    initial_species: torch.Tensor) -> torch.Tensor:
        """Convert standardized log-ratio predictions back to absolute species values."""
        if self.ratio_stats is None:
            raise ValueError("Ratio statistics not available for denormalization.")

        device = standardized_log_ratios.device
        initial_species = initial_species.to(device)

        ratio_means = torch.tensor([self.ratio_stats[var]["mean"] for var in self.species_vars], device=device, dtype=torch.float32)
        ratio_stds = torch.tensor([self.ratio_stats[var]["std"] for var in self.species_vars], device=device, dtype=torch.float32)
        
        log_ratios = (standardized_log_ratios * ratio_stds) + ratio_means
        log_ratios = torch.clamp(log_ratios, min=-38.0, max=38.0)
        
        ratios = torch.pow(10.0, log_ratios)
        predicted_species = initial_species * ratios
        return predicted_species

